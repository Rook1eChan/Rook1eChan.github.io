<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.150.0"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>陈</title><meta name=description content><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://Rook1eChan.github.io/index.xml><link rel=alternate type=application/json href=https://Rook1eChan.github.io/index.json><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/"><meta property="og:site_name" content="陈"><meta property="og:title" content="陈"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="陈"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"陈","url":"https://Rook1eChan.github.io/","description":"","logo":"https://Rook1eChan.github.io/icon.png","sameAs":["https://github.com/Rook1eChan"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="陈 (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>陈</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>你好，欢迎来到我的博客 👋</h1></header><div class=entry-content>1234567</div><footer class=entry-footer><div class=social-icons><a href=https://github.com/Rook1eChan target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Article</h2></header><div class=entry-content><p>你好 aaaaa</p></div><footer class=entry-footer><span title='2025-09-22 11:45:39 +0800 +0800'>September 22, 2025</span></footer><a class=entry-link aria-label="post link to Article" href=https://Rook1eChan.github.io/posts/blog/article/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>LumberChunker</h2></header><div class=entry-content><p>EMNLP2024
0.主要贡献 提出了Lumberchunker文本分割方法 提出了GuntenQA数据集 验证了Lumberchunker在下游RAG任务上的效果 1.LumberChunker 使用LLM动态的将文档分割为语义独立的片段。每个片段的长短是不固定的，确保每个片段的语义完整性、独立性。也就是说分割后，每一段包含的语义是完整的，同时与其它段有区别。由LLM来确定合适的分割点，这一决策过程考虑到文本的结构和语义，从而能够创建出大小最优且上下文连贯的片段。
1.先按照paragraph分割目标文档，然后把paragraph顺序连接，直到累计的token数超过一个阈值 $\theta$，形成 $ G_i$。该阈值如何设置后文会说。$\theta$ 应该足够大，防止把具有相关性的段落分开；同时 $\theta$​ 也要足够小，防止过多内容影响LLM进行推理。
2.让LLM寻找 $G_i$ 中“语义断层”的地方，作为分割点。分割点之前即形成一个chunk。剩下的内容继续与paragraph顺序拼接、超过阈值停止、LLM分割……分割整体是串行进行的。
2.GutenQA 数据来源于Project Gutenberg电子图书馆。
1.100本英文书籍，手动提取HTML内容（附录里和NarrativeQA进行了对比，手动提取没有编码错误等问题）
2.使用ChatGPT3.5为每本书生成问题、答案和包含答案的原文片段，人工为每本书筛选30个高质量问题。
问题需要基于给定片段中的具体信息，且不能用书中的其它地方的信息来回答。问题大多以‘what,’ ‘when,’ ‘where’ 开头， ‘why’ and ‘how’较少。
3.原文片段需要简短，以确保任何分块方法都不会把它切开。评估方法是在检索到的文本中精确匹配字符串。
3.Experiments 3.1 propmt的阈值怎么选择 这个阈值就是paragraph顺序连接的阈值 $\theta$​ 。由于是LLM寻找分割点，token过长会影响模型的推理能力。
在不同阈值下使用DCG评估效果。DCG表明了是否检索到，检索结果是否靠前。
3.2 Lumberchunk是否增强了检索效果？ 与其它分块基准进行对比。评估指标为DCG@K、RECALL@K。
此外，注意到semantic chunk和paragraph level的指标并没有随K有效增加，表明其在大规模文档检索方面的局限性。
proposition level的引用在哪？？？
附录F展示了各分割方法的统计结果：
Lumberchunk切分后的块平均长度为334，比预设的550阈值低了40%，这说明LLM有效的对文本进行了切分，而不是持续选择靠近末尾的ID。说明未出现Lost in the Middle现象。
在论文《Lost in the Middle: How Language Models Use Long Contexts》中，作者发现，当针对长文本的不同位置信息设计专门问题，测试大语言模型对不同位置信息的记忆能力时，模型的性能呈现一种 “U 型” 表现，即对于前段与后段的信息有着较强的关注与记忆能力，能较好地解决问题，而对于中段信息的利用则有所逊色。
这种现象的产生可能是由于训练数据中的无意偏差。LLM 的预训练侧重于根据最近的一些 token 预测下一个 token，而在微调过程中，真正的指令又往往位于上下文开始的位置，这在不知不觉中引入了一种立场偏见，让 LLM 认为重要信息总是位于上下文的开头和结尾。
...</p></div><footer class=entry-footer><span title='2025-08-16 21:23:00 +0800 +0800'>August 16, 2025</span></footer><a class=entry-link aria-label="post link to LumberChunker" href=https://Rook1eChan.github.io/posts/lumberchunker/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>论文阅读 | DeepSeek-VL: Towards Real-World Vision-Language Understanding</h2></header><div class=entry-content><p>DeepSeek-VL: Towards Real-World Vision-Language Understanding 0. Abstract 本文的主要贡献：
数据集构建：构建了具有多样性和可扩展性，广泛覆盖真实世界场景的数据集。包括网页截图、PDF文档、OCR文本、图表以及知识型内容（如专家知识、教科书）等。此外，根据真实用户场景将数据进行分类，并据此创建了指令微调数据集。通过该数据集的微调，显著提升了模型在实际应用中的用户体验。
创新的模型架构：采用了混合视觉编码器（hybrid vision encoder），能在固定的token预算下高效处理高分辨率图像（1024*1024），同时保持较低的计算开销。该架构保证模型多种视觉任务中能捕捉到关键的语义和细节信息。
创新的训练策略：既使LLM学会新模态，也保证原有的的语言能力不退化。调控语言和视觉的竞争关系，实现两种模态的均衡融合。
1. Introduction 大语言模型的巨大成功引发了人们对多模态模型的追求。这些模型能同时理解语言和图像，在执行现实世界任务时展现出巨大的潜力。
目前出现了很多开源的VLM方案，在benchmark上表现优秀，但在现实世界中表现不佳。大都存在以下问题（本文的改进方案）：
许多方案将重心放在指令微调阶段。作者认为应当使用大量的视觉-语言数据进行充分预训练。（深度预训练）
现有方案多使用学术上的数据集进行微调，缺乏现实世界经验。（精心构建数据集）
现有方案多采用vision transformer与预训练语言模型结合的方式，这类模型分辨率低，不能胜任OCR或微小物体识别任务。（高分辨率处理架构）
有些模型在长期的多模态训练中会出现语言能力的退化。应采用一种既保留语言能力，又掌握新模态能力的训练方式。（平衡多模态特征的训练策略）
DeepSeek-VL具有通用的多模态理解能力，能够处理逻辑图、网页、公式识别、科学文献、自然图像等。
DeepSeek-VL的优势：
Deepseek-VL的预训练数据涵盖了广泛的世界知识，包括网络爬虫、网页代码、电子书、教育资料、arxiv文章等等，全面覆盖现实世界中的场景，数据质量高，具有广泛性和实用性。同时作者团队还精心设计了指令调优数据集，具体来说，作者从网上收集了GPT-4V和Gemini的真实案例，并进行分类，为每个测试图像选择合适的prompt。该分类体系还用于构建评估数据集。
视觉模块采用混合视觉编码器架构，384$\times$384的文本对齐编码器用于粗粒度语义提取，1024$\times$1024的高分辨率编码器用于捕捉细节视觉信息。两者结合，可以将1024$\times$1024的图像压缩为576个token，在视觉表征和token开销间取得平衡，使视觉模块支持文-图交织处理和多轮推理场景。
为了使多模态模型不出现语言能力的退化：1.保持至少70%的语言数据，这对维护模型内部的语言知识完整性至关重要。2.作者提出了模态预热(modality warm-up)策略。该方法通过在训练过程中动态调整模态比例，逐步引入更多视觉-语言数据。
在迭代模型时，首先在小模型上进行实验。然而，形如1B的小模型在benchmark上难以展现理想性能，无法真实的反映模型的实际表现。因此，作者把评估措施从多选改为了各选项的困惑度（PPL）对比；此外，为避免指令跟随能力成为瓶颈，在预训练阶段我们混合了少量指令调优数据。通过这种方式，我们既能利用1B模型获得合理性能表现，又能更精准地量化实验中每次迭代的影响效果。
2. Data Construction 数据集包括两大模块：VL-Pretrain数据、VL-SFT数据
VL-Pretrain整合了多源视觉文本数据，旨在强化模型的基础跨模态理解能力。
VL-SFT相对较小，主要用于训练模型完成特定下游任务。
在stage1，VL-Pretrain用于预热VL adapter
stage2，VL-Pretrain用于联合预训练VL adaptor和VL model
stage3，使用VL-SFT微调整个模型
2.1 VL-Pretraining Data 分为以下7个类别：
Interleaved image-text data（交错式图文数据，使模型对多模态输入具有更好的上下文学习能力），MMC4、Wiki等
Image caption data（图像描述，包含高质量图-文对），Capsfusion、TaiSu等
Table and chart data（图表数据），Chart2text、Unichart
Web Code data（网页代码，使模型具有从图形界面或图表重建代码的能力。从Stack数据集中的jupyter notebook清洗出2million图像-代码对。最终选择1.1million作为是主要训练集，包括一张图像-至少5行代码）
OCR data（文档光学字符识别数据，作者构建了一个中英混合的OCR数据集，包括两部分：1.arxiv文章 2.电子书和教育材料，来自Anna’s Archive）
Scene text OCR（增强模型识别场景中文本的能力）ArT、MLT-17等。
Text-only corpus（纯文本，和DeepSeek LLM的一致）
2.2 VL-SFT Data 包括多个知名开源数据集ShareGPT4V、LAION-GPTV等。
...</p></div><footer class=entry-footer><span title='2025-07-24 16:00:00 +0800 +0800'>July 24, 2025</span></footer><a class=entry-link aria-label="post link to 论文阅读 | DeepSeek-VL: Towards Real-World Vision-Language Understanding" href=https://Rook1eChan.github.io/posts/deepseekvl/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>博客 | Stop Saying RAG Is Dead</h2></header><div class=entry-content><p>这是一个系列博客，包括五篇文章。博客地址：Stop Saying RAG Is Dead – Hamel’s Blog
作者批驳了“RAG已死”的说法，认为真正被淘汰的是“Chuck documents into a vector database, do cosine similarity, call it a day. ”的过时的RAG。RAG技术仍在进化，在后面的的文章里可以看到在检索、评估等方面上的创新。很高兴看到有人对RAG持积极态度，毕竟在一个有希望的领域进行研究学习更有动力。
五篇文章的简介如下：
标题 内容简介 Part 1: I don’t use RAG, I just retrieve documents Ben Clavié 介绍了RAG的现状 Part 2: Modern IR Evals For RAG 评估是必不可少的步骤，高质量的benchmark有助于我们选择更好的方法。Nandan Thakur （BIER作者）认为传统的IR指标不适合评估RAG的表现，应该采用新的指标 Part 3: Optimizing Retrieval with Reasoning Models Orion Weller 提出了一种能遵循instruct的检索系统，在检索时就进行推理，优于传统的语义检索 Part 4: Late Interaction Models For RAG Antoine Chaffin 介绍了ColBERT这类迟交互、多向量模型 Part 5: RAG with Multiple Representations Bryan Bischof and Ayush Chaurasia 提出，我们需要对不同模态的问题智能化的选用不用的指标 P1: I don’t use RAG, I just retrieve documents 现在有一些说法，认为长上下文窗口的出现使得我们不再需要RAG了。
...</p></div><footer class=entry-footer><span title='2025-07-17 10:07:00 +0800 +0800'>July 17, 2025</span></footer><a class=entry-link aria-label="post link to 博客 | Stop Saying RAG Is Dead" href=https://Rook1eChan.github.io/posts/%E5%8D%9A%E5%AE%A2-stop-saying-rag-is-dead/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>在Autodl中使用LLaMA-Factory进行微调</h2></header><div class=entry-content><p>LLaMA-Factory使用教程
一、环境准备 1.1创建虚拟环境 conda create -n lf python==3.11 conda init 然后重开cmd
conda activate lf 1.2下载相关的包 conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia 验证GPU版本的Pytorch是否成功
python -c "import torch; print(torch.cuda.is_available())" 1.3下载llama factory sudo apt install git 开科学上网
git clone https://github.com/hiyouga/LLaMA-Factory.git 1.4安装依赖 python -m pip install --upgrade pip pip install -r requirements.txt pip install -e ".[torch,metrics]" 如果下载有问题，可以尝试清华源
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e ".[torch,metrics]" 1.5清理pip pip cache purge 二、下载模型 2.1从modelscope下载模型权重文件 pip install modelscope 可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面
...</p></div><footer class=entry-footer><span title='2025-07-17 10:07:00 +0800 +0800'>July 17, 2025</span></footer><a class=entry-link aria-label="post link to 在Autodl中使用LLaMA-Factory进行微调" href=https://Rook1eChan.github.io/posts/llama-factory/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>BERT</h2></header><div class=entry-content><p>参考：https://zhuanlan.zhihu.com/p/403495863
1.介绍 BERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，该模型在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。
BERT的网络架构使用的是《Attention is all you need》中提出的多层Transformer结构。其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。Transformer的结构在NLP领域中已经得到了广泛应用。
2.模型结构 下图展示的是BERT的总体结构图，多个Transformer Encoder一层一层地堆叠起来，就组装成了BERT了，在论文中，作者分别用12层和24层Transformer Encoder组装了两套BERT模型，两套模型的参数总数分别为110M和340M。
BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非像Bi-LSTM那样把句子倒序输入一遍。在BERT之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。
2.1Embedding Embedding由三种Embedding求和而成：
token embedding
将输入的文本进行Word Piece分词，如playing切割成play，##ing，使用Word Piece是为了解决未登录词。tokenization后，在开头插入[CLS]，在每句话的末尾插入[SEP]。[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。
Bert 在处理英文文本时只需要 30522 个词，Token Embeddings 层会将每个词转换成 768 维向量，如下图的例子中， 5 个Token 会被转换成一个 (5, 768) 的矩阵或 (1, 5, 768) 的张量。
segment embedding
用来区分两种句子。bert在训练时包括两种任务，其一是MLM（masked language model，掩码语言模型），就是遮住某个词让模型去预测；其二是NSP（next sentence prediction，下一句预测），输入两个句子，让模型判断是否相关。这时候就需要segment embedding对两个句子做区别。
进行问答等需要预测下一句的任务时，segment embedding层把0赋值给第一个句子的各token，把1赋值给第二个句子的各token。在文本分类任务时，segment embedding全部为0。
position embedding
和transformer的实现不同，不是固定的三角函数，而是可学习的参数。
Transformer 中通过植入关于 Token 的相对位置或者绝对位置信息来表示序列的顺序信息。作者测试用学习的方法来得到 Position Embeddings，最终发现固定位置和相对位置效果差不多，所以最后用的是固定位置的，而正弦可以处理更长的 Sequence，且可以用前面位置的值线性表示后面的位置。
...</p></div><footer class=entry-footer><span title='2025-07-08 17:02:00 +0800 +0800'>July 8, 2025</span></footer><a class=entry-link aria-label="post link to BERT" href=https://Rook1eChan.github.io/posts/bert/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Retrieval_Learning</h2></header><div class=entry-content><p>简介：检索相关算法的学习
1.TF-IDF 1.1原理 TF：term frequency（词频）
IDF：inverse document frequency（逆文档频率）
字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。
$TF=\frac{某个词在文章中出现的次数}{文章的总词数}$
考虑到文章长短不同，除以总词数进行标准化
$IDF=log(\frac{语料库的文章总数}{包含该词的文章数量+1})$
加1防止不存在包含该词的文档时分母为0
$TF-IDF=TF \times IDF$
优点：
简单快速、易理解 缺点：
没考虑词语的语义 仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 1.2自己实现 import math # 计算每句话的词频 def counter(word_list): wordcount = [] for doc in word_list: count = {} for word in doc: count[word] = count.get(word, 0) + 1 wordcount.append(count) return wordcount # 计算tf=某个词在文章中出现的总次数/文章的总词数 def tf(word, word_list): return word_list.get(word) / sum(word_list.values()) # 统计含有该单词的句子数 def count_sentence(word, wordcount): return sum(1 for i in wordcount if i.get(word)) # 计算idf=log(语料库中的文档总数/(包含该词的文档数+1)) def idf(word, wordcount): return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1 # tf-idf=tf*idf def tfidf(word, word_list, wordcount): return tf(word, word_list) * idf(word, wordcount) if __name__ == "__main__": docs = [ "what is the weather like today", "what is for dinner tonight", "this is a question worth pondering", "it is a beautiful day today" ] word_list = [] # 记录每个文档分词后的结果 for doc in docs: word_list.append(doc.split(" ")) # 使用停用词 # stopwords = ["is", "the"] # for i in docs: # all_words = i.split() # new_words = [] # for j in all_words: # if j not in stopwords: # new_words.append(j) # word_list.append(new_words) wordcount = counter(word_list) # 统计每个文档词的次数 for cnt, doc in enumerate(wordcount): print("doc{}".format(cnt)) for word, _ in doc.items(): print("word:{} --- TF-IDF:{}".format(word, tfidf(word, doc, wordcount))) 1.3使用sklearn库 from sklearn.feature_extraction.text import TfidfVectorizer if __name__ == "__main__": docs = [ "what is the weather like today", "what is for dinner tonight", "this is a question worth pondering", "it is a beautiful day today" ] tfidf_vec = TfidfVectorizer() # 利用fit_transform得到TFIDF矩阵 tfidf_matrix = tfidf_vec.fit_transform(docs) # 利用get_feature_names_out得到不重复的单词 print(tfidf_vec.get_feature_names_out()) # 利用vocabulary_得到各单次的编号 print(tfidf_vec.vocabulary_) # 输出TFIDF矩阵，即每个文档中每个词的tfidf值 print(tfidf_matrix) 2.BM25 2.1原理 BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。
...</p></div><footer class=entry-footer><span title='2025-07-07 10:07:00 +0800 +0800'>July 7, 2025</span></footer><a class=entry-link aria-label="post link to Retrieval_Learning" href=https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Inference Scaling for Long-Context Retrieval Augmented Generation</h2></header><div class=entry-content><p>ICLR2025，来自Google DeepMind团队的工作
https://arxiv.org/abs/2410.04343v2
0.目标 先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。
目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。
1.贡献 提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。 得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。 根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。 2.相关工作 2.1长上下文LLMs 早期采用稀疏/低秩核来减少内存需求。
I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.
...</p></div><footer class=entry-footer><span title='2025-05-07 23:04:00 +0800 +0800'>May 7, 2025</span></footer><a class=entry-link aria-label="post link to Inference Scaling for Long-Context Retrieval Augmented Generation" href=https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models</h2></header><div class=entry-content><p>1.Motivation 尽管有了RAG的帮助，LLM仍有可能给出与所提供知识不符的回答。因此需要构建一个数据集来检测幻觉。
2.Contributions 提出RAGTruth，一个大规模词级别的幻觉检测数据集，由LLM自然产生（作者认为故意触发的幻觉与自然产生的幻觉存在差异） 对现有幻觉检测方法进行比较 提出了微调LLM用于幻觉检测的基线。Llama-2-13B在RAGTruth training data上微调后比得上GPT4 证明了使用微调得到的幻觉检测器，能降低幻觉 3.Related Work 4.Methods 1.Hallucination Taxonomy幻觉类型 本文将幻觉类型分为：
Evident Conflict明显冲突：与提供的文本明显相反，容易辨别，如事实错误、拼写错误、数字错误。 Subtle Conflict轻微冲突：生成的信息与提供的文本有歧义，比如术语的替换，需要结合上下文判断。 Evident Introduction of Baseless Information明显引入无根据知识：生成的内容不在提供的信息之内。 Subtle Introduction of Baseless Information轻微引入无根据知识：生成内容超出了提供的信息，比如主观的假设或推断。 2.Response Generation回答生成 选择三个任务: Question Answering,Data-to-text Writing, and News Summarization.（问题回答、数据到文本的写作、新闻摘要），生成回答并人工标注幻觉部分。
Question Answering：从MS MARCO选择与生活相关的QA，每个问题保留三段提取内容，然后使用LLM根据内容回答问题。 Data-to-text Writing：从Yelp数据集选择有关商家的结构化信息和用户的评论，用LLM生成对商家的描述。如果数据出现空值而大模型将其解释为“假”，认为这是出现了幻觉。 News Summarization：数据来自CNN/Daily Mail dataset+某新闻平台的新闻，使用LLM对每篇内容生成摘要。 使用的LLM：GPT-3.5-turbo-0613、GPT-4-0613、Mistral-7b-Instruct、Llama-2-7B-chat、 Llama-2-13B-chat、 Llama-2-70B-chat
每个任务都用6个模型跑一遍，得到6个回答。
5.Result 各项任务中幻觉类型的比例：
如图2所示，在上下文中无根据的信息生成显著多于与上下文冲突的信息生成，尤其是在问答任务中。在两大类无根据信息和冲突信息中，更严重的幻觉，即明显的无根据信息和明显的冲突信息，占据了相当大的比例。这一观察结果说明即使有RAG，还是存在严重幻觉。
数据转文本的任务幻觉率最高，可能与JSON格式有关。另外，较新的新闻的幻觉率不比过时新闻高，可能是由于较新的新闻的文本长度较短。
各模型出现幻觉的比例：
（span、density什么意思）
表3显示，在我们收集的数据中，OpenAI的两个模型表现出显著较低的幻觉率。具体来说，GPT-4-0613的幻觉频率最低。为了更清晰地比较不同模型的幻觉率，我们计算了每个模型在三个任务中的幻觉密度。幻觉密度定义为每一百个单词响应中平均出现的幻觉跨度数。在Llama2系列中，除了数据总文本写作任务外，模型规模与幻觉密度之间存在明显的负相关关系。尽管Mistral-7B-Instruct模型在各种基准和排行榜上的表现强劲（Zheng等人，2023），但它生成的包含幻觉的回答数量最多。
幻觉与文本长度的关系：
对于上下文长度（CLB），只有新闻摘要呈现出上下文越长，越容易幻觉的特点。
对于回答长度（RLB），都有回答越长，越容易幻觉的特点。
幻觉与位置的关系：
在问答和新闻摘要任务中，幻觉更倾向于出现在回答的末尾。数据到文本写作任务在前半部分较易出现幻觉。</p></div><footer class=entry-footer><span title='2025-05-07 15:49:00 +0800 +0800'>May 7, 2025</span></footer><a class=entry-link aria-label="post link to RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models" href=https://Rook1eChan.github.io/posts/ragtruth/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>RAG,QA相关数据集及评价标准</h2></header><div class=entry-content><p>RAG，QA常用的数据集和评价标准 多为知识密集型、问答型数据集
数据集 1.UltraDomain lightrag曾使用
使用MemoRAG提出的Benchmark。
在UltraDomain里，包含多个领域的数据，每个数据包括多本书。以cs为例，共含有100本书和100个对应的问题。该领域专注于计算机科学，涵盖数据科学和软件工程的关键领域。它特别强调机器学习和大数据处理，内容涉及推荐系统、分类算法以及使用Spark进行实时分析。：
{ input: How does Spark Streaming enable real-time data processing? answers: ['Spark Streaming extends ...... '] context: "Whole Book......" length: 131651 context_id: 7bcef8714a477fd61fc8fb0d499b2cc3 _id: b2fd8d9c6d1499d521d778ce3d6d06fa label: cs meta: {'title': 'Machine Learning With Spark', 'authors': 'Nick Pentreath'} } 数据集地址：TommyChien/UltraDomain · Datasets at Hugging Face
Lightrag使用LLM生成问题-答案对
生成问题的方法来自于From Local to Global: A Graph RAG Approach to Query-Focused Summarization
提供文本，让大模型生成K个使用该数据集的用户身份（比如数据集是财经新闻，user就可能是收集金融市场趋势的财经记者），对于每个用户再生成N个任务，每个用户-任务提出M个高层次问题（理解整个数据集、无需提取具体事实）
User: A tech journalist looking for insights and trends in the tech industry Task: Understanding how tech leaders view the role of policy and regulation Questions: 1. Which episodes deal primarily with tech policy and government regulation? 2. How do guests perceive the impact of privacy laws on technology development? 3. Do any guests discuss the balance between innovation and ethical considerations? 4. What are the suggested changes to current policies mentioned by the guests? 5. Are collaborations between tech companies and governments discussed and how? 2.DAPR使用的数据集 MS MARCO、Natural Questions、MIRACL、Genomics 和 ConditionalQA
...</p></div><footer class=entry-footer><span title='2025-05-04 09:12:03 +0800 +0800'>May 4, 2025</span></footer><a class=entry-link aria-label="post link to RAG,QA相关数据集及评价标准" href=https://Rook1eChan.github.io/posts/ragqa-%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%84%E4%BC%B0/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://Rook1eChan.github.io/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>陈</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>