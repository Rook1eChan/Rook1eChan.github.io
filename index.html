<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.148.1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>Chan's Blog</title><meta name=description content><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.7f5d6d31e606c3178e091cb55298baed021a501ad4c10fd725847674935b1b15.css integrity="sha256-f11tMeYGwxeOCRy1Upi67QIaUBrUwQ/XJYR2dJNbGxU=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://Rook1eChan.github.io/index.xml><link rel=alternate type=application/json href=https://Rook1eChan.github.io/index.json><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/"><meta property="og:site_name" content="Chan's Blog"><meta property="og:title" content="Chan's Blog"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Chan's Blog"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Chan's Blog","url":"https://Rook1eChan.github.io/","description":"","logo":"https://Rook1eChan.github.io/apple-touch-icon.png","sameAs":["https://github.com/Rook1eChan"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="Chan's Blog (Alt + H)"><img src=https://Rook1eChan.github.io/apple-touch-icon.png alt aria-label=logo height=35>Chan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>你好，欢迎来到我的博客 👋</h1></header><div class=entry-content>welcome!</div><footer class=entry-footer><div class=social-icons><a href=https://github.com/Rook1eChan target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>RAG 数据集及评价标准</h2></header><div class=entry-content><p>1.使用LLM生成问题及答案
2.使用标注数据集
1.LightRAG 数据集 使用MemoRAG提出的Benchmark。
在UltraDomain里，包含多个领域的数据，每个数据包括多本书。以cs为例，共含有100本书和100个对应的问题。该领域专注于计算机科学，涵盖数据科学和软件工程的关键领域。它特别强调机器学习和大数据处理，内容涉及推荐系统、分类算法以及使用Spark进行实时分析。：
{ input: How does Spark Streaming enable real-time data processing? answers: ['Spark Streaming extends ...... '] context: "Whole Book......" length: 131651 context_id: 7bcef8714a477fd61fc8fb0d499b2cc3 _id: b2fd8d9c6d1499d521d778ce3d6d06fa label: cs meta: {'title': 'Machine Learning With Spark', 'authors': 'Nick Pentreath'} } 数据集地址：TommyChien/UltraDomain · Datasets at Hugging Face
问题生成 生成问题的方法来自于From Local to Global: A Graph RAG Approach to Query-Focused Summarization
提供文本，让大模型生成K个使用该数据集的用户身份（比如数据集是财经新闻，user就可能是收集金融市场趋势的财经记者），对于每个用户再生成N个任务，每个用户-任务提出M个高层次问题（理解整个数据集、无需提取具体事实）
User: A tech journalist looking for insights and trends in the tech industry Task: Understanding how tech leaders view the role of policy and regulation Questions: 1. Which episodes deal primarily with tech policy and government regulation? 2. How do guests perceive the impact of privacy laws on technology development? 3. Do any guests discuss the balance between innovation and ethical considerations? 4. What are the suggested changes to current policies mentioned by the guests? 5. Are collaborations between tech companies and governments discussed and how? 评价标准 不使用黄金标准答案，使用LLM评价。包括
...</p></div><footer class=entry-footer><span title='2025-05-04 09:12:03 +0800 +0800'>May 4, 2025</span></footer><a class=entry-link aria-label="post link to RAG 数据集及评价标准" href=https://Rook1eChan.github.io/posts/rag-%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%84%E4%BC%B0/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>博客 | Stop Saying RAG Is Dead</h2></header><div class=entry-content><p>这是一个系列博客，包括五篇文章。博客地址：Stop Saying RAG Is Dead – Hamel’s Blog
作者批驳了“RAG已死”的说法，认为真正被淘汰的是“Chuck documents into a vector database, do cosine similarity, call it a day. ”的过时的RAG。RAG技术仍在进化，在后面的的文章里可以看到在检索、评估等方面上的创新。很高兴看到有人对RAG持积极态度，毕竟在一个有希望的领域进行研究学习更有动力。
五篇文章的简介如下：
标题 内容简介 Part 1: I don’t use RAG, I just retrieve documents Ben Clavié 介绍了RAG的现状 Part 2: Modern IR Evals For RAG 评估是必不可少的步骤，高质量的benchmark有助于我们选择更好的方法。Nandan Thakur （BIER作者）认为传统的IR指标不适合评估RAG的表现，应该采用新的指标 Part 3: Optimizing Retrieval with Reasoning Models Orion Weller 提出了一种能遵循instruct的检索系统，在检索时就进行推理，优于传统的语义检索 Part 4: Late Interaction Models For RAG Antoine Chaffin 介绍了ColBERT这类迟交互、多向量模型 Part 5: RAG with Multiple Representations Bryan Bischof and Ayush Chaurasia 提出，我们需要对不同模态的问题智能化的选用不用的指标 P1: I don’t use RAG, I just retrieve documents 现在有一些说法，认为长上下文窗口的出现使得我们不再需要RAG了。
...</p></div><footer class=entry-footer><span title='2025-07-17 10:07:00 +0800 +0800'>July 17, 2025</span></footer><a class=entry-link aria-label="post link to 博客 | Stop Saying RAG Is Dead" href=https://Rook1eChan.github.io/posts/%E5%8D%9A%E5%AE%A2-stop-saying-rag-is-dead/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>BERT</h2></header><div class=entry-content><p>参考：https://zhuanlan.zhihu.com/p/403495863
1.介绍 BERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，该模型在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。
BERT的网络架构使用的是《Attention is all you need》中提出的多层Transformer结构。其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。Transformer的结构在NLP领域中已经得到了广泛应用。
2.模型结构 下图展示的是BERT的总体结构图，多个Transformer Encoder一层一层地堆叠起来，就组装成了BERT了，在论文中，作者分别用12层和24层Transformer Encoder组装了两套BERT模型，两套模型的参数总数分别为110M和340M。
BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非像Bi-LSTM那样把句子倒序输入一遍。在BERT之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。
2.1Embedding Embedding由三种Embedding求和而成：
token embedding
将输入的文本进行Word Piece分词，如playing切割成play，##ing，使用Word Piece是为了解决未登录词。tokenization后，在开头插入[CLS]，在每句话的末尾插入[SEP]。[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。
Bert 在处理英文文本时只需要 30522 个词，Token Embeddings 层会将每个词转换成 768 维向量，如下图的例子中， 5 个Token 会被转换成一个 (5, 768) 的矩阵或 (1, 5, 768) 的张量。
segment embedding
用来区分两种句子。bert在训练时包括两种任务，其一是MLM（masked language model，掩码语言模型），就是遮住某个词让模型去预测；其二是NSP（next sentence prediction，下一句预测），输入两个句子，让模型判断是否相关。这时候就需要segment embedding对两个句子做区别。
进行问答等需要预测下一句的任务时，segment embedding层把0赋值给第一个句子的各token，把1赋值给第二个句子的各token。在文本分类任务时，segment embedding全部为0。
position embedding
和transformer的实现不同，不是固定的三角函数，而是可学习的参数。
Transformer 中通过植入关于 Token 的相对位置或者绝对位置信息来表示序列的顺序信息。作者测试用学习的方法来得到 Position Embeddings，最终发现固定位置和相对位置效果差不多，所以最后用的是固定位置的，而正弦可以处理更长的 Sequence，且可以用前面位置的值线性表示后面的位置。
...</p></div><footer class=entry-footer><span title='2025-07-08 17:02:00 +0800 +0800'>July 8, 2025</span></footer><a class=entry-link aria-label="post link to BERT" href=https://Rook1eChan.github.io/posts/bert/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Retrieval_Learning</h2></header><div class=entry-content><p>简介：检索相关算法的学习
1.TF-IDF 1.1原理 TF：term frequency（词频）
IDF：inverse document frequency（逆文档频率）
字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。
$TF=\frac{某个词在文章中出现的次数}{文章的总词数}$
考虑到文章长短不同，除以总词数进行标准化
$IDF=log(\frac{语料库的文章总数}{包含该词的文章数量+1})$
加1防止不存在包含该词的文档时分母为0
$TF-IDF=TF \times IDF$
优点：
简单快速、易理解 缺点：
没考虑词语的语义 仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 1.2自己实现 import math # 计算每句话的词频 def counter(word_list): wordcount = [] for doc in word_list: count = {} for word in doc: count[word] = count.get(word, 0) + 1 wordcount.append(count) return wordcount # 计算tf=某个词在文章中出现的总次数/文章的总词数 def tf(word, word_list): return word_list.get(word) / sum(word_list.values()) # 统计含有该单词的句子数 def count_sentence(word, wordcount): return sum(1 for i in wordcount if i.get(word)) # 计算idf=log(语料库中的文档总数/(包含该词的文档数+1)) def idf(word, wordcount): return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1 # tf-idf=tf*idf def tfidf(word, word_list, wordcount): return tf(word, word_list) * idf(word, wordcount) if __name__ == "__main__": docs = [ "what is the weather like today", "what is for dinner tonight", "this is a question worth pondering", "it is a beautiful day today" ] word_list = [] # 记录每个文档分词后的结果 for doc in docs: word_list.append(doc.split(" ")) # 使用停用词 # stopwords = ["is", "the"] # for i in docs: # all_words = i.split() # new_words = [] # for j in all_words: # if j not in stopwords: # new_words.append(j) # word_list.append(new_words) wordcount = counter(word_list) # 统计每个文档词的次数 for cnt, doc in enumerate(wordcount): print("doc{}".format(cnt)) for word, _ in doc.items(): print("word:{} --- TF-IDF:{}".format(word, tfidf(word, doc, wordcount))) 1.3使用sklearn库 from sklearn.feature_extraction.text import TfidfVectorizer if __name__ == "__main__": docs = [ "what is the weather like today", "what is for dinner tonight", "this is a question worth pondering", "it is a beautiful day today" ] tfidf_vec = TfidfVectorizer() # 利用fit_transform得到TFIDF矩阵 tfidf_matrix = tfidf_vec.fit_transform(docs) # 利用get_feature_names_out得到不重复的单词 print(tfidf_vec.get_feature_names_out()) # 利用vocabulary_得到各单次的编号 print(tfidf_vec.vocabulary_) # 输出TFIDF矩阵，即每个文档中每个词的tfidf值 print(tfidf_matrix) 2.BM25 2.1原理 BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。
...</p></div><footer class=entry-footer><span title='2025-07-07 10:07:00 +0800 +0800'>July 7, 2025</span></footer><a class=entry-link aria-label="post link to Retrieval_Learning" href=https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Inference Scaling for Long-Context Retrieval Augmented Generation</h2></header><div class=entry-content><p>ICLR2025，来自Google DeepMind团队的工作
https://arxiv.org/abs/2410.04343v2
0.目标 先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。
目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。
1.贡献 提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。 得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。 根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。 2.相关工作 2.1长上下文LLMs 早期采用稀疏/低秩核来减少内存需求。
I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.
...</p></div><footer class=entry-footer><span title='2025-05-07 23:04:00 +0800 +0800'>May 7, 2025</span></footer><a class=entry-link aria-label="post link to Inference Scaling for Long-Context Retrieval Augmented Generation" href=https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models</h2></header><div class=entry-content><p>1.Motivation 尽管有了RAG的帮助，LLM仍有可能给出与所提供知识不符的回答。因此需要构建一个数据集来检测幻觉。
2.Contributions 提出RAGTruth，一个大规模词级别的幻觉检测数据集，由LLM自然产生（作者认为故意触发的幻觉与自然产生的幻觉存在差异） 对现有幻觉检测方法进行比较 提出了微调LLM用于幻觉检测的基线。Llama-2-13B在RAGTruth training data上微调后比得上GPT4 证明了使用微调得到的幻觉检测器，能降低幻觉 3.Related Work 4.Methods 1.Hallucination Taxonomy幻觉类型 本文将幻觉类型分为：
Evident Conflict明显冲突：与提供的文本明显相反，容易辨别，如事实错误、拼写错误、数字错误。 Subtle Conflict轻微冲突：生成的信息与提供的文本有歧义，比如术语的替换，需要结合上下文判断。 Evident Introduction of Baseless Information明显引入无根据知识：生成的内容不在提供的信息之内。 Subtle Introduction of Baseless Information轻微引入无根据知识：生成内容超出了提供的信息，比如主观的假设或推断。 2.Response Generation回答生成 选择三个任务: Question Answering,Data-to-text Writing, and News Summarization.（问题回答、数据到文本的写作、新闻摘要），生成回答并人工标注幻觉部分。
Question Answering：从MS MARCO选择与生活相关的QA，每个问题保留三段提取内容，然后使用LLM根据内容回答问题。 Data-to-text Writing：从Yelp数据集选择有关商家的结构化信息和用户的评论，用LLM生成对商家的描述。如果数据出现空值而大模型将其解释为“假”，认为这是出现了幻觉。 News Summarization：数据来自CNN/Daily Mail dataset+某新闻平台的新闻，使用LLM对每篇内容生成摘要。 使用的LLM：GPT-3.5-turbo-0613、GPT-4-0613、Mistral-7b-Instruct、Llama-2-7B-chat、 Llama-2-13B-chat、 Llama-2-70B-chat
每个任务都用6个模型跑一遍，得到6个回答。
5.Result 各项任务中幻觉类型的比例：
如图2所示，在上下文中无根据的信息生成显著多于与上下文冲突的信息生成，尤其是在问答任务中。在两大类无根据信息和冲突信息中，更严重的幻觉，即明显的无根据信息和明显的冲突信息，占据了相当大的比例。这一观察结果说明即使有RAG，还是存在严重幻觉。
数据转文本的任务幻觉率最高，可能与JSON格式有关。另外，较新的新闻的幻觉率不比过时新闻高，可能是由于较新的新闻的文本长度较短。
各模型出现幻觉的比例：
（span、density什么意思）
表3显示，在我们收集的数据中，OpenAI的两个模型表现出显著较低的幻觉率。具体来说，GPT-4-0613的幻觉频率最低。为了更清晰地比较不同模型的幻觉率，我们计算了每个模型在三个任务中的幻觉密度。幻觉密度定义为每一百个单词响应中平均出现的幻觉跨度数。在Llama2系列中，除了数据总文本写作任务外，模型规模与幻觉密度之间存在明显的负相关关系。尽管Mistral-7B-Instruct模型在各种基准和排行榜上的表现强劲（Zheng等人，2023），但它生成的包含幻觉的回答数量最多。
幻觉与文本长度的关系：
对于上下文长度（CLB），只有新闻摘要呈现出上下文越长，越容易幻觉的特点。
对于回答长度（RLB），都有回答越长，越容易幻觉的特点。
幻觉与位置的关系：
在问答和新闻摘要任务中，幻觉更倾向于出现在回答的末尾。数据到文本写作任务在前半部分较易出现幻觉。</p></div><footer class=entry-footer><span title='2025-05-07 15:49:00 +0800 +0800'>May 7, 2025</span></footer><a class=entry-link aria-label="post link to RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models" href=https://Rook1eChan.github.io/posts/ragtruth/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation</h2></header><div class=entry-content><p>CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation
只挂了arxiv，粗看，了解一下各模块的实现方式。
1.Motivation 现有的RAG框架主要是静态检索，且依赖于语义相似性和关联性，这些方法优先考虑主题相关的文档，而并非提供解释或因果关系的文档。这导致响应结果是基于事实的，但是没能理解因果关系。
此外，基于大规模观察语料库训练的语言模型倾向于建模共现模式而非因果依赖，这使得它们容易将相关性与因果关系混淆——尤其是在存在不完整或模糊证据的情况下。这些局限性在多跳检索中尤为明显。
另外，用户的提问可能是模糊的，现有机制缺乏动态适应和因果机制。
2.Contributions 本文提出了CDFRAG框架，将强化学习查询优化、多跳因果图检索和基于对齐的幻觉检测整合到一个推理循环中。
证明了基于强化学习的查询重写显著提升了多跳因果推理和检索质量，优于先前的细化方法。
本方法在四个数据集中均sota，在因果正确性、一致性和可解释性方面均有所改进。
3.Method 1.构建因果知识图谱 使用UniCausal提取因果对（Causal，Effect）。经过GPT4验证后，编码为（C，E，Relation）并存入有向图G。
2.根据强化学习进行查询重写 给定用户初始查询q，重写q的过程是一个马尔可夫决策过程（MDP），有三种操作：
扩展：添加相关的因果因素 简化：去除多余的细节 分解：复杂查询拆解为子查询 策略通过SFT微调生成，然后使用PPO优化。
监督微调（Supervised Fine-Tuning, SFT） 目的：用标注的示范数据（如人工修正的样本）初始化策略 $$\pi_\theta(a|s)$$，使其初步具备期望的行为模式。 方法：通过最大化对数似然来微调模型参数，损失函数为： $$L_{\text{SFT}} = -\sum_{t=1}^{T} \log P_\phi(y_t \mid y_{...</p></div><footer class=entry-footer><span title='2025-05-02 23:17:00 +0800 +0800'>May 2, 2025</span></footer><a class=entry-link aria-label="post link to CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation" href=https://Rook1eChan.github.io/posts/cdf-rag-causal-dynamic-feedback-for-adaptive-retrieval-augmented-generation/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>DAPR A Benchmark on Document-Aware Passage Retrieval</h2></header><div class=entry-content><p>1.Motivation 现有的神经检索（neural retrieval）的方法主要集中在短文本排序，在长篇文章中做检索效果并不好（由于自注意力机制token数量的限制；或者返回的文档过长，不便于用户使用）。另外，作者发现在先进检索器的检索错误中，半数错误与缺少上下文有关。
比如：在A剧场中演出过的演员有哪些？如果只检索关键字“A剧场”，可能找不到答案，需要结合上下文找到“……在这里演出过……”的内容才是真正答案。
因此，作者针对上下文强关联的任务建立了一个数据集，使用两类方法（hybrid retrieval with BM25、 contextualized passage representations）进行实验，并详细解释了实验结果。
2.Related work Document Question Answering（DocQA）：要求模型回答关于输入文档的问题，通常假设文档在提问前就已给出。本文提出的(Document-Awarepassage Retrieval, DAPR)与DocQA类似，区别在于DAPR希望用户提问时不知道目标文档，由模型来寻找目标文档。 Long-document retrieval（长文档检索）：对于长文档检索有一些简单的方法：将文档中段落相关性的最大值作为文档的相关性（MaxP）；仅编码文档中的第一个段落（FirstP）……与DAPR相比，所有这些先前的工作都没有研究如何在考虑文档上下文的情况下检索段落。 Hybrid retrieval（混合检索）：对于一个查询使用多个检索系统（常常是BM25+神经检索） rank fusion（排名融合）——通过凸组合、互逆排名等方法将不同检索系统的个体排名合并为一个。 hierarchical retrieval（层次检索）——首先检索文档，然后从这些文档中检索段落。只适用于段落本身足以对查询做出响应的情况。 本文探讨了段落排名和文档排名结合的有效性。 Relation to pre-training tasks（和预训练任务的关系）：有的研究在预训练中加入上下文。但推理时仍然只关注独立的段落。 补充： NQ：谷歌的一个问答数据集 NDCG：评价检索序列的相关性和位置 共指信息：描述文本中不同表达式指向同一实体或概念的语言现象，如*“玛丽打开了门，她随后拿起包。”* → “她”与“玛丽”共指同一人。 共指消解（Coreference Resolution）：自动识别文本中所有指向同一实体的表达式并分组。 3.Method DAPR任务要求系统提取+排序。给出段落集合$C$，文档集合$D$，对于查询集合$q \in Q$，检索系统$s$应该提取出最好的$K$个段落集合$R$。
3.1NQ-Hard 对SOTA的检索器（DRAGON+,SPLADEv2, and ColBERTv2)使用NQ数据集，发现一半的错误来自于不了解上下文。将这些数据命名为NQ-hard，并分为4类：
共指消解（CR）：关键的共指信息需要通过特定文档上下文来解析； 主要主题（MT）：只有了解文档的背景主题（通常是标题），才能回答查询； 多跳推理（MHR）：连接查询和查询相关段落中的实体的推理路径包括文档上下文中的其他节点； 缩写（AC）：在相关段落（或查询）中出现一个缩写，该缩写对应于查询（或相关段落）中的全称，文档上下文解释了这种映射； 3.2Datasets MS MARCO、Natural Questions、MIRACL、Genomics 和 ConditionalQA（具体处理方式见附录A）有语料库的直接用，没有的把黄金段落文本收集起来当语料库。（也是很神奇）
3.3Evaluation 使用nDCG@10和recall@100做指标。
将binary/3-scale转换为0-1/0-1-2，然后使用pytrec_eval计算指标。
考虑到现实世界中的检索系统多用于零样本、跨领域的情景，本文进行了一项测试：在MS MARCO训练集训练，然后在MS MARCO测试集测试，作为域内评估；在其它四个数据集上测试，作为域外评估。
4.Experiments 4.1基础检索器 BM25（使用PySerini的默认配置） neural retrievers：DRAGON+、SPLADEv2、ColBERTv2（在MS MARCO上训练） 4.2两种将上下文引入神经检索器的方法 4.2.1加入BM25的混合检索 （1）Rank fusion融合检索
...</p></div><footer class=entry-footer><span title='2025-05-02 16:41:03 +0800 +0800'>May 2, 2025</span></footer><a class=entry-link aria-label="post link to DAPR A Benchmark on Document-Aware Passage Retrieval" href=https://Rook1eChan.github.io/posts/dapr-a-benchmark-on-document-aware-passage-retrieval/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>从0开始建立Github个人博客(hugo&amp;PaperMod)</h2></header><div class=entry-content><p>github提供给每个用户一个网址，用户可以建立自己的静态网站。
一、Hugo hugo是一个快速搭建网站的工具，由go语言编写。
1.安装hugo 到hugo的github标签页Tags · gohugoio/hugo选择一个版本，下载对应的安装包。比如hugo_extended_withdeploy_0.147.0_windows-amd64.zip。
解压后，在根目录打开cmd，输入
hugo new site YourSiteName 为你的网站建立文件夹。YourSiteName更改为你的网站的名字。 根目录会出现YourSiteName文件夹。
3.将根目录的hugo.exe复制到YourSiteName里。 在YourSiteName文件夹里打开cmd，输入
hugo server -D 会返回如下信息：
| EN -------------------+----- Pages | 11 Paginator pages | 0 Non-page files | 0 Static files | 0 Processed images | 0 Aliases | 2 Cleaned | 0 Built in 79 ms Environment: "development" Serving pages from disk Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 在浏览器中输入http://localhost:1313/，显示Page Not Found，说明服务器正常运行，但是此时网站还没有页面。
...</p></div><footer class=entry-footer><span title='2025-05-02 12:39:00 +0800 +0800'>May 2, 2025</span></footer><a class=entry-link aria-label="post link to 从0开始建立Github个人博客(hugo&PaperMod)" href=https://Rook1eChan.github.io/posts/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%BB%BA%E7%AB%8Bgithub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugopapermod/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Neural-IR Models（博客）</h2></header><div class=entry-content><p>原文：Neural-IR Models.. Neural IR(Information Retrieval) is a… | by Muhammad Hammad Khan | Medium
译文：【翻译】一文详解神经信息检索领域的最新进展 - 知乎
神经信息检索(Neural Information Retrieval, Neural IR)是信息检索领域的一个重要研究课题。自从谷歌在2018年发布BERT以来，它在11个NLP任务上获得了最先进的结果，一举改变了整个NLP领域的研究范式。2019年1月，Nogueira和Cho在MS MARCO Passage Ranking测试集上首次使用BERT。从那时起，人们开始研究神经信息检索的范式，也提出了许多基于BERT的文本排序方法。这些方法用于多阶段搜索架构的重排阶段(Re-Ranker)。如下图所示。
Figure1 展示了一个简化的多阶段搜索结构。第一步：倒排索引（Inverted Index）+BM25得分进行排序，得到topK文档，这一步也叫候选项生成（Candidates Generation）。第二步，通过基于BERT的上下文排序模型来确定前N个文档的最终排序。
神经重排模型(Neural re-ranking models)一般可以分为以下四种，如Figure2所示：
基于表征(representation-focused) 基于交互(interaction-focused) 全交互（也被称作交叉编码器,）(all-to-all interaction(cross encoder) ) 迟交互(late interaction) 1.基于表征——双塔模型(Bi-encoder Models) 双塔模型将Query和Doc分别表征为密集的向量嵌入，用向量相似度分数来估计Q和D的相关性。在训练时需要正负样本进行对比学习，因为如果只给模型看正样本，它会偷懒——把所有向量都变成一样的，这样“相似度”永远最高。负样本强迫模型学会区分相关和不相关的内容。
在将模型训练好后，doc和query的表征可以独立进行，不用像交叉编码器那样每次都要把Query和Doc拼在一起重新计算。
1.1密集段落检索器(Dense passage retriever, DPR) 论文：Dense Passage Retrieval for Open-Domain Question Answering EMNLP 2020, Facebook Research Code: github.com/facebookresearch/DPR 讲解博客：【IR 论文】DPR — 最早提出使用嵌入向量来检索文档的模型_dpr模型-CSDN博客
DPR是一个应用于问答领域的双塔模型，旨在最大限度地提高查询与相关文档的相似度，同时最小化与非相关文档的相似度。DPR是RAG中R的经典方案。
正样本往往数据集已给定，而负样本比较难选择。为此，DPR提出了一种Batch内负采样的技术，从同一批次的其他样本中选择样本作为负样本。这种方法是有效且高效的。
1.2最近邻负对比估计 (Approximate nearest neighbour Negative Contrastive Estimation, ANCE) 该论文证明了强负样本能够加速模型收敛，提升模型性能。负样本分为易区别的和不易区别的，显然不易区别（即强负样本）的对模型学习帮助更大。本文使用ANN寻找强负样本。
...</p></div><footer class=entry-footer><span title='2025-05-02 01:18:03 +0800 +0800'>May 2, 2025</span></footer><a class=entry-link aria-label="post link to Neural-IR Models（博客）" href=https://Rook1eChan.github.io/posts/neural-ir-models/></a></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>Chan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>