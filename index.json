[{"content":"你好 aaaaa ​\n​\n123123\n123\n","permalink":"https://Rook1eChan.github.io/posts/blog/article/","summary":"\u003cp\u003e你好 aaaaa\n\u003cimg alt=\"alt\" loading=\"lazy\" src=\"../pic/123.png\"\u003e\u003c/p\u003e\n\u003cp\u003e​\u003c/p\u003e\n\u003cp\u003e​\u003c/p\u003e\n\u003cp\u003e123123\u003c/p\u003e\n\u003cp\u003e123\u003c/p\u003e","title":"Article"},{"content":"EMNLP2024\n0.主要贡献 提出了Lumberchunker文本分割方法 提出了GuntenQA数据集 验证了Lumberchunker在下游RAG任务上的效果 1.LumberChunker 使用LLM动态的将文档分割为语义独立的片段。每个片段的长短是不固定的，确保每个片段的语义完整性、独立性。也就是说分割后，每一段包含的语义是完整的，同时与其它段有区别。由LLM来确定合适的分割点，这一决策过程考虑到文本的结构和语义，从而能够创建出大小最优且上下文连贯的片段。\n1.先按照paragraph分割目标文档，然后把paragraph顺序连接，直到累计的token数超过一个阈值 $\\theta$，形成 $ G_i$。该阈值如何设置后文会说。$\\theta$ 应该足够大，防止把具有相关性的段落分开；同时 $\\theta$​ 也要足够小，防止过多内容影响LLM进行推理。\n2.让LLM寻找 $G_i$ 中“语义断层”的地方，作为分割点。分割点之前即形成一个chunk。剩下的内容继续与paragraph顺序拼接、超过阈值停止、LLM分割……分割整体是串行进行的。\n2.GutenQA 数据来源于Project Gutenberg电子图书馆。\n1.100本英文书籍，手动提取HTML内容（附录里和NarrativeQA进行了对比，手动提取没有编码错误等问题）\n2.使用ChatGPT3.5为每本书生成问题、答案和包含答案的原文片段，人工为每本书筛选30个高质量问题。\n问题需要基于给定片段中的具体信息，且不能用书中的其它地方的信息来回答。问题大多以‘what,’ ‘when,’ ‘where’ 开头， ‘why’ and ‘how’较少。\n3.原文片段需要简短，以确保任何分块方法都不会把它切开。评估方法是在检索到的文本中精确匹配字符串。\n3.Experiments 3.1 propmt的阈值怎么选择 这个阈值就是paragraph顺序连接的阈值 $\\theta$​ 。由于是LLM寻找分割点，token过长会影响模型的推理能力。\n在不同阈值下使用DCG评估效果。DCG表明了是否检索到，检索结果是否靠前。\n3.2 Lumberchunk是否增强了检索效果？ 与其它分块基准进行对比。评估指标为DCG@K、RECALL@K。\n此外，注意到semantic chunk和paragraph level的指标并没有随K有效增加，表明其在大规模文档检索方面的局限性。\nproposition level的引用在哪？？？\n附录F展示了各分割方法的统计结果：\nLumberchunk切分后的块平均长度为334，比预设的550阈值低了40%，这说明LLM有效的对文本进行了切分，而不是持续选择靠近末尾的ID。说明未出现Lost in the Middle现象。\n在论文《Lost in the Middle: How Language Models Use Long Contexts》中，作者发现，当针对长文本的不同位置信息设计专门问题，测试大语言模型对不同位置信息的记忆能力时，模型的性能呈现一种 “U 型” 表现，即对于前段与后段的信息有着较强的关注与记忆能力，能较好地解决问题，而对于中段信息的利用则有所逊色。\n这种现象的产生可能是由于训练数据中的无意偏差。LLM 的预训练侧重于根据最近的一些 token 预测下一个 token，而在微调过程中，真正的指令又往往位于上下文开始的位置，这在不知不觉中引入了一种立场偏见，让 LLM 认为重要信息总是位于上下文的开头和结尾。\n由于文章中对话较多，所以paragraph chunk token较少；recursive接近langchain预设的450token；proposition level与先前研究中的token接近，11token左右。semantic chunk由于对话较多，段落较短，可能语义比较跳跃，导致其avg token偏小。\n3.3 Lumber是否增强了下游的答案生成效果？ 开发了一个RAGpipeline进行测试。测试数据来自4部自传体作品，包括280个问题。与其它RAG进行对比，还与纯LLM对比。\n3.4 Lumberchunker与人类手动分块的效果有多相似？这种相似性又与准确性有什么关系？ 与recursive比较，用Rough-L评价，确实更像人分的块。这可能是导致RAG效果好的原因。\n4.Limitations 串行使用LLM，慢——改为并行 只使用了GutenQA一个数据集，叙事型的，不具有普遍性——多几个数据集/寻找通用的prompt ","permalink":"https://Rook1eChan.github.io/posts/lumberchunker/","summary":"\u003cp\u003e\u003cem\u003eEMNLP2024\u003c/em\u003e\u003c/p\u003e\n\u003ch1 id=\"0主要贡献\"\u003e0.主要贡献\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e提出了Lumberchunker文本分割方法\u003c/li\u003e\n\u003cli\u003e提出了GuntenQA数据集\u003c/li\u003e\n\u003cli\u003e验证了Lumberchunker在下游RAG任务上的效果\u003c/li\u003e\n\u003c/ul\u003e\n\u003cBR\u003e\n\u003ch1 id=\"1lumberchunker\"\u003e1.LumberChunker\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"overview\" loading=\"lazy\" src=\"/lumberchunker/p1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e使用LLM动态的将文档分割为语义独立的片段。每个片段的长短是不固定的，确保每个片段的语义完整性、独立性。也就是说分割后，每一段包含的语义是完整的，同时与其它段有区别。由LLM来确定合适的分割点，这一决策过程考虑到文本的结构和语义，从而能够创建出大小最优且上下文连贯的片段。\u003c/p\u003e\n\u003cp\u003e1.先按照paragraph分割目标文档，然后把paragraph顺序连接，直到累计的token数超过一个阈值 $\\theta$，形成 $ G_i$。该阈值如何设置后文会说。$\\theta$ 应该足够大，防止把具有相关性的段落分开；同时 $\\theta$​ 也要足够小，防止过多内容影响LLM进行推理。\u003c/p\u003e\n\u003cp\u003e2.让LLM寻找 $G_i$ 中“语义断层”的地方，作为分割点。分割点之前即形成一个chunk。剩下的内容继续与paragraph顺序拼接、超过阈值停止、LLM分割……分割整体是串行进行的。\u003c/p\u003e\n\u003cBR\u003e\n\u003ch1 id=\"2gutenqa\"\u003e2.GutenQA\u003c/h1\u003e\n\u003cp\u003e数据来源于Project Gutenberg电子图书馆。\u003c/p\u003e\n\u003cp\u003e1.100本英文书籍，手动提取HTML内容（附录里和NarrativeQA进行了对比，手动提取没有编码错误等问题）\u003c/p\u003e\n\u003cp\u003e2.使用ChatGPT3.5为每本书生成问题、答案和包含答案的原文片段，人工为每本书筛选30个高质量问题。\u003c/p\u003e\n\u003cp\u003e问题需要基于给定片段中的具体信息，\u003cu\u003e且不能用书中的其它地方的信息来回答\u003c/u\u003e。问题大多以‘what,’ ‘when,’ ‘where’ 开头， ‘why’ and ‘how’较少。\u003c/p\u003e\n\u003cp\u003e3.原文片段需要简短，以确保任何分块方法都不会把它切开。评估方法是在检索到的文本中精确匹配字符串。\u003c/p\u003e\n\u003cBR\u003e\n\u003ch1 id=\"3experiments\"\u003e3.Experiments\u003c/h1\u003e\n\u003ch3 id=\"31-propmt的阈值怎么选择\"\u003e3.1 propmt的阈值怎么选择\u003c/h3\u003e\n\u003cp\u003e这个阈值就是paragraph顺序连接的阈值 $\\theta$​ 。由于是LLM寻找分割点，token过长会影响模型的推理能力。\u003c/p\u003e\n\u003cp\u003e在不同阈值下使用DCG评估效果。DCG表明了是否检索到，检索结果是否靠前。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"p2\" loading=\"lazy\" src=\"/lumberchunker/p2.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"32-lumberchunk是否增强了检索效果\"\u003e3.2 Lumberchunk是否增强了检索效果？\u003c/h3\u003e\n\u003cp\u003e与其它分块基准进行对比。评估指标为DCG@K、RECALL@K。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"p3\" loading=\"lazy\" src=\"lumberchunker/p3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e此外，注意到semantic chunk和paragraph level的指标并没有随K有效增加，表明其在大规模文档检索方面的局限性。\u003c/p\u003e\n\u003cp\u003eproposition level的引用在哪？？？\u003c/p\u003e\n\u003cp\u003e附录F展示了各分割方法的统计结果：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"p3\" loading=\"lazy\" src=\"/lumberchunker/p3.png\"\u003e\u003c/p\u003e\n\u003cp\u003eLumberchunk切分后的块平均长度为334，比预设的550阈值低了40%，这说明LLM有效的对文本进行了切分，而不是持续选择靠近末尾的ID。说明未出现Lost in the Middle现象。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在论文《Lost in the Middle: How Language Models Use Long Contexts》中，作者发现，当针对长文本的不同位置信息设计专门问题，测试大语言模型对不同位置信息的记忆能力时，模型的性能呈现一种 “U 型” 表现，即对于前段与后段的信息有着较强的关注与记忆能力，能较好地解决问题，而对于中段信息的利用则有所逊色。\u003c/p\u003e\n\u003cp\u003e这种现象的产生可能是由于训练数据中的无意偏差。LLM 的预训练侧重于根据最近的一些 token 预测下一个 token，而在微调过程中，真正的指令又往往位于上下文开始的位置，这在不知不觉中引入了一种立场偏见，让 LLM 认为重要信息总是位于上下文的开头和结尾。\u003c/p\u003e","title":"LumberChunker"},{"content":"DeepSeek-VL: Towards Real-World Vision-Language Understanding 0. Abstract 本文的主要贡献：\n数据集构建：构建了具有多样性和可扩展性，广泛覆盖真实世界场景的数据集。包括网页截图、PDF文档、OCR文本、图表以及知识型内容（如专家知识、教科书）等。此外，根据真实用户场景将数据进行分类，并据此创建了指令微调数据集。通过该数据集的微调，显著提升了模型在实际应用中的用户体验。\n创新的模型架构：采用了混合视觉编码器（hybrid vision encoder），能在固定的token预算下高效处理高分辨率图像（1024*1024），同时保持较低的计算开销。该架构保证模型多种视觉任务中能捕捉到关键的语义和细节信息。\n创新的训练策略：既使LLM学会新模态，也保证原有的的语言能力不退化。调控语言和视觉的竞争关系，实现两种模态的均衡融合。\n1. Introduction 大语言模型的巨大成功引发了人们对多模态模型的追求。这些模型能同时理解语言和图像，在执行现实世界任务时展现出巨大的潜力。\n目前出现了很多开源的VLM方案，在benchmark上表现优秀，但在现实世界中表现不佳。大都存在以下问题（本文的改进方案）：\n许多方案将重心放在指令微调阶段。作者认为应当使用大量的视觉-语言数据进行充分预训练。（深度预训练）\n现有方案多使用学术上的数据集进行微调，缺乏现实世界经验。（精心构建数据集）\n现有方案多采用vision transformer与预训练语言模型结合的方式，这类模型分辨率低，不能胜任OCR或微小物体识别任务。（高分辨率处理架构）\n有些模型在长期的多模态训练中会出现语言能力的退化。应采用一种既保留语言能力，又掌握新模态能力的训练方式。（平衡多模态特征的训练策略）\nDeepSeek-VL具有通用的多模态理解能力，能够处理逻辑图、网页、公式识别、科学文献、自然图像等。\nDeepSeek-VL的优势：\nDeepseek-VL的预训练数据涵盖了广泛的世界知识，包括网络爬虫、网页代码、电子书、教育资料、arxiv文章等等，全面覆盖现实世界中的场景，数据质量高，具有广泛性和实用性。同时作者团队还精心设计了指令调优数据集，具体来说，作者从网上收集了GPT-4V和Gemini的真实案例，并进行分类，为每个测试图像选择合适的prompt。该分类体系还用于构建评估数据集。\n视觉模块采用混合视觉编码器架构，384$\\times$384的文本对齐编码器用于粗粒度语义提取，1024$\\times$1024的高分辨率编码器用于捕捉细节视觉信息。两者结合，可以将1024$\\times$1024的图像压缩为576个token，在视觉表征和token开销间取得平衡，使视觉模块支持文-图交织处理和多轮推理场景。\n为了使多模态模型不出现语言能力的退化：1.保持至少70%的语言数据，这对维护模型内部的语言知识完整性至关重要。2.作者提出了模态预热(modality warm-up)策略。该方法通过在训练过程中动态调整模态比例，逐步引入更多视觉-语言数据。\n在迭代模型时，首先在小模型上进行实验。然而，形如1B的小模型在benchmark上难以展现理想性能，无法真实的反映模型的实际表现。因此，作者把评估措施从多选改为了各选项的困惑度（PPL）对比；此外，为避免指令跟随能力成为瓶颈，在预训练阶段我们混合了少量指令调优数据。通过这种方式，我们既能利用1B模型获得合理性能表现，又能更精准地量化实验中每次迭代的影响效果。\n2. Data Construction 数据集包括两大模块：VL-Pretrain数据、VL-SFT数据\nVL-Pretrain整合了多源视觉文本数据，旨在强化模型的基础跨模态理解能力。\nVL-SFT相对较小，主要用于训练模型完成特定下游任务。\n在stage1，VL-Pretrain用于预热VL adapter\nstage2，VL-Pretrain用于联合预训练VL adaptor和VL model\nstage3，使用VL-SFT微调整个模型\n2.1 VL-Pretraining Data 分为以下7个类别：\nInterleaved image-text data（交错式图文数据，使模型对多模态输入具有更好的上下文学习能力），MMC4、Wiki等\nImage caption data（图像描述，包含高质量图-文对），Capsfusion、TaiSu等\nTable and chart data（图表数据），Chart2text、Unichart\nWeb Code data（网页代码，使模型具有从图形界面或图表重建代码的能力。从Stack数据集中的jupyter notebook清洗出2million图像-代码对。最终选择1.1million作为是主要训练集，包括一张图像-至少5行代码）\nOCR data（文档光学字符识别数据，作者构建了一个中英混合的OCR数据集，包括两部分：1.arxiv文章 2.电子书和教育材料，来自Anna\u0026rsquo;s Archive）\nScene text OCR（增强模型识别场景中文本的能力）ArT、MLT-17等。\nText-only corpus（纯文本，和DeepSeek LLM的一致）\n2.2 VL-SFT Data 包括多个知名开源数据集ShareGPT4V、LAION-GPTV等。\n从网络资源中收集GPT-4V和Gemini的多样化真实测试案例，进行分析、归类（蒸馏）：\nRecognition（考察大型模型对图像内容的理解和描述能力，不需要模型具备高知识储备和推理能力，部分任务可使用传统机器学习模型完成）\nConversion（要求模型描述和识别图像内容，并将图像转换为另一种形式，如UI2Code，Image2Prompt等）\nAnalysis（基于图像内容进行分析，如表格分析，医学图像分析等）\nCommonsense Reasoning（考察模型对常识的掌握，如物体间关系推理、物体功能推理等）\nLogical Reasoning（考察模型对领域知识的运用和逻辑推理能力，如数学、物理方面的推理）\nEvaluation（要求模型根据特定标准对图像内容进行评估）\nMulti-graph（考察模型分析理解多个图像的能力，如时序理解、多图对比等）\nSafety（检查模型安全性，如暗示性提问、反事实性提问和prompt注入）\n此外还有一部分纯文本SFT数据。\n3. Approach 3.1 Architecture 包含三个模块：a hybrid vision encoder, a vision adaptor, and a language model\nHybrid Vision Encoder：作者先是使用了SigLIP作为视觉编码器，但是发现单一的SigLIP编码器存在“CLIP-blind pairs”现象（视觉上不同，但是CLIP编码后却很相似），而且受限于低分辨率的输入。\n于是作者采用整合额外的vision-only self-supervised encoders（不依赖语言信息，只基于图像数据进行自监督学习的编码器模型。它通过对图像自身的特征提取和学习，生成图像的特征表示）来增强多模态模型的视觉能力。作者选择了SAM-B高分辨率视觉编码器。\n具体来说：\n图像 → 缩放到1024×1024 → SAM-B → 64×64×256 特征图（相当于一张图片分为64×64块，每块16×16，每块特征维度为256） 64×64×256 → VL 适配器 → 插值扩展为96×96×256 96×96×256 → 2 个步长为 2 的卷积 → 24×24×1024 → 重塑为 576×1024 低分辨率图像 → SigLIP-L → 576×1024 特征图 两个 576×1024 特征拼接 → 576×2048 视觉标记 经 GeLU 和嵌入层 → 与语言模型连接 Vision-Language Adaptor：采用双层混合多层感知机（two-layer hybrid MLP）。首先，分别使用独立的单层MLP处理高分辨率特征和低分辨率特征。随后，将这些特征按维度拼接，并通过另一层MLP转换为LLM的输入空间。\nLanguage Model：语言模型使用DeepSeek LLM，其微架构大多模仿LLaMA。使用RMSNorm进行Pre-Norm在前馈网络中采用SwiGLU作为激活函数，中间层维度为$\\frac{8}{3}d_{model}$。此外，在位置编码中采用了旋转编码。使用DeepSeek相同的分词器。选用DeepSeek的中间检查点继续预训练。\n具体来说，DeepSeek-VL-1B模型是基于DeekSeek-LLM-1B模型构建的，该模型使用了约5000亿文本标记的语料库进行训练。而DeekSeek-VL-7B模型则是基于DeepSeek-LLM-7B模型开发的，后者使用了估计2万亿文本标记进行训练。\n3.2 Training Pipelines 包含三个阶段\nStage 1）训练Vision-Language Adaptor\n将视觉编码器和LLM冻结，使用1.25million 图文对（来自ShareGPT4V）和2.5million 文档OCR数据训练VL adaptor。\n目的是在嵌入空间中建立视觉与语言的概念连接，帮助LLM全面理解图像中的实体。\n相较于LLM，VL-adaptors参数量较小（比如两层的MLP），限制了该阶段学习的能力。作者通过实验证明，在stage 1扩大数据规模反而会导致性能下降。law of data scaling不起作用。\nStage 2）Joint Vision-Language pretraining\n冻结vision ecoder，将LLM和VL adaptor联合训练。\n作者一开始仅使用多模态数据训练LLM，在模型多模态能力上升的同时，语言能力却迅速下降。这是可能是因为：1）多模态数据在语言上的简单化，与语言数据的复杂性和分布存在巨大差异；2）多模态与语言模态存在竞争关系，导致LLM出现灾难性遗忘。\n因此，作者在训练中进行了多模态数据和语言数据的平衡。作者通过不同比例的数据实验发现：1）加入语言数据能缓解语言能力退化；2）加入语言数据不会导致多模态能力下降；3）不同模态的表现与数据集比例高度相关，多模态：语言大约在3：7最佳。\n考虑到7B模型训练过于耗时，可以先训练1.3B的模型，再逐步扩展到7B模型。通过SFT，1.3B模型的知识可以有效的传递到7B模型中。\n然而，在第二阶段的训练中，1.3B模型的指标出现剧烈波动，使得训练过程难以监督。这一现象之前有关相关探讨：尽管模型家族的单个词错误率会随着规模扩大而平稳、持续且可预测地变化，但研究者选择的测量方法仍可能引发剧烈且不可预测的变化。作者后来找到了问题的根源：1.3B模型容量有限，再加上训练数据集中缺乏SFT数据，阻碍了模型准确执行命令，即使掌握了正确的知识也难以精准生成选项。\n为了应对这些问题，作者采用以下策略：1）采用Multi-choice PPL methodology来监督模型进展。除了输入图片和prompt，还输入与问题相关的答案。通过计算每个候选答案的PPL值来确定模型最终的输出。（既然生成不稳定，就不生成，直接选择）；2）在训练数据集中加入少量STF数据，让模型在遵循指令方面获得一定熟练度。\n这两种方法的结合确保了1.3B模型训练指标的稳定性。\nStage3）SFT\n增强模型遵循指令和参与对话的能力。\n对VL-adaptor、LLM、hybrid vision encoder进行微调。由于GPU内存容量有限，SAM-B仍冻结。\n仅对答案和特殊标记进行监督，屏蔽系统提示和用户输入。答案是模型生成的结果，特殊token是对话中用于标记的特殊符号。也就是损失函数只看输出不看输入。\n3.3 Hyperparameters and Infrastructres 使用轻量级分布式训练框架HAI-LLM进行DeepSeek-VL的训练与评估。\n将视觉编码和文本嵌入视为一个模块，把它作为最终模型的第一层。这个模块有着复杂的模型架构，标准的张量并行技术无法使用，但是与上层的transformer模块相比，其计算量相对较小。我们只需在所有张量并行维度上重新计算视觉编码器的前向传播过程。\nvisual encoders还使得模型各层的执行时间不同，所以我们重新划分模型层级来实现更好的负载均衡和吞吐量。DeepSeek-VL的上层结构和DeepSeekLLM一致。\n通过这种微调，我们可以采用Megatron中的标准3D并行技术。以及DeepSeek-LLM中的重叠计算与通信机制。\n在64节点集群上运行时，DeepSeek-VL- 7B耗时5天，每个节点配备8个Nvidia A100 GPU；而在包含16个节点的配置下，DeepSeek-VL-1B则需要7天时间。\n其余超参数如下所示：\n4. Evaluation 4.1 公开多模态benchmark 多模态理解数据集：MMMU、CMMMU等\n图表理解：OCRBench等\n幻觉：POPE\n科学问题：ScienceQA、MathVista\n采用基于生成的评估方法（关注模型生成的文本结果，将其与groundtruth对比），结合贪心解码技术（在生成每个 token时，始终选择当前概率最高的 token，而不考虑未来可能的最优选择）。\nDeekSeek-VL-7B超过了大多数同规模的开源模型，和比闭源模型仍有差距，这种差异可能源于基础模型规模的差异。\nDeepSeek-VL-1.3B在同类规模模型中表现显著更优。在MMB基准测试中，其性能超越了主流开源模型，而参数量仅为其约一半，1.3B vs 2.7B。\n4.2 公开语言benchmark 多主题多选：MMLU\n语言理解和推理：HellaSwag\n语言建模：Pile\n数学：GSM8K\n编程：MBPP\n标准考试：AGIEval\n对于从多个答案进行选择的数据集（HellaSwag、MMLU），采用困惑度评估，选择困惑度最小的选项作为最终选择。基于困惑度的评估方法能有效区分模型预测间的细微概率差异，避免了精确匹配式评估带来的不连续性问题。\n针对GSM8K和AGIEval数据集，我们采用基于生成的评估方法结合贪心解码技术。这里的生成评估具体指让模型自动生成自由文本，并解析生成文本的解析结果。\n我们采用基于语言模型的评估方法进行堆测试，即计算测试语料库的每字节比特数。\n可以观察到，在大多数语言基准测试中，DeepSeek-VL的表现与DeepSeek-7B相当甚至更胜一筹。例如，在作为通用语言能力评估基准的HellaSwag数据集上，其得分达到68.4分，而DeepSeek-7B为68.5分。在MMLU和AGIEval等指标上，DeepSeek-VL更胜一筹，这表明多模态训练方法甚至可能助力语言任务。然而，DeepSeek-VL-7B在数学测试（GSM8K）中出现一定程度的退步，这说明尽管视觉与语言模态间的协调性得到提升，二者之间仍存在竞争关系。这可能归因于模型容量有限（7B），而更大规模的模型或许能显著缓解这一问题。总体而言，DeepSeek-VL致力于在应对这些挑战的同时，实现语言能力下降幅度最小化的目标。\n4.3 Human Evaluation 作者团队构建了人工评估数据集，包含100个问题，包含7个类别，与SFT的分类一致。\nDeepSeek-VL-7B与InternLM-XComposer2-VL、CogVLM及GPT- 4V进行对比\nGPT- 4V在多数维度均展现出卓越性能，而所有开源模型在逻辑推理方面仍远逊于GPT- 4V，这凸显了扩大大型语言模型（LLM）规模的必要性。DeepSeek-VL-7B在整体表现上更胜一筹，在识别、转换和常识推理等任务中接近GPT- 4V的水平。\n此外还进行了LLM评估，使用GPT-4V对两种模型的答案进行评估。多数情况下DeekSeek-VL的回答更受青睐。\n4.4 Ablation Study Scale up Projector Training\n在stage1 逐渐扩大数据集，但性能并未提升。说明projector的容量有限，无法捕捉多模态任务所需的广泛知识。\nTraining Stage\n对比使用不同训练阶段的组合（1+3，2+3，1+2+3），三阶段联合使用效果最好。此外，2+3效果差于1+2+3，说明了VL adaptor预热的必要性。\nModality Group Training\n当语言数据和多模态数据混合时，发现直接在batch层面混合显著降低训练效率。这是因为要等待计算较慢的多模态样本完成后才能梯度反向传播，拖慢了整体速度。\n作者在每个global step上将不同模态的数据分开，batch内只包含语言或多模态数据。这种方法提升了模型训练效率且不会影响性能。\nModality Warmup\n若一开始就按比例混合多模态数据，可能会导致模型不稳定。初始阶段先将语言数据比例设为1，再逐渐降低至最终训练目标值，能避免训练初期模型语言能力下降，同时在语言和多模态领域后期均取得更优效果。\nVision Encoder Selection\n为了提高获取和利用图像信息的能力，对比了不同视觉编码器的训练损失。引入纯视觉编码器SAM在Loss指标上有显著提升。所以最终采用SigLP+SAM的方案。\nVision-Language Adaptor Design\n为了提高从视觉编码器中提取信息的效率，且遵循当前的token长度限制，有两种方式来调整VL Adaptor：视觉特征的组合方法和MLP adaptor的设计。\n先前研究表明，沿序列维度整合视觉特征能提升模型性能，但这也意味着需要视觉特征标记的序列变长，计算开销增大。在序列拼接之前，沿图像宽或高对视觉特征进行堆叠，不比直接沿嵌入维度合并效果好。\n（Token Pooling - W：[H,W,C]，把每一高度的W方向作池化，得到[H,C]，然后接在文本序列后面；Token Pooling - H则是把高度方向压缩）\n在adaptor架构方面，每个视觉特征编码器单独使用MLP adaptor可以更精确地调整视觉特征的具体值和分布模式，促进更平稳的模型训练。为不同的视觉编码器使用共享的MLP适配器有助于足够的特征融合。作者采用了混合策略。\nConclusion、Limitation and Future Work\n介绍了DeepSeek-VL，一个系列MLLM。\n采用了创新的三阶段训练方法，保证了模型的语言和多模态能力。\n集成了混合视觉编码器，能处理高分辨率图像，在各类测试中表现优异。\n未来将进一步扩大模型规模，并引入MoE技术。\n开源万岁！\n","permalink":"https://Rook1eChan.github.io/posts/deepseekvl/","summary":"\u003ch1 id=\"deepseek-vl-towards-real-world-vision-language-understanding\"\u003eDeepSeek-VL: Towards Real-World Vision-Language Understanding\u003c/h1\u003e\n\u003ch2 id=\"0-abstract\"\u003e0. Abstract\u003c/h2\u003e\n\u003cp\u003e本文的主要贡献：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e数据集构建：构建了具有多样性和可扩展性，广泛覆盖真实世界场景的数据集。包括网页截图、PDF文档、OCR文本、图表以及知识型内容（如专家知识、教科书）等。此外，根据真实用户场景将数据进行分类，并据此创建了指令微调数据集。通过该数据集的微调，显著提升了模型在实际应用中的用户体验。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e创新的模型架构：采用了混合视觉编码器（hybrid vision encoder），能在固定的token预算下高效处理高分辨率图像（1024*1024），同时保持较低的计算开销。该架构保证模型多种视觉任务中能捕捉到关键的语义和细节信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e创新的训练策略：既使LLM学会新模态，也保证原有的的语言能力不退化。调控语言和视觉的竞争关系，实现两种模态的均衡融合。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cBR\u003e\n\u003ch2 id=\"1-introduction\"\u003e1. Introduction\u003c/h2\u003e\n\u003cp\u003e大语言模型的巨大成功引发了人们对多模态模型的追求。这些模型能同时理解语言和图像，在执行现实世界任务时展现出巨大的潜力。\u003c/p\u003e\n\u003cp\u003e目前出现了很多开源的VLM方案，在benchmark上表现优秀，但在现实世界中表现不佳。大都存在以下问题（本文的改进方案）：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e许多方案将重心放在指令微调阶段。作者认为应当使用大量的视觉-语言数据进行充分预训练。（深度预训练）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e现有方案多使用学术上的数据集进行微调，缺乏现实世界经验。（精心构建数据集）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e现有方案多采用vision transformer与预训练语言模型结合的方式，这类模型分辨率低，不能胜任OCR或微小物体识别任务。（高分辨率处理架构）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e有些模型在长期的多模态训练中会出现语言能力的退化。应采用一种既保留语言能力，又掌握新模态能力的训练方式。（平衡多模态特征的训练策略）\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cBR\u003e\n\u003cp\u003eDeepSeek-VL具有通用的多模态理解能力，能够处理逻辑图、网页、公式识别、科学文献、自然图像等。\u003c/p\u003e\n\u003cp\u003eDeepSeek-VL的优势：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDeepseek-VL的预训练数据涵盖了广泛的世界知识，包括网络爬虫、网页代码、电子书、教育资料、arxiv文章等等，全面覆盖现实世界中的场景，数据质量高，具有广泛性和实用性。同时作者团队还精心设计了指令调优数据集，具体来说，作者从网上收集了GPT-4V和Gemini的真实案例，并进行分类，为每个测试图像选择合适的prompt。该分类体系还用于构建评估数据集。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e视觉模块采用混合视觉编码器架构，384$\\times$384的文本对齐编码器用于粗粒度语义提取，1024$\\times$1024的高分辨率编码器用于捕捉细节视觉信息。两者结合，可以将1024$\\times$1024的图像压缩为576个token，在视觉表征和token开销间取得平衡，使视觉模块支持文-图交织处理和多轮推理场景。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e为了使多模态模型不出现语言能力的退化：1.保持至少70%的语言数据，这对维护模型内部的语言知识完整性至关重要。2.作者提出了模态预热(modality warm-up)策略。该方法通过在训练过程中动态调整模态比例，逐步引入更多视觉-语言数据。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e在迭代模型时，首先在小模型上进行实验。然而，形如1B的小模型在benchmark上难以展现理想性能，无法真实的反映模型的实际表现。因此，作者把评估措施从多选改为了各选项的困惑度（PPL）对比；此外，为避免指令跟随能力成为瓶颈，在预训练阶段我们混合了少量指令调优数据。通过这种方式，我们既能利用1B模型获得合理性能表现，又能更精准地量化实验中每次迭代的影响效果。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cBR\u003e\n\u003ch2 id=\"2-data-construction\"\u003e2. Data Construction\u003c/h2\u003e\n\u003cp\u003e数据集包括两大模块：VL-Pretrain数据、VL-SFT数据\u003c/p\u003e\n\u003cp\u003eVL-Pretrain整合了多源视觉文本数据，旨在强化模型的基础跨模态理解能力。\u003c/p\u003e\n\u003cp\u003eVL-SFT相对较小，主要用于训练模型完成特定下游任务。\u003c/p\u003e\n\u003cp\u003e在stage1，VL-Pretrain用于预热VL adapter\u003c/p\u003e\n\u003cp\u003estage2，VL-Pretrain用于联合预训练VL adaptor和VL model\u003c/p\u003e\n\u003cp\u003estage3，使用VL-SFT微调整个模型\u003c/p\u003e\n\u003cBR\u003e\n\u003ch3 id=\"21-vl-pretraining-data\"\u003e2.1 VL-Pretraining Data\u003c/h3\u003e\n\u003cp\u003e分为以下7个类别：\u003c/p\u003e\n\u003cp\u003eInterleaved image-text data（交错式图文数据，使模型对多模态输入具有更好的上下文学习能力），MMC4、Wiki等\u003c/p\u003e\n\u003cp\u003eImage caption data（图像描述，包含高质量图-文对），Capsfusion、TaiSu等\u003c/p\u003e\n\u003cp\u003eTable and chart data（图表数据），Chart2text、Unichart\u003c/p\u003e\n\u003cp\u003eWeb Code data（网页代码，使模型具有从图形界面或图表重建代码的能力。从Stack数据集中的jupyter notebook清洗出2million图像-代码对。最终选择1.1million作为是主要训练集，包括一张图像-至少5行代码）\u003c/p\u003e\n\u003cp\u003eOCR data（文档光学字符识别数据，作者构建了一个中英混合的OCR数据集，包括两部分：1.arxiv文章 2.电子书和教育材料，来自Anna\u0026rsquo;s Archive）\u003c/p\u003e\n\u003cp\u003eScene text OCR（增强模型识别场景中文本的能力）ArT、MLT-17等。\u003c/p\u003e\n\u003cp\u003eText-only corpus（纯文本，和DeepSeek LLM的一致）\u003c/p\u003e\n\u003ch3 id=\"22-vl-sft-data\"\u003e2.2 VL-SFT Data\u003c/h3\u003e\n\u003cp\u003e包括多个知名开源数据集ShareGPT4V、LAION-GPTV等。\u003c/p\u003e","title":"论文阅读 | DeepSeek-VL: Towards Real-World Vision-Language Understanding"},{"content":"这是一个系列博客，包括五篇文章。博客地址：Stop Saying RAG Is Dead – Hamel’s Blog\n作者批驳了“RAG已死”的说法，认为真正被淘汰的是“Chuck documents into a vector database, do cosine similarity, call it a day. ”的过时的RAG。RAG技术仍在进化，在后面的的文章里可以看到在检索、评估等方面上的创新。很高兴看到有人对RAG持积极态度，毕竟在一个有希望的领域进行研究学习更有动力。\n五篇文章的简介如下：\n标题 内容简介 Part 1: I don’t use RAG, I just retrieve documents Ben Clavié 介绍了RAG的现状 Part 2: Modern IR Evals For RAG 评估是必不可少的步骤，高质量的benchmark有助于我们选择更好的方法。Nandan Thakur （BIER作者）认为传统的IR指标不适合评估RAG的表现，应该采用新的指标 Part 3: Optimizing Retrieval with Reasoning Models Orion Weller 提出了一种能遵循instruct的检索系统，在检索时就进行推理，优于传统的语义检索 Part 4: Late Interaction Models For RAG Antoine Chaffin 介绍了ColBERT这类迟交互、多向量模型 Part 5: RAG with Multiple Representations Bryan Bischof and Ayush Chaurasia 提出，我们需要对不同模态的问题智能化的选用不用的指标 P1: I don’t use RAG, I just retrieve documents 现在有一些说法，认为长上下文窗口的出现使得我们不再需要RAG了。\n10M的上下文窗口出世，并不意味着RAG就被淘汰了。就好像512MB的RAM出世，并不意味着硬盘就被淘汰了。毕竟外部知识的数量远超上下文窗口，从成本和效率来看，长上下文窗口并不会替代RAG。\nRAG是已冻结参数的模型了解外部世界的桥梁。模型不需要知道那么多无关紧要的、用过即丢的知识，更何况模型学习这些知识要经过复杂的过程。所以，RAG是提供必要的外部知识的极佳方式。\n回到刚才RAM和硬盘的例子，硬盘确实被取代了，不过是被SSD。所以，淘汰的RAG的，是更先进的RAG。\nP3: Optimizing Retrieval with Reasoning Models 长久以来，我们的搜索引擎的形式一直没有变化，仍然是根据关键字/语义进行检索。\n比如说，有三篇文章：\nData Encryption Standards Wolves Outside Your Data Digital Protection query为：“Find websites explaining data privacy”\n采用关键词检索，会得到1 2，因为含有data关键词。\n采用语义检索，会得到1 2 3，因为它了解“data”和“digital”以及“privacy”和“protection”之间的关系。\n但是，如果我们把query改为“Find websites explaining data privacy and uses extended metaphors”，我们想要检索出2，但是模型无法理解约束，只会尝试匹配与“and uses extended metaphors”语义相近的文章，与我们的目的相差甚远。\n包括现在LLM的联网搜索，也不过是在传统检索的外面，多加了一层LLM进行整理。\n因此，作者希望检索器能遵循指令（instruct）。指令可以和文档的属性相关，比如日期、长度或来源。或者涉及NLU 方面，例如文档情感或写作风格。第三，它们可以包含逻辑条件，将多个约束与 AND、OR 和 NOT 等运算符组合在一起。\n作者提出了Promptriever模型，让我们在检索时也用上prompt吧。\nPromptriever 传统的数据集不带有指令。作者使用LLM构建了一个带有指令的数据集。然后使用RepLLaMA在数据集上训练。\n在评测时使用了两个带指令的数据集：FollIR和InstructIR。结果当然是Promptriever好，因为其它模型根本没法利用指令。\n那Promptriever在没有prompt时怎么办？可以直接不带prompt，或者加一句 “Find the most relevant document”，表现会更好。\nPromptriever可以通过调整自然语言提示来更改模型的行为，不需要设置固定的topk阈值，另外还能解决 “find documents with high recall” 这样的指令。\n为了证明模型真的理解了指令，作者对每个数据使用了10个表述不同的指令，结果该模型的标准差最小，说明它确实理解了指令的含义，而不是简单匹配关键词。\n总结：\n经过特制数据的训练，bi-encoder能变得像大模型一样可遵循指令。 可以使用新的询问方式，精细到风格、情感、逻辑层面。 用户不需要纠结关键词，只需要告诉模型他们想要什么。 ","permalink":"https://Rook1eChan.github.io/posts/%E5%8D%9A%E5%AE%A2-stop-saying-rag-is-dead/","summary":"\u003cp\u003e这是一个系列博客，包括五篇文章。博客地址：\u003ca href=\"https://hamel.dev/notes/llm/rag/not_dead.html\"\u003eStop Saying RAG Is Dead – Hamel’s Blog\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e作者批驳了“RAG已死”的说法，认为真正被淘汰的是“Chuck documents into a vector database, do cosine similarity, call it a day. ”的过时的RAG。RAG技术仍在进化，在后面的的文章里可以看到在检索、评估等方面上的创新。很高兴看到有人对RAG持积极态度，毕竟在一个有希望的领域进行研究学习更有动力。\u003c/p\u003e\n\u003cp\u003e五篇文章的简介如下：\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003e标题\u003c/th\u003e\n          \u003cth\u003e内容简介\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003ca href=\"https://hamel.dev/notes/llm/rag/p1-intro.html\"\u003ePart 1\u003c/a\u003e: \u003cstrong\u003eI don’t use RAG, I just retrieve documents\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eBen Clavié 介绍了RAG的现状\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003ca href=\"https://hamel.dev/notes/llm/rag/p2-evals.html\"\u003ePart 2\u003c/a\u003e: \u003cstrong\u003eModern IR Evals For RAG\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e评估是必不可少的步骤，高质量的benchmark有助于我们选择更好的方法。Nandan Thakur （BIER作者）认为传统的IR指标不适合评估RAG的表现，应该采用新的指标\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003ca href=\"https://hamel.dev/notes/llm/rag/p3_reasoning.html\"\u003ePart 3\u003c/a\u003e: \u003cstrong\u003eOptimizing Retrieval with Reasoning Models\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eOrion Weller 提出了一种能遵循instruct的检索系统，在检索时就进行推理，优于传统的语义检索\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003ca href=\"https://hamel.dev/notes/llm/rag/p4_late_interaction.html\"\u003ePart 4\u003c/a\u003e: \u003cstrong\u003eLate Interaction Models For RAG\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eAntoine Chaffin 介绍了ColBERT这类迟交互、多向量模型\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003ca href=\"https://hamel.dev/notes/llm/rag/p5_map.html\"\u003ePart 5\u003c/a\u003e: \u003cstrong\u003eRAG with Multiple Representations\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eBryan Bischof and Ayush Chaurasia 提出，我们需要对不同模态的问题智能化的选用不用的指标\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cBR\u003e\n\u003ch1 id=\"p1-i-dont-use-rag-i-just-retrieve-documents\"\u003eP1: I don’t use RAG, I just retrieve documents\u003c/h1\u003e\n\u003cp\u003e现在有一些说法，认为长上下文窗口的出现使得我们不再需要RAG了。\u003c/p\u003e","title":"博客 | Stop Saying RAG Is Dead"},{"content":"LLaMA-Factory使用教程\n一、环境准备 1.1创建虚拟环境 conda create -n lf python==3.11 conda init 然后重开cmd\nconda activate lf 1.2下载相关的包 conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia 验证GPU版本的Pytorch是否成功\npython -c \u0026#34;import torch; print(torch.cuda.is_available())\u0026#34; 1.3下载llama factory sudo apt install git 开科学上网\ngit clone https://github.com/hiyouga/LLaMA-Factory.git 1.4安装依赖 python -m pip install --upgrade pip pip install -r requirements.txt pip install -e \u0026#34;.[torch,metrics]\u0026#34; 如果下载有问题，可以尝试清华源\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e \u0026#34;.[torch,metrics]\u0026#34; 1.5清理pip pip cache purge 二、下载模型 2.1从modelscope下载模型权重文件 pip install modelscope 可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面\nmodelscope download --model Qwen/Qwen3-0.6B 更建议下载到指定文件夹，便于管理\nmodelscope download --model \u0026#39;Qwen/Qwen3-4B-Base\u0026#39; --local_dir \u0026#39;/root/autodl-tmp/qwen3-4b-base/\u0026#39; 三、运行webui 3.1运行webui llamafactory-cli webui 或者 python src/webui.py 在autodl中可能会报错：To create a public link, set share=True in launch().\n打开interface.py文件，路径为：LLaMA-Factory/src/llamafactory/webui/interface.py\n将run_web_ui的create_ui().queue().launch的share参数修改为True\n如果还不行，可以尝试以下方法：\n从https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64下载文件 将文件重命名为：frpc_linux_amd64_v0.2 然后把文件放在/root/miniconda3/envs/lf/lib/python3.11/site-packages/gradio/，这个方法我没用 或者\ncp /root/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3 /root/miniconda3/envs/lf/lib/python3.11/site-packages/gradio/ 我是将frpc文件拷贝了一份到另一个文件夹，然后给一个权限 chmod +x frpc_linux_amd64_v0.3 3.2SSH 对于autodl来说，要想连接到页面还要经过ssh（如果是本地，直接点击最后的网页链接，应该就可以运行了）\n在autodl控制台——你这台机子——快捷工具——自定义服务——下载autodl ssh tool——打开\n输入该机子的登陆指令和密码\n代理到本地端口：6006\n代理到远程端口：1080\n然后，本地cmd运行\nssh -CNg -L 7860:127.0.0.1:7860 root@connect.bjb1.seetacloud.com -p 54073 54073换为该机子的端口，然后输入机子的密码\n然后访问127.0.0.1:7860即可进入webui\n四、对话页面开启 开启对话页面，来验证下载的模型是否正常\n在cmd输入\nCUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat --model_name_or_path /root/autodl-tmp/qwen3-0.6b --template qwen model_name_or_path：模型下载到默认地址，就填hf官方命名如Qwen3/Qwen3-0.6B；下载到指定地址就填绝对地址如/root/autodl-tmp/qwen3-0.6b\ntemplate：根据模型来选。如果模型出现不停的输出、胡言乱语现象就设置为default再试试\n然后访问 http://127.0.0.1:7860/\n五、微调 上手进行微调\n5.1简单实验 首先进行identity的实验\n使用系统自带的identity.json数据集，对应文件已经在data目录下\n如果使用自己的数据，要放到data下并在dataset_info.json中注册\n我们希望微调后，模型能输出“我是Alex，我由Rook1e制造”\n这里我们把identity.json里改掉，你也可以改成你的名字\nsed -i \u0026#39;s/{{name}}/Alex/g\u0026#39; data/identity.json sed -i \u0026#39;s/{{author}}/Rook1echan/g\u0026#39; data/identity.json 再次打开该文件发现所有的部分都发生了替换\n然后进入webui\n对于indentity这个简单实验的相关参数选择，没写的保持默认：\n参数名 选择 语言 zh 模型名称 用啥选啥 模型路径 下到指定文件夹就写文件夹绝对路径，不然就写hf模型标识符 模型下载源 modelscope 微调方法 lora 选择Train 训练阶段 SFT 数据集 identit 训练轮数 100（差不多到100loss就不下降了） 截断长度 256（数据不是很长） 批处理大小 建议设置不同数量多试验几次，让GPU到80%最好，充分利用也不至于OOM，我这里是4090 24G，设置15 其它参数设置-启用思考 不知道有啥印象，我关掉了，应该会推理快一点 然后点击开始就行了，下面能看到loss的曲线图\n5.2使用sft后的模型 qwen3-4b-base:\nCUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat --model_name_or_path /root/autodl-tmp/qwen3-4b-base --adapter_name_or_path /root/autodl-tmp/LLaMA-Factory/saves/Qwen3-4B-Base/lora/train_2025-09-18-19-41-50 --finetuning_type lora --template default 多了adapter_name_or_path（check point位置）和finetuning_type参数\n注意，由于我们使用的是单词对话模型，所以在chat里每次对话完后要清除历史！不然就会胡言乱语。。。\n经过实验，qwen3需要\u0026ndash;template default才不会说胡话\n","permalink":"https://Rook1eChan.github.io/posts/llama-factory/","summary":"\u003cp\u003eLLaMA-Factory使用教程\u003c/p\u003e\n\u003ch1 id=\"一环境准备\"\u003e一、环境准备\u003c/h1\u003e\n\u003ch2 id=\"11创建虚拟环境\"\u003e1.1创建虚拟环境\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003econda create -n lf \u003cspan class=\"nv\"\u003epython\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e3.11\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003econda init\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e然后重开cmd\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003econda activate lf\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cbr\u003e\n\u003ch2 id=\"12下载相关的包\"\u003e1.2下载相关的包\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003econda install \u003cspan class=\"nv\"\u003epytorch\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e2.4.0 \u003cspan class=\"nv\"\u003etorchvision\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e0.19.0 \u003cspan class=\"nv\"\u003etorchaudio\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e2.4.0 pytorch-cuda\u003cspan class=\"o\"\u003e=\u003c/span\u003e12.1 -c pytorch -c nvidia\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e验证GPU版本的Pytorch是否成功\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epython -c \u003cspan class=\"s2\"\u003e\u0026#34;import torch; print(torch.cuda.is_available())\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cbr\u003e\n\u003ch2 id=\"13下载llama-factory\"\u003e1.3下载llama factory\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt install git\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e开科学上网\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit clone https://github.com/hiyouga/LLaMA-Factory.git\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cbr\u003e\n\u003ch2 id=\"14安装依赖\"\u003e1.4安装依赖\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epython -m pip install --upgrade pip\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install -r requirements.txt\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install -e \u003cspan class=\"s2\"\u003e\u0026#34;.[torch,metrics]\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如果下载有问题，可以尝试清华源\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e \u003cspan class=\"s2\"\u003e\u0026#34;.[torch,metrics]\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cbr\u003e\n\u003ch2 id=\"15清理pip\"\u003e1.5清理pip\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip cache purge\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cbr\u003e\n\u003ch1 id=\"二下载模型\"\u003e二、下载模型\u003c/h1\u003e\n\u003ch2 id=\"21从modelscope下载模型权重文件\"\u003e2.1从modelscope下载模型权重文件\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install modelscope\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面\u003c/p\u003e","title":"在Autodl中使用LLaMA-Factory进行微调"},{"content":"参考：https://zhuanlan.zhihu.com/p/403495863\n1.介绍 BERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，该模型在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。\nBERT的网络架构使用的是《Attention is all you need》中提出的多层Transformer结构。其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。Transformer的结构在NLP领域中已经得到了广泛应用。\n2.模型结构 下图展示的是BERT的总体结构图，多个Transformer Encoder一层一层地堆叠起来，就组装成了BERT了，在论文中，作者分别用12层和24层Transformer Encoder组装了两套BERT模型，两套模型的参数总数分别为110M和340M。\nBERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非像Bi-LSTM那样把句子倒序输入一遍。在BERT之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。\n2.1Embedding Embedding由三种Embedding求和而成：\ntoken embedding\n将输入的文本进行Word Piece分词，如playing切割成play，##ing，使用Word Piece是为了解决未登录词。tokenization后，在开头插入[CLS]，在每句话的末尾插入[SEP]。[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。\nBert 在处理英文文本时只需要 30522 个词，Token Embeddings 层会将每个词转换成 768 维向量，如下图的例子中， 5 个Token 会被转换成一个 (5, 768) 的矩阵或 (1, 5, 768) 的张量。\nsegment embedding\n用来区分两种句子。bert在训练时包括两种任务，其一是MLM（masked language model，掩码语言模型），就是遮住某个词让模型去预测；其二是NSP（next sentence prediction，下一句预测），输入两个句子，让模型判断是否相关。这时候就需要segment embedding对两个句子做区别。\n进行问答等需要预测下一句的任务时，segment embedding层把0赋值给第一个句子的各token，把1赋值给第二个句子的各token。在文本分类任务时，segment embedding全部为0。\nposition embedding\n和transformer的实现不同，不是固定的三角函数，而是可学习的参数。\nTransformer 中通过植入关于 Token 的相对位置或者绝对位置信息来表示序列的顺序信息。作者测试用学习的方法来得到 Position Embeddings，最终发现固定位置和相对位置效果差不多，所以最后用的是固定位置的，而正弦可以处理更长的 Sequence，且可以用前面位置的值线性表示后面的位置。\n在BERT中，Position Embeddings层被引入以解决Transformer模型无法编码输入序列顺序性的问题。在自然语言处理任务中，词的顺序往往很重要。例如，“I think, therefore I am”中，“I”的顺序不同，表达的含义也不同。Position Embeddings层通过添加位置信息，让BERT能够理解词的位置关系，从而更好地处理文本数据。在BERT中，位置信息被编码成一系列向量，这些向量被加到Token Embeddings层的输出上，形成最终的词向量表示。通过这种方式，BERT能够理解词的位置关系，从而更好地处理文本数据。\nBERT 中处理的最长序列是 512 个 Token，长度超过 512 会被截取，BERT 在各个位置上学习一个向量来表示序列顺序的信息编码进来，这意味着 Position Embeddings 实际上是一个 (512, 768) 的 lookup 表，表第一行是代表第一个序列的每个位置，第二行代表序列第二个位置。\n最后，BERT 模型将 Token Embeddings (1, n, 768) + Segment Embeddings(1, n, 768) + Position Embeddings(1, n, 768) 求和的方式得到一个 Embedding(1, n, 768) 作为模型的输入。\n（不明白怎么赋值的）\n[CLS]的作用\nBERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层的信息传递后编码了全局语义，相比其他正常词，可以更好的表征句子语义。\n分类的时候把cls位置的向量取出来即可。\n2.2Encoder BERT使用了Transformer的encoder侧的网络。\n在Transformer中，模型的输入会被转换成512维的向量，然后分为8个head，每个head的维度是64维，但是BERT的维度是768维度，然后分成12个head，每个head的维度是64维，这是一个微小的差别。Transformer中position Embedding是用的三角函数，BERT中也有一个Postion Embedding是随机初始化，然后从数据中学出来的。\nBERT模型分为24层和12层两种，其差别就是使用transformer encoder的层数的差异，BERT-base使用的是12层的Transformer Encoder结构，BERT-Large使用的是24层的Transformer Encoder结构。\n3.BERT训练 BERT的训练包含pre-train和fine-tune两个阶段。pre-train阶段模型是在无标注的标签数据上进行训练；fine-tune阶段BERT模型首先是被pre-train模型参数初始化，然后所有的参数会用下游的有标注的数据进行训练。\n3.1 BERT预训练 BERT是一个多任务模型，它的预训练（Pre-training）任务是由两个自监督任务组成，即MLM和NSP。\nMLM\nMLM是指在训练的时候随即从输入语料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，做以下处理。\n80%的时候会直接替换为[Mask]，将句子 \u0026ldquo;my dog is cute\u0026rdquo; 转换为句子 \u0026ldquo;my dog is [Mask]\u0026quot;。\n10%的时候将其替换为其它任意单词，将单词 \u0026ldquo;cute\u0026rdquo; 替换成另一个随机词，例如 \u0026ldquo;apple\u0026rdquo;。将句子 \u0026ldquo;my dog is cute\u0026rdquo; 转换为句子 \u0026ldquo;my dog is apple\u0026rdquo;。\n10%的时候会保留原始Token，例如保持句子为 \u0026ldquo;my dog is cute\u0026rdquo; 不变。\n这么做的原因是如果句子中的某个Token 100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’cute‘。至于单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。 另外文章指出每次只预测15%的单词，因此模型收敛的比较慢。\n优点\n1）被随机选择15%的词当中以10%的概率用任意词替换去预测正确的词，相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力；\n2）被随机选择15%的词当中以10%的概率保持不变，缓解了finetune时候与预训练时候输入不匹配的问题（预训练时候输入句子当中有mask，而finetune时候输入是完整无缺的句子，即为输入不匹配问题）。\n缺点\n针对有两个及两个以上连续字组成的词，随机mask字割裂了连续字之间的相关性，使模型不太容易学习到词的语义信息。主要针对这一短板，因此google此后发表了BERT-WWM，国内的哈工大联合讯飞发表了中文版的BERT-WWM。 NSP\nNext Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中。\n输入 = [CLS] 我 喜欢 玩 [Mask] 联盟 [SEP] 我 最 擅长 的 [Mask] 是 亚索 [SEP]\n类别 = IsNext\n输入 = [CLS] 我 喜欢 玩 [Mask] 联盟 [SEP] 今天 天气 很 [Mask] [SEP]\n类别 = NotNext\n注意，在此后的研究（论文《Crosslingual language model pretraining》等）中发现，NSP任务可能并不是必要的，消除NSP损失在下游任务的性能上能够与原始BERT持平或略有提高。这可能是由于BERT以单句子为单位输入，模型无法学习到词之间的远程依赖关系。针对这一点，后续的RoBERTa、ALBERT、spanBERT都移去了NSP任务。\nBERT预训练模型最多只能输入512个词，这是因为在BERT中，Token，Position，Segment Embeddings 都是通过学习来得到的。在直接使用Google 的BERT预训练模型时，输入最多512个词（还要除掉[CLS]和[SEP]），最多两个句子合成一句。这之外的词和句子会没有对应的embedding。\n如果有足够的硬件资源自己重新训练BERT，可以更改 BERT config，设置更大max_position_embeddings 和 type_vocab_size值去满足自己的需求。\n4.BERT的优缺点 优点 BERT 相较于原来的 RNN、LSTM 可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。 相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。 缺点 模型参数太多，而且模型太大，少量数据训练时，容易过拟合。 BERT的NSP任务效果不明显，MLM存在和下游任务mismathch的情况。 BERT对生成式任务和长序列建模支持不好。 ","permalink":"https://Rook1eChan.github.io/posts/bert/","summary":"\u003cp\u003e参考：https://zhuanlan.zhihu.com/p/403495863\u003c/p\u003e\n\u003ch1 id=\"1介绍\"\u003e1.介绍\u003c/h1\u003e\n\u003cp\u003eBERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，该模型在机器阅读理解顶级水平测试\u003ca href=\"https://zhida.zhihu.com/search?content_id=177795576\u0026amp;content_type=Article\u0026amp;match_order=1\u0026amp;q=SQuAD1.1\u0026amp;zhida_source=entity\"\u003eSQuAD1.1\u003c/a\u003e中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，\u003ca href=\"https://zhida.zhihu.com/search?content_id=177795576\u0026amp;content_type=Article\u0026amp;match_order=1\u0026amp;q=MultiNLI\u0026amp;zhida_source=entity\"\u003eMultiNLI\u003c/a\u003e准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。\u003c/p\u003e\n\u003cp\u003eBERT的网络架构使用的是\u003ca href=\"https://zhida.zhihu.com/search?content_id=177795576\u0026amp;content_type=Article\u0026amp;match_order=1\u0026amp;q=%E3%80%8AAttention+is+all+you+need%E3%80%8B\u0026amp;zhida_source=entity\"\u003e《Attention is all you need》\u003c/a\u003e中提出的多层Transformer结构。其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。Transformer的结构在NLP领域中已经得到了广泛应用。\u003c/p\u003e\n\u003cBR\u003e\n\u003ch1 id=\"2模型结构\"\u003e2.模型结构\u003c/h1\u003e\n\u003cp\u003e下图展示的是BERT的总体结构图，多个Transformer Encoder一层一层地堆叠起来，就组装成了BERT了，在论文中，作者分别用12层和24层Transformer Encoder组装了两套BERT模型，两套模型的参数总数分别为110M和340M。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"P1\" loading=\"lazy\" src=\"/bert/p1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eBERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非像Bi-LSTM那样把句子倒序输入一遍。在BERT之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。\u003c/p\u003e\n\u003ch2 id=\"21embedding\"\u003e2.1Embedding\u003c/h2\u003e\n\u003cp\u003eEmbedding由三种Embedding求和而成：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"p2\" loading=\"lazy\" src=\"/bert/p2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003etoken embedding\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e将输入的文本进行Word Piece分词，如playing切割成play，##ing，使用Word Piece是为了解决未登录词。tokenization后，在开头插入[CLS]，在每句话的末尾插入[SEP]。[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。\u003c/p\u003e\n\u003cp\u003eBert 在处理英文文本时只需要 30522 个词，Token Embeddings 层会将每个词转换成 768 维向量，如下图的例子中， 5 个Token 会被转换成一个 (5, 768) 的矩阵或 (1, 5, 768) 的张量。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"p3\" loading=\"lazy\" src=\"/bert/p3.jpg\"\u003e\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e\u003cstrong\u003esegment embedding\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e用来区分两种句子。bert在训练时包括两种任务，其一是MLM（masked language model，掩码语言模型），就是遮住某个词让模型去预测；其二是NSP（next sentence prediction，下一句预测），输入两个句子，让模型判断是否相关。这时候就需要segment embedding对两个句子做区别。\u003c/p\u003e\n\u003cp\u003e进行问答等需要预测下一句的任务时，segment embedding层把0赋值给第一个句子的各token，把1赋值给第二个句子的各token。在文本分类任务时，segment embedding全部为0。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"p4\" loading=\"lazy\" src=\"/bert/p4.jpg\"\u003e\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e\u003cstrong\u003eposition embedding\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e和transformer的实现不同，不是固定的三角函数，而是可学习的参数。\u003c/p\u003e\n\u003cp\u003eTransformer 中通过植入关于 Token 的相对位置或者绝对位置信息来表示序列的顺序信息。作者测试用学习的方法来得到 Position Embeddings，最终发现固定位置和相对位置效果差不多，所以最后用的是固定位置的，而正弦可以处理更长的 Sequence，且可以用前面位置的值线性表示后面的位置。\u003c/p\u003e","title":"BERT"},{"content":"简介：检索相关算法的学习\n1.TF-IDF 1.1原理 TF：term frequency（词频）\nIDF：inverse document frequency（逆文档频率）\n字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\nTF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n$TF=\\frac{某个词在文章中出现的次数}{文章的总词数}$\n考虑到文章长短不同，除以总词数进行标准化\n$IDF=log(\\frac{语料库的文章总数}{包含该词的文章数量+1})$\n加1防止不存在包含该词的文档时分母为0\n$TF-IDF=TF \\times IDF$\n优点：\n简单快速、易理解 缺点：\n没考虑词语的语义 仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 1.2自己实现 import math # 计算每句话的词频 def counter(word_list): wordcount = [] for doc in word_list: count = {} for word in doc: count[word] = count.get(word, 0) + 1 wordcount.append(count) return wordcount # 计算tf=某个词在文章中出现的总次数/文章的总词数 def tf(word, word_list): return word_list.get(word) / sum(word_list.values()) # 统计含有该单词的句子数 def count_sentence(word, wordcount): return sum(1 for i in wordcount if i.get(word)) # 计算idf=log(语料库中的文档总数/(包含该词的文档数+1)) def idf(word, wordcount): return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1 # tf-idf=tf*idf def tfidf(word, word_list, wordcount): return tf(word, word_list) * idf(word, wordcount) if __name__ == \u0026#34;__main__\u0026#34;: docs = [ \u0026#34;what is the weather like today\u0026#34;, \u0026#34;what is for dinner tonight\u0026#34;, \u0026#34;this is a question worth pondering\u0026#34;, \u0026#34;it is a beautiful day today\u0026#34; ] word_list = [] # 记录每个文档分词后的结果 for doc in docs: word_list.append(doc.split(\u0026#34; \u0026#34;)) # 使用停用词 # stopwords = [\u0026#34;is\u0026#34;, \u0026#34;the\u0026#34;] # for i in docs: # all_words = i.split() # new_words = [] # for j in all_words: # if j not in stopwords: # new_words.append(j) # word_list.append(new_words) wordcount = counter(word_list) # 统计每个文档词的次数 for cnt, doc in enumerate(wordcount): print(\u0026#34;doc{}\u0026#34;.format(cnt)) for word, _ in doc.items(): print(\u0026#34;word:{} --- TF-IDF:{}\u0026#34;.format(word, tfidf(word, doc, wordcount))) 1.3使用sklearn库 from sklearn.feature_extraction.text import TfidfVectorizer if __name__ == \u0026#34;__main__\u0026#34;: docs = [ \u0026#34;what is the weather like today\u0026#34;, \u0026#34;what is for dinner tonight\u0026#34;, \u0026#34;this is a question worth pondering\u0026#34;, \u0026#34;it is a beautiful day today\u0026#34; ] tfidf_vec = TfidfVectorizer() # 利用fit_transform得到TFIDF矩阵 tfidf_matrix = tfidf_vec.fit_transform(docs) # 利用get_feature_names_out得到不重复的单词 print(tfidf_vec.get_feature_names_out()) # 利用vocabulary_得到各单次的编号 print(tfidf_vec.vocabulary_) # 输出TFIDF矩阵，即每个文档中每个词的tfidf值 print(tfidf_matrix) 2.BM25 2.1原理 BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。\n对于TF-IDF算法，如果文档长，词语多，TF值就会很大。BM25通过b参数对文档长度进行打压，随着TF的增加，BM25score趋于稳定，如下图所示。\n符号说明:\n$ Q $: 查询，由一组词项组成 $ Q = (q_1, q_2, ..., q_n) $ $ D $: 文档 $ f_i $: 词项 $ q_i $ 在文档 $ D $ 中的出现频率（词频，TF） $ dl $: 文档长度（文档中的词项总数） $ avgdl $: 语料库中所有文档的平均长度 $ N $: 语料库中文档总数 $ n_i $: 包含词项 $ q_i $ 的文档数量 $ k $: 词频调节参数（通常取1.2~2.0） $ b $: 文档长度调节参数（通常取0.75） 参数 典型值 作用 $ k $ 1.2~2.0 控制词频饱和度，值越大饱和度变化越慢 $ b $ 0.75 控制文档长度归一化程度，0=不归一化，1=完全归一化 b 参数默认 0.75，主要是对长文档做惩罚。如果设置为 0，分数将与文档的长度无关。从下图可以看到，b=0时，L与分数无关，b=1时，L越大，分数打压越厉害。\n完整公式：\n$$ \\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\frac{{(k+1) \\cdot f_i}}{{f_i + k \\cdot (1 - b + b \\cdot \\frac{{\\lvert D \\rvert}}{{\\text{avgdl}}})}} \\cdot \\log\\left(\\frac{{N - n_i + 0.5}}{{n_i + 0.5}}\\right) $$单个词项得分\n对于查询中的单个词项 $ q_i $，其BM25得分为：\n$\\text{score}(q_i, D) = \\text{IDF}(q_i) \\cdot \\frac{f_i \\cdot (k_1 + 1)}{f_i + k_1 \\cdot (1 - b + b \\cdot \\frac{dl}{avgdl})}$\nIDF计算\n逆文档频率（IDF）的计算公式为：\n$\\text{IDF}(q_i) = \\log \\left( \\frac{N - n_i + 0.5}{n_i + 0.5} + 1 \\right)$\n整体查询得分\n对于完整查询 $ Q $，BM25得分为所有词项得分的总和： $\\text{BM25}(Q, D) = \\sum_{i=1}^{n} \\text{score}(q_i, D)$\n2.2自己实现 import math from collections import Counter class BM25: def __init__(self, docs, k1=1.5, b=0.75): \u0026#34;\u0026#34;\u0026#34; BM25算法的构造器 :param docs: 分词后的文档列表，每个文档是一个包含词汇的列表 :param k1: BM25算法中的调节参数k1 :param b: BM25算法中的调节参数b \u0026#34;\u0026#34;\u0026#34; self.docs = docs self.k1 = k1 self.b = b self.doc_len = [len(doc) for doc in docs] # 计算每个文档的长度 self.avgdl = sum(self.doc_len) / len(docs) # 计算所有文档的平均长度 self.doc_freqs = [] # 存储每个文档的词频 self.idf = {} # 存储每个词的逆文档频率 self.initialize() def initialize(self): \u0026#34;\u0026#34;\u0026#34; 初始化方法，计算所有词的逆文档频率 \u0026#34;\u0026#34;\u0026#34; df = {} # 用于存储每个词在多少不同文档中出现 for doc in self.docs: # 为每个文档创建一个词频统计 self.doc_freqs.append(Counter(doc)) # 更新df值 for word in set(doc): df[word] = df.get(word, 0) + 1 # 计算每个词的IDF值（出现的文档数越少，得分越高） for word, freq in df.items(): self.idf[word] = math.log((len(self.docs) - freq + 0.5) / (freq + 0.5) + 1) def score(self, doc, query): \u0026#34;\u0026#34;\u0026#34; 计算文档与查询的BM25得分 :param doc: 文档的索引 :param query: 查询词列表 :return: 该文档与查询的相关性得分 \u0026#34;\u0026#34;\u0026#34; score = 0.0 for word in query: if word in self.doc_freqs[doc]: freq = self.doc_freqs[doc][word] # 词在文档中的频率 # 应用BM25计算公式 score += (self.idf[word] * freq * (self.k1 + 1)) / ( freq + self.k1 * (1 - self.b + self.b * self.doc_len[doc] / self.avgdl)) return score if __name__ == \u0026#34;__main__\u0026#34;: # 示例文档集和查询 docs = [[\u0026#34;the\u0026#34;, \u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;fox\u0026#34;], [\u0026#34;the\u0026#34;, \u0026#34;lazy\u0026#34;, \u0026#34;dog\u0026#34;], [\u0026#34;the\u0026#34;, \u0026#34;quick\u0026#34;, \u0026#34;dog\u0026#34;], [\u0026#34;the\u0026#34;, \u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;fox\u0026#34;]] query = [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;] # 初始化BM25模型并计算得分 bm25 = BM25(docs) scores = [bm25.score(i, query) for i in range(len(docs))] print(scores) 2.3rank_bm25库 from rank_bm25 import BM25Okapi import jieba corpus = [ \u0026#34;BM25是一种常用的信息检索算法\u0026#34;, \u0026#34;这个Python库实现了BM25算法\u0026#34;, \u0026#34;信息检索是搜索引擎的核心技术\u0026#34;, \u0026#34;BM25比传统的TF-IDF效果更好\u0026#34;, \u0026#34;中文信息检索需要先进行分词处理\u0026#34;, \u0026#34;自然语言处理是人工智能的重要领域\u0026#34;, \u0026#34;Python是最受欢迎的编程语言之一\u0026#34;, ] tokenized_corpus = [list(jieba.cut(doc)) for doc in corpus] bm25 = BM25Okapi(tokenized_corpus) def search(query, top_n=3): tokenized_query = list(jieba.cut(query)) doc_scores = bm25.get_scores(tokenized_query) top_docs = bm25.get_top_n(tokenized_query, corpus, n=top_n) return doc_scores, top_docs query = \u0026#34;Python信息检索\u0026#34; print(f\u0026#34;查询: \u0026#39;{query}\u0026#39;\u0026#34;) scores, results = search(query) print(\u0026#34;\\n相关文档:\u0026#34;) for i, (score, doc) in enumerate(zip(scores, corpus)): print(f\u0026#34;{i}. [Score: {score:.2f}] {doc}\u0026#34;) print(\u0026#34;\\n最相关的3个文档:\u0026#34;) for i, doc in enumerate(results): print(f\u0026#34;{i+1}. {doc}\u0026#34;) 2.4bm25s 一个快速的bm25实现方法，在大数据上比其它实现要快，很好用。\n项目地址：xhluca/bm25s: Fast lexical search implementing BM25 in Python using Numpy, Numba and Scipy\n","permalink":"https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/","summary":"\u003cp\u003e\u003cem\u003e简介：检索相关算法的学习\u003c/em\u003e\u003c/p\u003e\n\u003cBR\u003e\n\u003ch1 id=\"1tf-idf\"\u003e1.TF-IDF\u003c/h1\u003e\n\u003ch2 id=\"11原理\"\u003e1.1原理\u003c/h2\u003e\n\u003cp\u003eTF：term frequency（词频）\u003c/p\u003e\n\u003cp\u003eIDF：inverse document frequency（逆文档频率）\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTF-IDF的主要思想是\u003c/strong\u003e：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e$TF=\\frac{某个词在文章中出现的次数}{文章的总词数}$\u003c/p\u003e\n\u003cp\u003e考虑到文章长短不同，除以总词数进行标准化\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e$IDF=log(\\frac{语料库的文章总数}{包含该词的文章数量+1})$\u003c/p\u003e\n\u003cp\u003e加1防止不存在包含该词的文档时分母为0\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e$TF-IDF=TF \\times IDF$\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e\u003cstrong\u003e优点：\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e简单快速、易理解\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e缺点：\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e没考虑词语的语义\u003c/li\u003e\n\u003cli\u003e仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。\u003c/li\u003e\n\u003cli\u003e没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cBR\u003e\n\u003ch2 id=\"12自己实现\"\u003e1.2自己实现\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003emath\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 计算每句话的词频\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ecounter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003edoc\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eword\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003edoc\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 计算tf=某个词在文章中出现的总次数/文章的总词数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"nb\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 统计含有该单词的句子数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ecount_sentence\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nb\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 计算idf=log(语料库中的文档总数/(包含该词的文档数+1))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eidf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003emath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003ecount_sentence\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# tf-idf=tf*idf\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etfidf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eidf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edocs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;what is the weather like today\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;what is for dinner tonight\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;this is a question worth pondering\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;it is a beautiful day today\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eword_list\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# 记录每个文档分词后的结果\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003edoc\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003edocs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edoc\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esplit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34; \u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# 使用停用词\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# stopwords = [\u0026#34;is\u0026#34;, \u0026#34;the\u0026#34;]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# for i in docs:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e#     all_words = i.split()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e#     new_words = []\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e#     for j in all_words:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e#         if j not in stopwords:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e#             new_words.append(j)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e#     word_list.append(new_words)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecounter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword_list\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# 统计每个文档词的次数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ecnt\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edoc\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003eenumerate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;doc\u003c/span\u003e\u003cspan class=\"si\"\u003e{}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eformat\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecnt\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003e_\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003edoc\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eitems\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;word:\u003c/span\u003e\u003cspan class=\"si\"\u003e{}\u003c/span\u003e\u003cspan class=\"s2\"\u003e --- TF-IDF:\u003c/span\u003e\u003cspan class=\"si\"\u003e{}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eformat\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etfidf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eword\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edoc\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewordcount\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cBR\u003e\n\u003ch2 id=\"13使用sklearn库\"\u003e1.3使用sklearn库\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003esklearn.feature_extraction.text\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eTfidfVectorizer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edocs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;what is the weather like today\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;what is for dinner tonight\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;this is a question worth pondering\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;it is a beautiful day today\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003etfidf_vec\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eTfidfVectorizer\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# 利用fit_transform得到TFIDF矩阵\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003etfidf_matrix\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etfidf_vec\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit_transform\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edocs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# 利用get_feature_names_out得到不重复的单词\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etfidf_vec\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_feature_names_out\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# 利用vocabulary_得到各单次的编号\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etfidf_vec\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evocabulary_\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# 输出TFIDF矩阵，即每个文档中每个词的tfidf值\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etfidf_matrix\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cBR\u003e\n\u003ch1 id=\"2bm25\"\u003e2.BM25\u003c/h1\u003e\n\u003ch2 id=\"21原理\"\u003e2.1原理\u003c/h2\u003e\n\u003cp\u003eBM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。\u003c/p\u003e","title":"Retrieval_Learning"},{"content":"ICLR2025，来自Google DeepMind团队的工作\nhttps://arxiv.org/abs/2410.04343v2\n0.目标 先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。\n目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。\n1.贡献 提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。 得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。 根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。 2.相关工作 2.1长上下文LLMs 早期采用稀疏/低秩核来减少内存需求。\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\nK. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.\nN. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.\nM. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020\n此外递归和状态空间模型（SSMs）被提出，作为基于transformer模型的有效替代方案。\nM. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024\nA. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, L. Derczynski, et al. RWKV: Reinventing rnns for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14048–14077, 2023a.\n对于因果LLMs，外推和插值方法已被证明在扩展上下文窗口长度方面非常有效。\nS. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023b.\nO. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\nY. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590–14604, 2023.\n最近在高效注意力机制方面的进展，使得LLMs能够训练和推理包含数百万个标记的输入序列。\nT. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact attention with IO-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022\nS. A. Jacobs, M. Tanaka, C. Zhang, M. Zhang, L. Song, S. Rajbhandari, and Y. He. DeepSpeed Ulysses:System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023.\nH. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nM. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n2.2上下文学习 上下文学习（ICL）提供了一种计算效率高的方法，通过依赖少量任务示例来提高模型在推理时的表现。\n为了进一步提升ICL性能，现有研究集中在预训练策略上，优化语言模型以实现上下文学习。\n此外，选择性使用少量示例也被证明有助于提高下游任务的表现。\n值得注意的是，重新格式化或找到最佳顺序的场景示例也能提高ICL的有效性。\n随着长情境语言模型的出现，在ICL中扩展示例数量成为可能。\n例如，Agarwal等人表明，多样本ICL可以减轻LLM中的预训练偏差，从而提高各种任务的ICL性能。\n2.3RAG RAG通过引入外部知识来提高语言模型的表现。\n和naiveRAG相比，优化检索阶段可以有效提升上下文相关性，提高生成质量。\nREPLUG使用语言模型作为监督来学习一个密集检索器模型。\n此外，编码文档可以增加知识检索并提高生成能力。\nIzacard和Grave（2021）利用融合解码器架构来编码多个问题-段落对，同时保持模型效率。\n或者，有选择地利用文档中的知识可以提高语言模型对无关上下文的鲁棒性。例如，RAFT提出使用负文档训练语言模型以提高生成质量和相关性（Zhang等人，2024）。与我们的工作同时，提出了长文档检索和数据集扩展以优化RAG性能（Jiang等人，2024；Shao等人，2024）。\n3.基于RAG的推理扩展策略 我们使用有效上下文长度来测量推理计算，有效上下文长度定义为 LLM 输出最终答案之前，所有迭代的输入token总数。每次的输入受到LLM上下文窗口的限制。忽略输出的token和检索的开销。\n3.1DRAG DRAG基于NaiveRAG构建，将检索到的topk文档和上下文示例集成到输入中。颠倒文档顺序，将排名较高的文档放在离查询近的位置。\n3.2IterDRAG 将查询分解为更简单的子查询，每个子查询执行检索并生成中间答案。解决完所有子查询后，将所有上下文、子查询、中间答案组合在一起，生成最终答案。\n由于现有的数据集不带有子查询和中间答案，所以让大模型使用约束解码（constrained decoding）并遵循self-ask格式生成example。\n生成example的步骤：\n在每一轮中，生成子查询，然后将查询到的文档交错放入prompt，再生成中间答案。\n最终答案生成。或者达到最大轮数后，强制生成最终答案。\n文档、子查询-中间答案、最终答案一起构成example。\n推理过程中，上下文示例添加到初始文档之前。\nIterRAG还将学习：1.将问题分解为简单可控的子问题 2.提取子问题的相关信息\n这一方法有助于提高RAG回答复杂问题的能力。\n具体操作见附录H。\n4.RAG性能和推理计算规模 4.1给定预算（最大有效上下文长度）下的最佳性能 限制最大输入token即$L_{max}$时，可以通过调整推理参数 $\\theta$ 来优化计算资源的使用。\n在 DRAG 中，可以调整 检索文档数量 ( k ) 和 上下文示例数量 ( m )； 在 IterDRAG 中，额外引入了 检索与生成的迭代次数 ( n )。 对于每个输入查询及其真实答案 $ (x_i, y_i) \\in \\mathcal{X} $，我们可以应用参数为 $ \\theta $ 的 RAG 推理策略 $ f $，得到预测结果 $ \\hat{y}_i = f(x_i; \\theta) $，并计算评估指标 $ P(y_i, \\hat{y}_i) $。\n为了研究 RAG 性能与推理计算量之间的关系，我们在不同的计算预算 $ L_{\\text{max}} $ 下采样，并通过枚举不同的 $ \\theta \\in \\Theta $ 来寻找该预算下的最优平均性能 $ P^*(L_{\\text{max}}) $：\n$P^*(L_{\\text{max}}) := \\max_{\\theta \\in \\Theta} \\left\\{ \\frac{1}{|\\mathcal{X}|} \\sum_i P(y_i, f(x_i; \\theta)) \\ \\Bigg| \\ \\forall i, l(x_i; \\theta) \\leq L_{\\text{max}} \\right\\}. \\quad$\n$ \\mathcal{X} $：测试集，包含输入查询 $ x_i $ 和真实答案 $ y_i $ 的配对 $ (x_i, y_i) $。 $ \\theta $：RAG 推理参数，包括： $ k $：检索的文档数量， $ m $：上下文示例（in-context examples）数量， $ n $：生成迭代次数（DRAG 中 $ n=1 $，IterDRAG 中 $ n \\geq 1 $）。 $ f(x_i; \\theta) $：使用参数 $ \\theta$ 的 RAG 策略对查询 $ x_i $ 的预测结果 $ \\hat{y}_i $。 $ P(y_i, \\hat{y}_i) $：评估指标（如准确率、F1 分数等），衡量预测答案 $ \\hat{y}_i $ 与真实答案 $ y_i $ 的匹配程度。 $ l(x_i; \\theta) $：对查询 $ x_i $ 使用参数 $ \\theta $ 时的实际上下文长度（即所有输入 tokens 的总和）。 $ L_{\\text{max}} $：预算约束，即允许的最大上下文长度。 在所有满足约束的 $ \\theta $ 中，选择使平均性能 $ \\frac{1}{|\\mathcal{X}|} \\sum_i P(\\cdot) $ 最大的参数组合。\n实验：\n评估Genmini 1.5 Flash（上下文窗口最高1M）在知识密集型问答数据集（Bamboogle、HotpotQA、MuSiQue 和 2WikiMultiHopQA）上的性能。\n评估指标为exact match（EM）、F1、Acc\n$L_{max} \\in \\{16k, 32k, 128k, 1M, 5M \\} $tokens\n对于DRAG，检索文档数量 $k \\in \\{0,1,2,5,10,20,50,100,200,500,1000\\}$，示例数量 $m \\in \\{0,2^0,2^1,...,2^8\\}$\n对于IterRAG，$n \\in \\{1,2,3,4,5\\}$\n模型：\nzero-shot QA（ZS QA）纯使用LLM自身知识 many-shots QA（MS QA）只加入m个示例 RAG 只使用k个文档 DRAG IterRAG 4.2总体性能 结论：在任意上下文长度、任意数据集中，都是DRAG、IterRAG效果较好；达不到指定的上下文长度时会被省略。而且最好的P值随上下文长度线性变化。\nDRAG、IterdDRAG的性能随上下文长度的扩展而不断提高，而其余方法很快达到峰值。DRAG 在较短的最大长度下表现出色，而 IterDRAG 在更长的有效上下文长度下更有效。\n4.3RAG推理缩放定律 随着有效上下文长度的扩大，最佳性能表现出线性增长。所以可以通过增加计算来提高 RAG 性能，从而在给定可用计算资源的情况下更准确地预测性能。 对于 $L_{max}$ 高于 $10^5$​，IterDRAG 继续通过交错检索和迭代生成进行有效扩展。说明其更适合长上下文推理。 上下文超过1M后，性能提升不明显。 4.4对于特定参数的缩放 文档和示例并非起到同样作用。对于固定配置，增加检索到的文档数量通常会带来更好的性能提升。但k和m也是有阈值的。 增加示例对IterRAG更有帮助，如示例从0-1时，IterRAG的性能明显提升；而示例对DRAG不明显。 5.长上下文 RAG 的推理计算分配 构建计算分配模型，目的是为了能根据 $L_{max}$ 求 $\\theta$。\n性能指标（𝑃）\n表示在数据集 $ X $ 上的表现（如准确率），建模为参数 $ \\theta $ 的函数。 资源参数（𝜃）\n定义为三维向量 $ \\theta := (k, m, n)^T $，包含： ( k )：使用的文档数量 ( m )：上下文示例（demonstrations/shots）的数量 ( n )：最大迭代/生成步数 信息量参数（𝑖）\n衡量输入内容的信息价值，定义为 $ i := (i_{\\text{doc}}, i_{\\text{shot}}, 0)^T $： $ i_{\\text{doc}} $：文档的信息量 通过“添加1篇文档 vs 零样本（zero-shot）的性能差异”计算。 $ i_{\\text{shot}} $：示例的信息量 通过“添加1个示例 vs 零样本的性能差异”计算。 $ i_{\\text{iter}} $ 被忽略（设为0），因实验中发现增加生成步数对性能无显著提升。 性能模型公式\n$P(\\theta) \\approx \\sigma((a + b \\odot i)^T \\log(\\theta) + c)$\n符号说明： $ \\odot $：逐元素相乘（Hadamard积）。 $ a, b \\in \\mathbb{R}^3 $：待估计参数，分别表示资源的基础效应和与信息量的交互效应。 $ c $：常数偏置项。 a、b、c都是在特定情况下拟合的。 $ \\log(\\theta) $：对资源向量逐元素取对数。 $ \\sigma $：Sigmoid函数。 $L_{max}$ 相当于 $\\theta$ 的限制条件；拟合出abc；目标是最大化 $P(\\theta)$ ，就可以得到最好的 $\\theta$。\n","permalink":"https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/","summary":"\u003cp\u003eICLR2025，来自Google DeepMind团队的工作\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2410.04343v2\"\u003ehttps://arxiv.org/abs/2410.04343v2\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"0目标\"\u003e0.目标\u003c/h2\u003e\n\u003cp\u003e先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。\u003c/p\u003e\n\u003cp\u003e目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。\u003c/p\u003e\n\u003ch2 id=\"1贡献\"\u003e1.贡献\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。\u003c/li\u003e\n\u003cli\u003e得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。\u003c/li\u003e\n\u003cli\u003e根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2相关工作\"\u003e2.相关工作\u003c/h2\u003e\n\u003ch3 id=\"21长上下文llms\"\u003e2.1长上下文LLMs\u003c/h3\u003e\n\u003cp\u003e早期采用稀疏/低秩核来减少内存需求。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. \u003cem\u003earXiv preprint\u003c/em\u003e \u003cem\u003earXiv:2004.05150\u003c/em\u003e, 2020.\u003c/p\u003e\n\u003cp\u003eK. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. \u003cem\u003earXiv preprint arXiv:2009.14794\u003c/em\u003e,2020.\u003c/p\u003e\n\u003cp\u003eN. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In \u003cem\u003eInternational Conference\u003c/em\u003e \u003cem\u003eon Learning Representations\u003c/em\u003e, 2019.\u003c/p\u003e","title":"Inference Scaling for Long-Context Retrieval Augmented Generation"},{"content":"1.Motivation 尽管有了RAG的帮助，LLM仍有可能给出与所提供知识不符的回答。因此需要构建一个数据集来检测幻觉。\n2.Contributions 提出RAGTruth，一个大规模词级别的幻觉检测数据集，由LLM自然产生（作者认为故意触发的幻觉与自然产生的幻觉存在差异） 对现有幻觉检测方法进行比较 提出了微调LLM用于幻觉检测的基线。Llama-2-13B在RAGTruth training data上微调后比得上GPT4 证明了使用微调得到的幻觉检测器，能降低幻觉 3.Related Work 4.Methods 1.Hallucination Taxonomy幻觉类型 本文将幻觉类型分为：\nEvident Conflict明显冲突：与提供的文本明显相反，容易辨别，如事实错误、拼写错误、数字错误。 Subtle Conflict轻微冲突：生成的信息与提供的文本有歧义，比如术语的替换，需要结合上下文判断。 Evident Introduction of Baseless Information明显引入无根据知识：生成的内容不在提供的信息之内。 Subtle Introduction of Baseless Information轻微引入无根据知识：生成内容超出了提供的信息，比如主观的假设或推断。 2.Response Generation回答生成 选择三个任务: Question Answering,Data-to-text Writing, and News Summarization.（问题回答、数据到文本的写作、新闻摘要），生成回答并人工标注幻觉部分。\nQuestion Answering：从MS MARCO选择与生活相关的QA，每个问题保留三段提取内容，然后使用LLM根据内容回答问题。 Data-to-text Writing：从Yelp数据集选择有关商家的结构化信息和用户的评论，用LLM生成对商家的描述。如果数据出现空值而大模型将其解释为“假”，认为这是出现了幻觉。 News Summarization：数据来自CNN/Daily Mail dataset+某新闻平台的新闻，使用LLM对每篇内容生成摘要。 使用的LLM：GPT-3.5-turbo-0613、GPT-4-0613、Mistral-7b-Instruct、Llama-2-7B-chat、 Llama-2-13B-chat、 Llama-2-70B-chat\n每个任务都用6个模型跑一遍，得到6个回答。\n5.Result 各项任务中幻觉类型的比例：\n如图2所示，在上下文中无根据的信息生成显著多于与上下文冲突的信息生成，尤其是在问答任务中。在两大类无根据信息和冲突信息中，更严重的幻觉，即明显的无根据信息和明显的冲突信息，占据了相当大的比例。这一观察结果说明即使有RAG，还是存在严重幻觉。\n数据转文本的任务幻觉率最高，可能与JSON格式有关。另外，较新的新闻的幻觉率不比过时新闻高，可能是由于较新的新闻的文本长度较短。\n各模型出现幻觉的比例：\n（span、density什么意思）\n表3显示，在我们收集的数据中，OpenAI的两个模型表现出显著较低的幻觉率。具体来说，GPT-4-0613的幻觉频率最低。为了更清晰地比较不同模型的幻觉率，我们计算了每个模型在三个任务中的幻觉密度。幻觉密度定义为每一百个单词响应中平均出现的幻觉跨度数。在Llama2系列中，除了数据总文本写作任务外，模型规模与幻觉密度之间存在明显的负相关关系。尽管Mistral-7B-Instruct模型在各种基准和排行榜上的表现强劲（Zheng等人，2023），但它生成的包含幻觉的回答数量最多。\n幻觉与文本长度的关系：\n对于上下文长度（CLB），只有新闻摘要呈现出上下文越长，越容易幻觉的特点。\n对于回答长度（RLB），都有回答越长，越容易幻觉的特点。\n幻觉与位置的关系：\n在问答和新闻摘要任务中，幻觉更倾向于出现在回答的末尾。数据到文本写作任务在前半部分较易出现幻觉。\n","permalink":"https://Rook1eChan.github.io/posts/ragtruth/","summary":"\u003ch1 id=\"1motivation\"\u003e1.Motivation\u003c/h1\u003e\n\u003cp\u003e尽管有了RAG的帮助，LLM仍有可能给出与所提供知识不符的回答。因此需要构建一个数据集来检测幻觉。\u003c/p\u003e\n\u003cbr\u003e\n\u003ch1 id=\"2contributions\"\u003e2.Contributions\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e提出RAGTruth，一个大规模词级别的幻觉检测数据集，由LLM自然产生（作者认为故意触发的幻觉与自然产生的幻觉存在差异）\u003c/li\u003e\n\u003cli\u003e对现有幻觉检测方法进行比较\u003c/li\u003e\n\u003cli\u003e提出了微调LLM用于幻觉检测的基线。Llama-2-13B在RAGTruth training data上微调后比得上GPT4\u003c/li\u003e\n\u003cli\u003e证明了使用微调得到的幻觉检测器，能降低幻觉\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"3related-work\"\u003e3.Related Work\u003c/h1\u003e\n\u003ch1 id=\"4methods\"\u003e4.Methods\u003c/h1\u003e\n\u003ch2 id=\"1hallucination-taxonomy幻觉类型\"\u003e1.Hallucination Taxonomy幻觉类型\u003c/h2\u003e\n\u003cp\u003e本文将幻觉类型分为：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eEvident Conflict明显冲突\u003c/strong\u003e：与提供的文本明显相反，容易辨别，如事实错误、拼写错误、数字错误。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSubtle Conflict轻微冲突\u003c/strong\u003e：生成的信息与提供的文本有歧义，比如术语的替换，需要结合上下文判断。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvident Introduction of Baseless Information明显引入无根据知识\u003c/strong\u003e：生成的内容不在提供的信息之内。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSubtle Introduction of Baseless Information轻微引入无根据知识\u003c/strong\u003e：生成内容超出了提供的信息，比如主观的假设或推断。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"2response-generation回答生成\"\u003e2.Response Generation回答生成\u003c/h2\u003e\n\u003cp\u003e选择三个任务: Question Answering,Data-to-text Writing, and News Summarization.（问题回答、数据到文本的写作、新闻摘要），生成回答并人工标注幻觉部分。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eQuestion Answering\u003c/strong\u003e：从MS MARCO选择与生活相关的QA，每个问题保留三段提取内容，然后使用LLM根据内容回答问题。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData-to-text Writing\u003c/strong\u003e：从Yelp数据集选择有关商家的结构化信息和用户的评论，用LLM生成对商家的描述。如果数据出现空值而大模型将其解释为“假”，认为这是出现了幻觉。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNews Summarization\u003c/strong\u003e：数据来自CNN/Daily Mail dataset+某新闻平台的新闻，使用LLM对每篇内容生成摘要。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e使用的LLM：GPT-3.5-turbo-0613、GPT-4-0613、Mistral-7b-Instruct、Llama-2-7B-chat、 Llama-2-13B-chat、 Llama-2-70B-chat\u003c/p\u003e\n\u003cp\u003e每个任务都用6个模型跑一遍，得到6个回答。\u003c/p\u003e\n\u003ch2 id=\"5result\"\u003e5.Result\u003c/h2\u003e\n\u003cp\u003e各项任务中幻觉类型的比例：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"f2\" loading=\"lazy\" src=\"/RAGTRUTH/f2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e如图2所示，在上下文中无根据的信息生成显著多于与上下文冲突的信息生成，尤其是在问答任务中。在两大类无根据信息和冲突信息中，更严重的幻觉，即明显的无根据信息和明显的冲突信息，占据了相当大的比例。这一观察结果说明即使有RAG，还是存在严重幻觉。\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e\u003cimg alt=\"t2\" loading=\"lazy\" src=\"/RAGTRUTH/t2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e数据转文本的任务幻觉率最高，可能与JSON格式有关。另外，较新的新闻的幻觉率不比过时新闻高，可能是由于较新的新闻的文本长度较短。\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e各模型出现幻觉的比例：\u003c/p\u003e\n\u003cp\u003e（span、density什么意思）\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"t2\" loading=\"lazy\" src=\"/RAGTRUTH/t3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e表3显示，在我们收集的数据中，OpenAI的两个模型表现出显著较低的幻觉率。具体来说，GPT-4-0613的幻觉频率最低。为了更清晰地比较不同模型的幻觉率，我们计算了每个模型在三个任务中的幻觉密度。幻觉密度定义为每一百个单词响应中平均出现的幻觉跨度数。在Llama2系列中，除了数据总文本写作任务外，模型规模与幻觉密度之间存在明显的负相关关系。尽管Mistral-7B-Instruct模型在各种基准和排行榜上的表现强劲（Zheng等人，2023），但它生成的包含幻觉的回答数量最多。\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e幻觉与文本长度的关系：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"t2\" loading=\"lazy\" src=\"/RAGTRUTH/t4.png\"\u003e\u003c/p\u003e\n\u003cp\u003e对于上下文长度（CLB），只有新闻摘要呈现出上下文越长，越容易幻觉的特点。\u003c/p\u003e\n\u003cp\u003e对于回答长度（RLB），都有回答越长，越容易幻觉的特点。\u003c/p\u003e\n\u003cBR\u003e\n\u003cp\u003e幻觉与位置的关系：\u003c/p\u003e\n\u003cp\u003e在问答和新闻摘要任务中，幻觉更倾向于出现在回答的末尾。数据到文本写作任务在前半部分较易出现幻觉。\u003c/p\u003e","title":"RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models"},{"content":" RAG，QA常用的数据集和评价标准 多为知识密集型、问答型数据集\n数据集 1.UltraDomain lightrag曾使用\n使用MemoRAG提出的Benchmark。\n在UltraDomain里，包含多个领域的数据，每个数据包括多本书。以cs为例，共含有100本书和100个对应的问题。该领域专注于计算机科学，涵盖数据科学和软件工程的关键领域。它特别强调机器学习和大数据处理，内容涉及推荐系统、分类算法以及使用Spark进行实时分析。：\n{ input: How does Spark Streaming enable real-time data processing? answers: [\u0026#39;Spark Streaming extends ...... \u0026#39;] context: \u0026#34;Whole Book......\u0026#34; length: 131651 context_id: 7bcef8714a477fd61fc8fb0d499b2cc3 _id: b2fd8d9c6d1499d521d778ce3d6d06fa label: cs meta: {\u0026#39;title\u0026#39;: \u0026#39;Machine Learning With Spark\u0026#39;, \u0026#39;authors\u0026#39;: \u0026#39;Nick Pentreath\u0026#39;} } 数据集地址：TommyChien/UltraDomain · Datasets at Hugging Face\nLightrag使用LLM生成问题-答案对\n生成问题的方法来自于From Local to Global: A Graph RAG Approach to Query-Focused Summarization\n提供文本，让大模型生成K个使用该数据集的用户身份（比如数据集是财经新闻，user就可能是收集金融市场趋势的财经记者），对于每个用户再生成N个任务，每个用户-任务提出M个高层次问题（理解整个数据集、无需提取具体事实）\nUser: A tech journalist looking for insights and trends in the tech industry Task: Understanding how tech leaders view the role of policy and regulation Questions: 1. Which episodes deal primarily with tech policy and government regulation? 2. How do guests perceive the impact of privacy laws on technology development? 3. Do any guests discuss the balance between innovation and ethical considerations? 4. What are the suggested changes to current policies mentioned by the guests? 5. Are collaborations between tech companies and governments discussed and how? 2.DAPR使用的数据集 MS MARCO、Natural Questions、MIRACL、Genomics 和 ConditionalQA\n3.HotpotQA 含有train（easy、medium、hard），test（distractor、full Wiki）\nDistractor：每个问题会提供 10 篇备选篇章，其中包含 2 段与问题答案相关的段落，8 段不相关的段落，这 10 篇文章限定了模型寻找答案的范围，相对较小。 Full Wiki：Full Wiki 属于开放域问答任务，模型需要从整个维基百科文档中抽取文档，然后再从文档中提取段落，最后从段落中抽取答案，数据范围是整个维基百科，范围要大得多，这使得任务更具挑战性。（实际上也是10篇文章，每篇的段落也没比distractor长多少）\n1.hotpot_train_v1.1.json\n总问题数量: 90447 问题类型分布: - comparison: 17456 (19.3%) - bridge: 72991 (80.7%) 难度分布: - medium: 56814 (62.8%) - hard: 15661 (17.3%) - easy: 17972 (19.9%) 问题长度分布: - 0-10: 0 (0.0%) - 11-20: 8 (0.0%) - 21-30: 96 (0.1%) - 31-40: 664 (0.7%) - 41-50: 3352 (3.7%) - 51-70: 18602 (20.6%) - 71-100: 31161 (34.5%) - 100+: 36564 (40.4%) 100左右最多 supporting_facts统计: - 几乎都是2，其次3、4，其他极少 context统计: - 几乎每个问题的上下文数量都在9-10 2.hotpot_dev_distractor_v1.json\n{ \u0026#34;_id\u0026#34;: \u0026#34;5a8b57f25542995d1e6f1371\u0026#34;, # 问题编号 \u0026#34;answer\u0026#34;: \u0026#34;yes\u0026#34;, # 回答（简短） \u0026#34;question\u0026#34;: \u0026#34;Were Scott Derrickson and Ed Wood of the same nationality?\u0026#34;, # 问题 \u0026#34;supporting_facts\u0026#34;: [ # 黄金段落所在文档的标题以及句子的编号 [\u0026#34;Scott Derrickson\u0026#34;, 0], [\u0026#34;Ed Wood\u0026#34;, 0] ], \u0026#34;context\u0026#34;: [ # 相关的文档，文档内包含多个段落 [ \u0026#34;Ed Wood (film)\u0026#34;, [ \u0026#34;Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.\u0026#34;, \u0026#34; The film concerns the period in Wood\u0026#39;s life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.\u0026#34;, \u0026#34; Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\u0026#34; ] ], [ \u0026#34;Scott Derrickson\u0026#34;, [ \u0026#34;Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\u0026#34;, \u0026#34; He lives in Los Angeles, California.\u0026#34;, \u0026#34; He is best known for directing horror films such as \\\u0026#34;Sinister\\\u0026#34;, \\\u0026#34;The Exorcism of Emily Rose\\\u0026#34;, and \\\u0026#34;Deliver Us From Evil\\\u0026#34;, as well as the 2016 Marvel Cinematic Universe installment, \\\u0026#34;Doctor Strange.\\\u0026#34;\u0026#34; ] ] ], \u0026#34;type\u0026#34;: \u0026#34;comparison\u0026#34;, # 问题类型 \u0026#34;level\u0026#34;: \u0026#34;hard\u0026#34; # 问题等级 } 总问题数量: 7405 问题类型分布: - comparison: 1487 (20.1%) - bridge: 5918 (79.9%) 难度分布: - hard: 7405 (100.0%) supporting_facts统计: - 几乎都是2，其次3、4，其他极少 - 平均每个问题的支持性事实数量: 2.43 - 最多支持性事实数量: 8 - 最少支持性事实数量: 2 context统计: - 10篇文章，2篇相关，8篇不相关 3.hotpot_dev_fullwiki_v1.json\n问题类型分布: - comparison: 1487 (20.1%) - bridge: 5918 (79.9%) 难度分布: - hard: 7405 (100.0%) 问题长度分布: - 0-10: 0 (0.0%) - 11-20: 0 (0.0%) - 21-30: 0 (0.0%) - 31-40: 49 (0.7%) - 41-50: 282 (3.8%) - 51-70: 1612 (21.8%) - 71-100: 2919 (39.4%) - 100+: 2543 (34.3%) supporting_facts数量分布: - 包含 2 个supporting_facts的问题: 4990 (67.4%) - 包含 3 个supporting_facts的问题: 1774 (24.0%) - 包含 5 个supporting_facts的问题: 80 (1.1%) - 包含 4 个supporting_facts的问题: 537 (7.3%) - 包含 7 个supporting_facts的问题: 9 (0.1%) - 包含 6 个supporting_facts的问题: 14 (0.2%) - 包含 8 个supporting_facts的问题: 1 (0.0%) context数量分布: - 10篇文章，属于是open-domain的开放域问答任务（没有人为设置负样本？） 4.2WikiMultiHopQA 论文链接： Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps - ACL Anthology\ngithub repo地址： Alab-NII/2wikimultihop\n数据地址：https://www.dropbox.com/s/npidmtadreo6df2/data.zip\n类似hotpotqa\n{ \u0026#34;_id\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;type\u0026#34;: [\t// 以下四种之一作为值。问题类型有：比较、推理、组合和桥接比较 \u0026#34;compositional\u0026#34;, \u0026#34;inference\u0026#34;, \u0026#34;bridge_comparison\u0026#34;, \u0026#34;comparison\u0026#34; ], \u0026#34;question\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;context\u0026#34;: [ //可组成corpus [ \u0026#34;str(Title)\u0026#34;, // 文档标题 [ \u0026#34;str(Sent)\u0026#34;, // 句子内容 \u0026#34;str(Sent)\u0026#34; // 句子内容 // ... ] ] // ... ], \u0026#34;supporting_facts\u0026#34;: [ //黄金段落 [\u0026#34;str(title)\u0026#34;, \u0026#34;int(sent_id)\u0026#34;] // 支持文档的标题和对应句子的序号（第几句） ], \u0026#34;evidences\u0026#34;: [ [\u0026#34;str(subject entity)\u0026#34;, \u0026#34;str(relation)\u0026#34;, \u0026#34;str(object entity)\u0026#34;] // 列表，每个元素是一个包含[主体实体, 关系, 客体实体]的三元组,有几组`supporting_facts`就有几组这个 ], \u0026#34;answer\u0026#34;: \u0026#34;str\u0026#34; } 5.Qasper NLP论文相关的问答\nhf地址：https://huggingface.co/datasets/allenai/qasper\n数据集地址：https://qasper-dataset.s3.us-west-2.amazonaws.com/qasper-train-dev-v0.3.tgz\n评价标准 1.Lightrag使用LLM评价 包括几个维度，和GraphRAG一致：\n• Comprehensiveness. How much detail does the answer provide to cover all aspects and details of the question?\n• Diversity. How varied and rich is the answer in providing different perspectives and insights on the question?\n• Empowerment. How well does the answer help the reader understand and make informed judgments about the topic?\n2.NDCG 一、NDCG是什么？ NDCG的全称是：Normalized Discounted Cumulative Gain(归一化折损累计增益)\n在搜索和推荐任务中，系统常返回一个item列表。如何衡量这个返回的列表是否优秀呢？\n例如，当我们检索【推荐排序】，网页返回了与推荐排序相关的链接列表。列表可能会是[A,B,C,G,D,E,F],也可能是[C,F,A,E,D]，现在问题来了，当系统返回这些列表时，怎么评价哪个列表更好？\n没错，NDCG就是用来评估排序结果的。搜索和推荐任务中比较常见。\n二、一点点来理解NDCG~ G-CG-DCG-NDCG\nGain: 表示一个列表中所有item的相关性分数。$rel(i)$表示$item(i)$相关性得分。$$Gain=rel(i)$$ Cumulative Gain: 表示对K个item的Gain进行累加。$CG_{k}=\\sum_{i=1}^{k}{rel(i)}$ CG只是单纯累加相关性，不考虑位置信息。 如果返回一个list_1=[A,B,C,D,E]，那list_1的CG为0.5+0.9+0.3+0.6+0.1=2.4\n如果返回一个list_2=[D,A,E,C,B]，那list_2的CG为0.6+0.5+0.1+0.3+0.9=2.4\n所以，顺序不影响CG得分。如果我们想评估不同顺序的影响，就需要使用另一个指标DCG来评估。\nDiscounted Cumulative Gain: 考虑排序顺序的因素，使得排名靠前的item增益更高，对排名靠后的item进行折损。 CG与顺序无关，而DCG评估了顺序的影响。DCG的思想是：list中item的顺序很重要，不同位置的贡献不同，一般来说，排在前面的item影响更大，排在后面的item影响较小。（例如一个返回的网页，肯定是排在前面的item会有更多人点击）。所以，相对CG来说，DCG使排在前面的item增加其影响，排在后面的item减弱其影响。\n$$DCG_{k}=\\sum_{i=1}^{k}{\\frac{rel(i)}{log_{2}(i+1)}}$$怎么实现这个思想呢？DCG在CG的基础上，给每个item的相关性比上log2(i+1)，i越大，log2(i+1)的值越大，相当于给每个item的相关性打个折扣，item越靠后，折扣越大。\n还是上面那个例子：\nlist_1=[A,B,C,D,E], 其对应计算如下：\ni rel(i) log(i+1) rel(i)/log(i+1) 1 = A 0.5 1 0.5 2 = B 0.9 1.59 0.57 3 = C 0.3 2 0.15 4 = D 0.6 2.32 0.26 5 = E 0.1 2.59 0.04 list_1的 DCG_1= 0.5+0.57+0.15+0.26+0.04=1.52\nlist_2=[D,A,E,C,B]，其对应计算如下：\ni rel(i) log(i+1) rel(i)/log(i+1) 1 = D 0.6 1 0.6 2 = A 0.5 1.59 0.31 3 = E 0.1 2 0.05 4 = C 0.3 2.32 0.13 5 = B 0.9 2.59 0.35 list_2的 DCG_2= 0.6+0.31+0.05+0.13+0.35=1.44\nDCG_1 \u0026gt; DCG_2, 所以在这个例子里list_1优于list_2。\n到这里，我们可以知道，使用DCG方法就可以对不同的list进行评估，那为什么后面还有一个NDCG呢？\nNDCG(Normalized DCG): 归一化折损累计增益 在NDCG之前，先了解一些IDGC(ideal DCG)\u0026ndash;理想的DCG，IDCG的依据是：是根据rel(i)降序排列，即排列到最好状态。算出最好排列的DCG，就是IDCG。\nIDCG=最好排列的DCG\n对于上述的例子，按照rel(i)进行降序排列的最好状态为list_best=[B,D,A,C,E]\ni rel(i) log(i+1) rel(i)/log(i+1) 1 = B 0.9 1 0.9 2 = D 0.6 1.59 0.38 3 = A 0.5 2 0.25 4 = C 0.3 2.32 0.13 5 = E 0.1 2.59 0.04 IDCG = list_best的DCG_best = 0.9+0.38+0.25+0.13+0.04=1.7 (理所当然，IDCG\u0026gt;DCG_1和DCG_2)\n因为不同query的搜索结果有多有少，所以不同query的DCG值就没有办法来做对比。所以提出NDCG。\n$$NDCG=\\frac{DCG}{IDCG}$$所以NDGC使用DCG/IDCG来表示，这样的话，NDCG就是一个相对值，那么不同query之间就可以通过NDCG值进行比较评估。\n3.Precision 所有检索到的结果中，有多少是应该是被检索到的\n$$Precision=\\frac{正确的结果}{返回的结果}$$4.Recall 5.F1 ","permalink":"https://Rook1eChan.github.io/posts/ragqa-%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%84%E4%BC%B0/","summary":"\u003cblockquote\u003e\n\u003cp\u003eRAG，QA常用的数据集和评价标准\n多为知识密集型、问答型数据集\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch1 id=\"数据集\"\u003e数据集\u003c/h1\u003e\n\u003ch2 id=\"1ultradomain\"\u003e1.UltraDomain\u003c/h2\u003e\n\u003cp\u003elightrag曾使用\u003c/p\u003e\n\u003cp\u003e使用\u003ca href=\"https://arxiv.org/abs/2409.05591\"\u003eMemoRAG\u003c/a\u003e提出的Benchmark。\u003c/p\u003e\n\u003cp\u003e在UltraDomain里，包含多个领域的数据，每个数据包括多本书。以cs为例，共含有100本书和100个对应的问题。该领域专注于计算机科学，涵盖数据科学和软件工程的关键领域。它特别强调机器学习和大数据处理，内容涉及推荐系统、分类算法以及使用Spark进行实时分析。：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eHow\u003c/span\u003e \u003cspan class=\"n\"\u003edoes\u003c/span\u003e \u003cspan class=\"n\"\u003eSpark\u003c/span\u003e \u003cspan class=\"n\"\u003eStreaming\u003c/span\u003e \u003cspan class=\"n\"\u003eenable\u003c/span\u003e \u003cspan class=\"n\"\u003ereal\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003etime\u003c/span\u003e \u003cspan class=\"n\"\u003edata\u003c/span\u003e \u003cspan class=\"n\"\u003eprocessing\u003c/span\u003e\u003cspan class=\"err\"\u003e?\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003eanswers\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;Spark Streaming extends ...... \u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \t\u003cspan class=\"n\"\u003econtext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Whole Book......\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e131651\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003econtext_id\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"n\"\u003ebcef8714a477fd61fc8fb0d499b2cc3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003e_id\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eb2fd8d9c6d1499d521d778ce3d6d06fa\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ecs\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003emeta\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;Machine Learning With Spark\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;authors\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;Nick Pentreath\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e数据集地址：\u003ca href=\"https://huggingface.co/datasets/TommyChien/UltraDomain\"\u003eTommyChien/UltraDomain · Datasets at Hugging Face\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eLightrag使用LLM生成问题-答案对\u003c/p\u003e\n\u003cp\u003e生成问题的方法来自于\u003ca href=\"https://arxiv.org/abs/2404.16130\"\u003eFrom Local to Global: A Graph RAG Approach to Query-Focused Summarization\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e提供文本，让大模型生成K个使用该数据集的用户身份（比如数据集是财经新闻，user就可能是收集金融市场趋势的财经记者），对于每个用户再生成N个任务，每个用户-任务提出M个高层次问题（理解整个数据集、无需提取具体事实）\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e User: A tech journalist looking for insights and trends in the tech industry\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e Task: Understanding how tech leaders view the role of policy and regulation\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e Questions:\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e 1. Which episodes deal primarily with tech policy and government regulation?\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e 2. How do guests perceive the impact of privacy laws on technology development?\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e 3. Do any guests discuss the balance between innovation and ethical considerations?\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e 4. What are the suggested changes to current policies mentioned by the guests?\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e 5. Are collaborations between tech companies and governments discussed and how?\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cBR\u003e\n\u003ch2 id=\"2dapr使用的数据集\"\u003e2.DAPR使用的数据集\u003c/h2\u003e\n\u003cp\u003eMS MARCO、Natural Questions、MIRACL、Genomics 和 ConditionalQA\u003c/p\u003e","title":"RAG,QA相关数据集及评价标准"},{"content":"1.Motivation 现有的神经检索（neural retrieval）的方法主要集中在短文本排序，在长篇文章中做检索效果并不好（由于自注意力机制token数量的限制；或者返回的文档过长，不便于用户使用）。另外，作者发现在先进检索器的检索错误中，半数错误与缺少上下文有关。\n比如：在A剧场中演出过的演员有哪些？如果只检索关键字“A剧场”，可能找不到答案，需要结合上下文找到“……在这里演出过……”的内容才是真正答案。\n因此，作者针对上下文强关联的任务建立了一个数据集，使用两类方法（hybrid retrieval with BM25、 contextualized passage representations）进行实验，并详细解释了实验结果。\n2.Related work Document Question Answering（DocQA）：要求模型回答关于输入文档的问题，通常假设文档在提问前就已给出。本文提出的(Document-Awarepassage Retrieval, DAPR)与DocQA类似，区别在于DAPR希望用户提问时不知道目标文档，由模型来寻找目标文档。 Long-document retrieval（长文档检索）：对于长文档检索有一些简单的方法：将文档中段落相关性的最大值作为文档的相关性（MaxP）；仅编码文档中的第一个段落（FirstP）……与DAPR相比，所有这些先前的工作都没有研究如何在考虑文档上下文的情况下检索段落。 Hybrid retrieval（混合检索）：对于一个查询使用多个检索系统（常常是BM25+神经检索） rank fusion（排名融合）——通过凸组合、互逆排名等方法将不同检索系统的个体排名合并为一个。 hierarchical retrieval（层次检索）——首先检索文档，然后从这些文档中检索段落。只适用于段落本身足以对查询做出响应的情况。 本文探讨了段落排名和文档排名结合的有效性。 Relation to pre-training tasks（和预训练任务的关系）：有的研究在预训练中加入上下文。但推理时仍然只关注独立的段落。 补充： NQ：谷歌的一个问答数据集 NDCG：评价检索序列的相关性和位置 共指信息：描述文本中不同表达式指向同一实体或概念的语言现象，如*“玛丽打开了门，她随后拿起包。”* → “她”与“玛丽”共指同一人。 共指消解（Coreference Resolution）：自动识别文本中所有指向同一实体的表达式并分组。 3.Method DAPR任务要求系统提取+排序。给出段落集合$C$，文档集合$D$，对于查询集合$q \\in Q$，检索系统$s$应该提取出最好的$K$个段落集合$R$。\n3.1NQ-Hard 对SOTA的检索器（DRAGON+,SPLADEv2, and ColBERTv2)使用NQ数据集，发现一半的错误来自于不了解上下文。将这些数据命名为NQ-hard，并分为4类：\n共指消解（CR）：关键的共指信息需要通过特定文档上下文来解析； 主要主题（MT）：只有了解文档的背景主题（通常是标题），才能回答查询； 多跳推理（MHR）：连接查询和查询相关段落中的实体的推理路径包括文档上下文中的其他节点； 缩写（AC）：在相关段落（或查询）中出现一个缩写，该缩写对应于查询（或相关段落）中的全称，文档上下文解释了这种映射； 3.2Datasets MS MARCO、Natural Questions、MIRACL、Genomics 和 ConditionalQA（具体处理方式见附录A）有语料库的直接用，没有的把黄金段落文本收集起来当语料库。（也是很神奇）\n3.3Evaluation 使用nDCG@10和recall@100做指标。\n将binary/3-scale转换为0-1/0-1-2，然后使用pytrec_eval计算指标。\n考虑到现实世界中的检索系统多用于零样本、跨领域的情景，本文进行了一项测试：在MS MARCO训练集训练，然后在MS MARCO测试集测试，作为域内评估；在其它四个数据集上测试，作为域外评估。\n4.Experiments 4.1基础检索器 BM25（使用PySerini的默认配置） neural retrievers：DRAGON+、SPLADEv2、ColBERTv2（在MS MARCO上训练） 4.2两种将上下文引入神经检索器的方法 4.2.1加入BM25的混合检索 （1）Rank fusion融合检索\n由于神经网络适合于检测512tokens内的段落，而BM25无长度限制，所以使用BM25检索整个文档，而使用神经检索器检索段落。\n相关性分数为：\n$$s_{convex}(q,p,d)=\\alpha\\hat s_{BM25}(q,p) + (1-\\alpha)\\hat s_{nueral}(q,d)$$（公式有误）\n$\\hat s_{BM25},\\hat s_{nueral}$的分数都是归一化的。归一化公式为：\n$$\\hat{s}(q, c) = \\frac{s(q, c) - m_q}{M_q - m_q}$$​\n$s(q, c)$：检索器对查询(q)和候选文本(c)（段落或文档）的原始分数。 $m_q和M_q$：当前查询(q)的Top候选结果中分数的最小值和最大值。 若候选文本(c)在某一检索器的结果中缺失（未进入Top列表），则其分数视为0。 分数计算示例：\n输入数据:\n5篇文章，每篇2个段落： 文章D1：段落P1, P2 文章D2：段落P3, P4 文章D3：段落P5, P6 文章D4：段落P7, P8 文章D5：段落P9, P10 文档级检索结果 (topk=2)： 得分：D1=0.8, D2=0.6（其他文档得分低于这两个） 段落级检索结果 (topk=2)： 得分：P3=0.9, P5=0.7（其他段落得分低于这两个） 段落到文档映射： pid2did = { \u0026#39;P1\u0026#39;:\u0026#39;D1\u0026#39;, \u0026#39;P2\u0026#39;:\u0026#39;D1\u0026#39;, \u0026#39;P3\u0026#39;:\u0026#39;D2\u0026#39;, \u0026#39;P4\u0026#39;:\u0026#39;D2\u0026#39;, \u0026#39;P5\u0026#39;:\u0026#39;D3\u0026#39;, \u0026#39;P6\u0026#39;:\u0026#39;D3\u0026#39;, \u0026#39;P7\u0026#39;:\u0026#39;D4\u0026#39;, \u0026#39;P8\u0026#39;:\u0026#39;D4\u0026#39;, \u0026#39;P9\u0026#39;:\u0026#39;D5\u0026#39;, \u0026#39;P10\u0026#39;:\u0026#39;D5\u0026#39; } 段落权重：假设passage_weight=0.4（文档权重自动为0.6） 计算过程:\n构建得分映射： did2score = {\u0026#39;D1\u0026#39;:0.8, \u0026#39;D2\u0026#39;:0.6} # 文档级top2 pid2score = {\u0026#39;P3\u0026#39;:0.9, \u0026#39;P5\u0026#39;:0.7} # 段落级top2 传播文档得分到段落： doc_pid2score = { \u0026#39;P3\u0026#39;: did2score[\u0026#39;D2\u0026#39;], # P3属于D2 → 0.6 \u0026#39;P5\u0026#39;: did2score[\u0026#39;D3\u0026#39;] # 但D3不在did2score中(文档级只返回了D1,D2) } 实际结果只有{'P3':0.6}，因为D3不在文档级top2中\nM2C2融合（假设是线性加权）： 对P3： 文档得分：0.6 段落得分：0.9 融合得分 = 0.6*0.6 + 0.9*0.4 = 0.36 + 0.36 = 0.72 对P5： 文档得分：无 → 可能视为0或忽略 段落得分：0.7 如果忽略文档部分：0.7*0.4 = 0.28 最终融合结果： fused = {\u0026#39;P3\u0026#39;:0.72, \u0026#39;P5\u0026#39;:0.28} （2）Hierarchical retrieval层次检索\n第一步先BM25检索文档，第二步神经检索搜索段落，并在第二步应用带有分数归一化的排名融合。\n4.2.2上下文化的段落表示 （1）**Prepending titles：**将文章标题放在每个段落的开头，并用特殊标记分割。可能会出现文档无标题或标题无意义。\n（2）**Prepending document keyphrases：**使用TopicRank算法（不知道在RAG中，和大模型抽取关键词相比如何）提取出文档的十个关键词，放在每个段落之前。\n（3）**Coreference resolution（共指消解）：**使用SpanBERT-large model，采用c2f-coref方法，在OntoNotes上微调。然后用模型对文档生成代词-先行词映射。将代词与文档中最早出现的先行词关联。将先行词放入括号，附在对应的代词后面。比如 “曾在该场地（xx剧场）”\n5.result 5.1混合检索\nrank fusion略好于hierarchical，但都不能解决NQ-Hard问题。说明这两种方法都只能提高自包含问题的表现。\n统计了检索性能随融合权重变化的曲线。发现NQ上的最佳融合权重不能直接转移到NQ-hard上。\n5.2上下文化段落表示\n添加标题和关键词都相当于添加摘要，提升效果相当；共指消解表现最差。\n在NQ-Hard问题（需要连接上下文的问题）的表现显著优于混合检索。\n6.Disscution 1.为什么混合检索在Hard问题上表现不佳\n通过MaxP方法将查询-段落的指标转为查询-文档的指标，作者发现rank fusion和hierarchical检索出的文档大都正确，说明它能找到相关文档，但段落排序表现非常差，说明这两种方法不能够有效的对段落进行排序。\n另外，作者提到fusion weight 在 NQ-Hard和普通问题中的变化趋势不同。所以使用混合检索不能同时在两类问题上达到最佳表现。\n2.为什么在Genomics上上下文表示结果变差\n计算了加/不加标题的文档与query的Jaccard相似度，只有Genomics加标题后值减小。说明添加标题引入了更多不相关的内容，帮倒忙。\nJaccard 相似度公式：$\\text{Jaccard}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n$A $ 和$B $：两个文本的词集合（或更广义的集合）。 $|A \\cap B|$: A 和 B 中**共同词（交集）**的数量。 $|A \\cup B| $: A 和 B 中**所有唯一词（并集）**的数量。 取值范围 $[0, 1]$（0 表示无重叠，1 表示完全一致）。\n优点：\n简单直观，适用于短文本或集合型数据 不依赖词序或语义（仅统计词重叠） 缺点：\n忽略语义：同义词（如“电脑”和“计算机”）会被视为不同词。 敏感于词频：未考虑词的重要性（如TF-IDF），可以给词添加权重作为改进。 长文本效果差：并集会急剧增大，导致相似度被低估。 3.错误分析\n在两种融合检索、三种上下文化的方法下，多跳推理MHR最难解决，缩写最易解决。使用共指消解不如直接添加标题，因为标题往往含有核心词，而共指消解多次在段落中插入内容，反而干扰了匹配。\n该文章中了ACL2024。在连接上下文方面使用的方法比较淳朴。主要是定义了NQ-Hard数据集，并在5个数据集、原始方法+2个混合检索+3个上下文检索上做了一系列实验，并很好的解释了现象。\n复现 把README里的loaddata和evaluation的代码复制到dapr根目录load.py和eval.py下\npip install -r requirements.txt\n有setup.py，在根目录下python setup.py build python setup.py install 解决module\u0026rsquo;dapr\u0026rsquo; not found\nsudo apt-get update\nConditionalQA数据集 ConditionalQA给定提问者特定情况下的情景，回答与英国政策相关的问题。每个问答实例都标注了来自英国政府政策网页的证据。我们将原始数据集中的所有此类网页作为语料库。每个网页最初被解析为HTML标签，我们将其视为段落，移除HTML标签，仅保留纯自然语言。对于每个问答实例，我们将情景和问题连接起来形成一个查询，并将相应的证据视为黄金相关段落。\n预处理后的train（原版的数据没有文章和句子的id，这是处理后的数据）\n{ \u0026#34;query\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;train-0\u0026#34;, //train条目编号 \u0026#34;text\u0026#34;: \u0026#34;My father, who was a widower and the owner of several large properties in Wales, died recently and apparently intestate. My paternal uncle is applying for probate, but I believe that I have a stronger claim. Do I have a greater right to probate in respect of my late father\u0026#39;s estate?\u0026#34; }, \u0026#34;judged_chunks\u0026#34;: [ //判断点，一个问题可能有多个黄金段落 { \u0026#34;chunk\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;74-24\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;p\u0026gt;You can apply to become the estate’s administrator if you are 18 or over and you are the most ‘entitled’ inheritor of the deceased’s estate. This is usually the deceased’s closest living relative.\u0026lt;/p\u0026gt;\u0026#34; }, \u0026#34;judgement\u0026#34;: 1, //都为1 \u0026#34;belonging_doc\u0026#34;: { //每个chunk都把\u0026#34;belonging_doc\u0026#34;详细写了一遍，导致文件很大 \u0026#34;id\u0026#34;: \u0026#34;74\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Applying for probate\u0026#34;, \u0026#34;chunks\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;74-0\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Overview\u0026#34; }, ... { \u0026#34;id\u0026#34;: \u0026#34;74-138\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Once you have probate you can start dealing with the estate.\u0026#34; } ], \u0026#34;candidate_chunk_ids\u0026#34;: [ \u0026#34;74-5\u0026#34;, ... \u0026#34;74-51\u0026#34; ] } } ... ] } 预处理后的test\n{ \u0026#34;query\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;dev-0\u0026#34;, //问题编号 \u0026#34;text\u0026#34;: \u0026#34;My brother and his wife are in prison for carrying out a large fraud scheme. Their 7 and 8 year old children have been living with me for the last 4 years. I want to become their Special Guardian to look after them permanently How long will it be before I hear back from the court?\u0026#34; }, //具体问题 \u0026#34;judged_chunks\u0026#34;: [ { \u0026#34;chunk\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;86-41\u0026#34;, //黄金匹配段落 \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;p\u0026gt;Within 10 days of receiving your application the court will send you a case number and a date for a meeting to set out:\u0026lt;/p\u0026gt;\u0026#34; }, \u0026#34;judgement\u0026#34;: 1, //不知道什么意思，取值应该全部为1 \u0026#34;belonging_doc\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;86\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Become a special guardian\u0026#34;, \u0026#34;chunks\u0026#34;: [ //chunks是该问题相关的法条界面的文本分割后的结果。一段话设置为一个chunk { \u0026#34;id\u0026#34;: \u0026#34;86-0\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;What is a special guardian\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;86-1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You can apply to be a child’s special guardian when they cannot live with their birth parents and adoption is not right for them.\u0026#34; }, ... { \u0026#34;id\u0026#34;: \u0026#34;86-62\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You might be able to get a special guardian allowance from the children’s services department of your local council.\u0026#34; } ], \u0026#34;candidate_chunk_ids\u0026#34;: [ //把chunkid重新排了一遍，不知道按照什么要求排的 \u0026#34;86-6\u0026#34;, \u0026#34;86-52\u0026#34;, ... \u0026#34;86-57\u0026#34;, \u0026#34;86-14\u0026#34;, \u0026#34;86-2\u0026#34;, \u0026#34;86-16\u0026#34; ] } } ] } prepending_titles/bm25/CQA\nquery是一个字典，包括id，content。DAPR把每句话所属文章的标题也作为一个title字段加进去了。比如{\u0026quot;id\u0026quot;: \u0026quot;0-0\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Child Tax Credit\u0026quot;, \u0026quot;contents\u0026quot;: \u0026quot;Overview\u0026quot;}\n这里的bm25使用了pyserini，是用java实现的。然后把corpus字典传给java接口去index。应该是把title和content并为一个字符串。而且是以一句话而不是一篇文章进行的index。\n那对于NQ这种没有文章标题，只有零散段落的怎么prepending_titles呢？\n","permalink":"https://Rook1eChan.github.io/posts/dapr-a-benchmark-on-document-aware-passage-retrieval/","summary":"\u003ch2 id=\"1motivation\"\u003e1.Motivation\u003c/h2\u003e\n\u003cp\u003e现有的神经检索（neural retrieval）的方法主要集中在短文本排序，在长篇文章中做检索效果并不好（由于自注意力机制token数量的限制；或者返回的文档过长，不便于用户使用）。另外，作者发现在先进检索器的检索错误中，半数错误与缺少上下文有关。\u003c/p\u003e\n\u003cp\u003e比如：在A剧场中演出过的演员有哪些？如果只检索关键字“A剧场”，可能找不到答案，需要结合上下文找到“……在这里演出过……”的内容才是真正答案。\u003c/p\u003e\n\u003cp\u003e因此，作者针对上下文强关联的任务建立了一个数据集，使用两类方法（hybrid retrieval with BM25、 contextualized passage representations）进行实验，并详细解释了实验结果。\u003c/p\u003e\n\u003cBR\u003e\n\u003ch2 id=\"2related-work\"\u003e2.Related work\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDocument Question Answering（DocQA）\u003c/strong\u003e：要求模型回答关于输入文档的问题，通常假设文档在提问前就已给出。本文提出的(Document-Awarepassage Retrieval, DAPR)与DocQA类似，区别在于DAPR希望用户提问时不知道目标文档，由模型来寻找目标文档。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLong-document retrieval（长文档检索）\u003c/strong\u003e：对于长文档检索有一些简单的方法：将文档中段落相关性的最大值作为文档的相关性（MaxP）；仅编码文档中的第一个段落（FirstP）……与DAPR相比，所有这些先前的工作都没有研究如何在考虑文档上下文的情况下检索段落。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHybrid retrieval（混合检索）\u003c/strong\u003e：对于一个查询使用多个检索系统（常常是BM25+神经检索）\n\u003cul\u003e\n\u003cli\u003erank fusion（排名融合）——通过凸组合、互逆排名等方法将不同检索系统的个体排名合并为一个。\u003c/li\u003e\n\u003cli\u003ehierarchical retrieval（层次检索）——首先检索文档，然后从这些文档中检索段落。只适用于段落本身足以对查询做出响应的情况。\u003c/li\u003e\n\u003cli\u003e本文探讨了段落排名和文档排名结合的有效性。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRelation to pre-training tasks（和预训练任务的关系）\u003c/strong\u003e：有的研究在预训练中加入上下文。但推理时仍然只关注独立的段落。\u003c/li\u003e\n\u003cli\u003e补充：\n\u003cul\u003e\n\u003cli\u003eNQ：谷歌的一个问答数据集\u003c/li\u003e\n\u003cli\u003eNDCG：评价检索序列的相关性和位置\u003c/li\u003e\n\u003cli\u003e共指信息：描述文本中不同表达式指向同一实体或概念的语言现象，如*“玛丽打开了门，\u003cstrong\u003e她\u003c/strong\u003e随后拿起包。”* → “她”与“玛丽”共指同一人。\u003c/li\u003e\n\u003cli\u003e共指消解（Coreference Resolution）：自动识别文本中所有指向同一实体的表达式并分组。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cBR\u003e\n\u003ch2 id=\"3method\"\u003e3.Method\u003c/h2\u003e\n\u003cp\u003eDAPR任务要求系统提取+排序。给出段落集合$C$，文档集合$D$，对于查询集合$q \\in Q$，检索系统$s$应该提取出最好的$K$个段落集合$R$。\u003c/p\u003e\n\u003ch3 id=\"31nq-hard\"\u003e3.1NQ-Hard\u003c/h3\u003e\n\u003cp\u003e对SOTA的检索器（DRAGON+,SPLADEv2, and ColBERTv2)使用NQ数据集，发现一半的错误来自于不了解上下文。将这些数据命名为\u003cstrong\u003eNQ-hard\u003c/strong\u003e，并分为4类：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e共指消解（CR）：关键的共指信息需要通过特定文档上下文来解析；\u003c/li\u003e\n\u003cli\u003e主要主题（MT）：只有了解文档的背景主题（通常是标题），才能回答查询；\u003c/li\u003e\n\u003cli\u003e多跳推理（MHR）：连接查询和查询相关段落中的实体的推理路径包括文档上下文中的其他节点；\u003c/li\u003e\n\u003cli\u003e缩写（AC）：在相关段落（或查询）中出现一个缩写，该缩写对应于查询（或相关段落）中的全称，文档上下文解释了这种映射；\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"nqques\" loading=\"lazy\" src=\"/DAPR/NQques.png\"\u003e\u003c/p\u003e\n\u003cBR\u003e\n\u003ch3 id=\"32datasets\"\u003e3.2Datasets\u003c/h3\u003e\n\u003cp\u003eMS MARCO、Natural Questions、MIRACL、Genomics 和 ConditionalQA（具体处理方式见附录A）有语料库的直接用，没有的把黄金段落文本收集起来当语料库。（也是很神奇）\u003c/p\u003e\n\u003cBR\u003e\n\u003ch3 id=\"33evaluation\"\u003e3.3Evaluation\u003c/h3\u003e\n\u003cp\u003e使用\u003cstrong\u003enDCG@10\u003c/strong\u003e和recall@100做指标。\u003c/p\u003e\n\u003cp\u003e将binary/3-scale转换为0-1/0-1-2，然后使用pytrec_eval计算指标。\u003c/p\u003e\n\u003cp\u003e考虑到现实世界中的检索系统多用于零样本、跨领域的情景，本文进行了一项测试：在MS MARCO训练集训练，然后在MS MARCO测试集测试，作为域内评估；在其它四个数据集上测试，作为域外评估。\u003c/p\u003e\n\u003cBR\u003e\n\u003ch2 id=\"4experiments\"\u003e4.Experiments\u003c/h2\u003e\n\u003ch3 id=\"41基础检索器\"\u003e4.1基础检索器\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBM25（使用PySerini的默认配置）\u003c/li\u003e\n\u003cli\u003eneural retrievers：DRAGON+、SPLADEv2、ColBERTv2（在MS MARCO上训练）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cBR\u003e\n\u003ch3 id=\"42两种将上下文引入神经检索器的方法\"\u003e4.2两种将上下文引入神经检索器的方法\u003c/h3\u003e\n\u003ch4 id=\"421加入bm25的混合检索\"\u003e4.2.1加入BM25的混合检索\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003e（1）Rank fusion融合检索\u003c/strong\u003e\u003c/p\u003e","title":"DAPR A Benchmark on Document-Aware Passage Retrieval"},{"content":"github提供给每个用户一个网址，用户可以建立自己的静态网站。\n一、Hugo hugo是一个快速搭建网站的工具，由go语言编写。\n1.安装hugo 到hugo的github标签页Tags · gohugoio/hugo选择一个版本，下载对应的安装包。比如hugo_extended_withdeploy_0.147.0_windows-amd64.zip。\n解压后，在根目录打开cmd，输入\nhugo new site YourSiteName 为你的网站建立文件夹。YourSiteName更改为你的网站的名字。 根目录会出现YourSiteName文件夹。\n3.将根目录的hugo.exe复制到YourSiteName里。 在YourSiteName文件夹里打开cmd，输入\nhugo server -D 会返回如下信息：\n| EN -------------------+----- Pages | 11 Paginator pages | 0 Non-page files | 0 Static files | 0 Processed images | 0 Aliases | 2 Cleaned | 0 Built in 79 ms Environment: \u0026#34;development\u0026#34; Serving pages from disk Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 在浏览器中输入http://localhost:1313/，显示Page Not Found，说明服务器正常运行，但是此时网站还没有页面。\n2.选择网站主题 在Hugo Themes选择你想要的theme，然后根据theme的安装说明操作就行了。 在此以PaperMod为例。官方安装教程界面：Installation · adityatelange/hugo-PaperMod Wiki\n安装PaperMod，可以：\n在你的网站的theme文件夹使用：\ngit clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 或者，在Tags · adityatelange/hugo-PaperMod选择版本，下载zip并解压到theme文件夹。\n在你的网站的根文件夹里的hugo.yml文件里添加\ntheme: [\u0026#34;PaperMod\u0026#34;] 3.新建一个笔记 在你的网站的根页面下使用cmd：\nhugo new posts/first.md YourSiteName/content/posts/first.md 就会建立，打开后，内容为：\n--- date: \u0026#39;2025-05-01T18:41:05+08:00\u0026#39; draft: true title: \u0026#39;first\u0026#39; --- 这三条短线围起来的是该笔记的属性。第一行是创建时间；第二行为false时表示草稿状态，改为true才会显示在网站中；第三行为该笔记的标题。之后还可以添加其他的属性。\n打开http://localhost:1313/，刷新后就能看到刚才创建的笔记了。如果没有就重新hugo server -D。\n你可以通过cmd，或者直接新建md文件来添加笔记。\n4.定制个人博客 4.1添加菜单 在hugo.yml文件中添加：\nmenu: main: - identifier: categories name: categories url: /categories/ weight: 10 - identifier: tags name: tags url: /tags/ weight: 20 - identifier: example name: example.org url: https://example.org weight: 30 在网站的右上角就能看到菜单了\n4.2置顶帖子 在笔记的md文件里添加：\n--- ... weight: 1 --- weight为正整数，表示笔记顺序。放到最顶上就设为1。\n4.3hugo.yaml的可选项 hugo.yaml是网站根目录的配置文件\n# 基础配置 baseURL: https://Rook1eChan.github.io # 网站部署的根URL（GitHub Pages地址） languageCode: zh-cn # 网站语言代码（简体中文） title: Chan\u0026#39;s Blog # 网站标题（ theme: [\u0026#34;PaperMod\u0026#34;] # 使用的主题（Hugo PaperMod主题） buildDrafts: false # 构建时是否包含草稿（false表示不构建草稿） # 主题参数配置 params: # 布局控制 ShowBreadCrumbs: true # 显示面包屑导航 ShowReadingTime: false # 隐藏文章阅读时间 ShowShareButtons: false # 隐藏分享按钮 ShowCodeCopyButtons: true # 显示代码复制按钮 # 搜索功能配置（使用Fuse.js） fuseOpts: isCaseSensitive: false # 搜索不区分大小写 shouldSort: true # 对搜索结果排序 location: 0 # 匹配位置权重 distance: 1000 # 匹配距离阈值 threshold: 0.4 # 匹配相似度阈值 minMatchCharLength: 0 # 最小匹配字符长度 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] # 搜索的字段范围 # 首页欢迎信息 homeInfoParams: Title: \u0026#34;你好，欢迎来到我的博客 \\U0001F44B\u0026#34; # 标题 Content: \u0026#34;welcome!\u0026#34; # 内容 # 社交媒体图标 socialIcons: - name: github # GitHub图标 url: \u0026#34;https://github.com/Rook1eChan\u0026#34; # GitHub个人主页 # 其他社交平台（已注释掉） # - name: twitter # url: \u0026#34;twitter.com\u0026#34; # 网站图标配置 assets: favicon: \u0026#34;/apple-touch-icon.png\u0026#34; # favicon路径 # 导航栏图标 label: icon: /apple-touch-icon.png # 导航栏图标路径 iconHeight: 35 # 图标高度（像素） # 输出格式配置 outputs: home: - HTML # 生成HTML页面 - RSS # 生成RSS订阅 - JSON # 生成JSON数据（可能用于搜索） # 内容标记配置 markup: highlight: codeFences: true # 启用代码块高亮 guessSyntax: true # 自动检测代码语言 hl_Lines: \u0026#39;\u0026#39; # 高亮指定行（未设置） lineNos: false # 不显示行号（与下面配置冲突） lineNumbersInTable: true # 用表格布局行号（避免复制时带行号） noClasses: false # 使用CSS类（必须为false） style: github # 代码高亮主题（github、monokai、solarized-dark、dracula） tabWidth: 4 # 代码缩进空格数 goldmark: renderer: unsafe: true # 允许渲染原始HTML/LaTeX math: true # 支持数学公式 # 导航菜单配置 menu: main: # 已注释的分类和标签菜单 # - identifier: categories # name: categories # url: /categories/ # weight: 10 - identifier: search # 搜索菜单项 name: search # 菜单显示名称（英文，与标识不一致） url: /search/ # 搜索页面路径 weight: 25 # 菜单项排序权重 baseURL: 网站的基础URL，这里是 \u0026ldquo;https://Rook1eChan.github.io\u0026rdquo;，必须要写，不然导航出现错误。不要写example.com\nlanguageCode: 网站语言代码，\u0026ldquo;zh-cn\u0026rdquo; 表示简体中文\ntitle: 网站标题\ntheme: 使用的主题\nbuildDrafts: false 表示不设置草稿文章，所有文章都会被展示\n显示相关:\nShowBreadCrumbs: true 显示面包屑导航 ShowReadingTime: false 不显示文章阅读时间 ShowShareButtons: false 不显示分享按钮 ShowCodeCopyButtons: true 显示代码块的复制按钮 搜索功能 (fuseOpts):\n配置了基于 Fuse.js 的搜索功能参数，包括不区分大小写、排序方式等 主页信息 (homeInfoParams):\nTitle：标题 Content：内容 主页社交媒体图标 (socialIcons):\n只启用了 GitHub 链接，指向 \u0026ldquo;https://github.com/Rook1eChan\u0026quot; Twitter 和 Facebook 链接 资源 (assets):\n设置了网站图标 (favicon) 为 \u0026ldquo;/apple-touch-icon.png\u0026rdquo; 标签 (label):\n设置了图标及其高度 指定了主页的输出格式为 HTML、RSS 和 JSON\n主菜单 (main) 中只配置了一个 \u0026ldquo;搜索\u0026rdquo; 项:\n标识符: \u0026ldquo;搜索\u0026rdquo; 名称: \u0026ldquo;search\u0026rdquo; URL: \u0026ldquo;/search/\u0026rdquo; 权重: 25 (用于菜单项排序) 分类和标签菜单项自选\n4.4更改字体 在 Hugo 项目下新建 assets/css/extended/custom.css ，写入\n/* 全局正文字体 - Noto Sans Simplified Chinese */ body, article { font-family: \u0026#34;Noto Sans SC\u0026#34;, sans-serif; } /* 代码块字体 - 保持等宽字体（如 JetBrains Mono 或系统默认） */ pre, code { font-family: \u0026#34;JetBrains Mono\u0026#34;, monospace; font-size: 0.9em; /* 可选：调整代码字体大小 */ } 在 themes/PaperMod/layouts/partials/head.html 中添加 Google Fonts 的链接\n\u0026lt;link href=\u0026#34;https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700\u0026amp;family=JetBrains+Mono\u0026amp;display=swap\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; 可以去Browse Fonts - Google Fonts选字体。代码怎么写问AI。\n4.5启用数学公式 以下方法可以使用$...$表示行内公式，$$...$$表示行间公式\n在themes\\PaperMod\\layouts\\partials\\math.html\u0026quot; 里加上：\n{{ if .Page.Params.math }} \u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [[\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;]], // 行内公式：$...$ 或 \\(...\\) displayMath: [[\u0026#39;$$\u0026#39;, \u0026#39;$$\u0026#39;], [\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;]] // 块级公式：$$...$$ 或 \\[...\\] } }; \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} 在themes\\PaperMod\\layouts\\_default\\baseof.html\u0026quot; 里加上：\n\u0026lt;head\u0026gt; ... {{ if .Param \u0026#34;math\u0026#34; }} {{ partialCached \u0026#34;math.html\u0026#34; . }} {{ end }} ... \u0026lt;/head\u0026gt; 在hugo.yaml里加上：\n# 内容标记配置 markup: goldmark: renderer: unsafe: true # 允许原始 HTML（某些数学公式需要） extensions: passthrough: enable: true delimiters: block: - [\u0026#39;$$\u0026#39;, \u0026#39;$$\u0026#39;] # 块级公式：$$...$$ - [\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;] # 或者 \\[...\\] inline: - [\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;] # 行内公式：$...$ - [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;] # 或者 \\(...\\) hugo.yaml不要加math: true，在每篇文章开头的参数里加上：\nparams: math: true 4.6侧边目录 在config.yml中，添加或修改params对应的配置为以下内容：\nparams: ShowToc: true TocOpen: true 虽然PaperMod原生就有目录，但是却是在顶部，便捷性几乎为0，放在侧边就会方便许多。\n在项目目录layouts/partials下添加toc.html文件，内容如下：\n{{- $headers := findRE \u0026#34;\u0026lt;h[1-6].*?\u0026gt;(.|\\n])+?\u0026lt;/h[1-6]\u0026gt;\u0026#34; .Content -}} {{- $has_headers := ge (len $headers) 1 -}} {{- if $has_headers -}} \u0026lt;aside id=\u0026#34;toc-container\u0026#34; class=\u0026#34;toc-container wide\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;toc\u0026#34;\u0026gt; \u0026lt;details {{if (.Param \u0026#34;TocOpen\u0026#34;) }} open{{ end }}\u0026gt; \u0026lt;summary accesskey=\u0026#34;c\u0026#34; title=\u0026#34;(Alt + C)\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;details\u0026#34;\u0026gt;{{- i18n \u0026#34;toc\u0026#34; | default \u0026#34;Table of Contents\u0026#34; }}\u0026lt;/span\u0026gt; \u0026lt;/summary\u0026gt; \u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt; {{- $largest := 6 -}} {{- range $headers -}} {{- $headerLevel := index (findRE \u0026#34;[1-6]\u0026#34; . 1) 0 -}} {{- $headerLevel := len (seq $headerLevel) -}} {{- if lt $headerLevel $largest -}} {{- $largest = $headerLevel -}} {{- end -}} {{- end -}} {{- $firstHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers 0) 1) 0)) -}} {{- $.Scratch.Set \u0026#34;bareul\u0026#34; slice -}} \u0026lt;ul\u0026gt; {{- range seq (sub $firstHeaderLevel $largest) -}} \u0026lt;ul\u0026gt; {{- $.Scratch.Add \u0026#34;bareul\u0026#34; (sub (add $largest .) 1) -}} {{- end -}} {{- range $i, $header := $headers -}} {{- $headerLevel := index (findRE \u0026#34;[1-6]\u0026#34; . 1) 0 -}} {{- $headerLevel := len (seq $headerLevel) -}} {{/* get id=\u0026#34;xyz\u0026#34; */}} {{- $id := index (findRE \u0026#34;(id=\\\u0026#34;(.*?)\\\u0026#34;)\u0026#34; $header 9) 0 }} {{- /* strip id=\u0026#34;\u0026#34; to leave xyz, no way to get regex capturing groups in hugo */ -}} {{- $cleanedID := replace (replace $id \u0026#34;id=\\\u0026#34;\u0026#34; \u0026#34;\u0026#34;) \u0026#34;\\\u0026#34;\u0026#34; \u0026#34;\u0026#34; }} {{- $header := replaceRE \u0026#34;\u0026lt;h[1-6].*?\u0026gt;((.|\\n])+?)\u0026lt;/h[1-6]\u0026gt;\u0026#34; \u0026#34;$1\u0026#34; $header -}} {{- if ne $i 0 -}} {{- $prevHeaderLevel := index (findRE \u0026#34;[1-6]\u0026#34; (index $headers (sub $i 1)) 1) 0 -}} {{- $prevHeaderLevel := len (seq $prevHeaderLevel) -}} {{- if gt $headerLevel $prevHeaderLevel -}} {{- range seq $prevHeaderLevel (sub $headerLevel 1) -}} \u0026lt;ul\u0026gt; {{/* the first should not be recorded */}} {{- if ne $prevHeaderLevel . -}} {{- $.Scratch.Add \u0026#34;bareul\u0026#34; . -}} {{- end -}} {{- end -}} {{- else -}} \u0026lt;/li\u0026gt; {{- if lt $headerLevel $prevHeaderLevel -}} {{- range seq (sub $prevHeaderLevel 1) -1 $headerLevel -}} {{- if in ($.Scratch.Get \u0026#34;bareul\u0026#34;) . -}} \u0026lt;/ul\u0026gt; {{/* manually do pop item */}} {{- $tmp := $.Scratch.Get \u0026#34;bareul\u0026#34; -}} {{- $.Scratch.Delete \u0026#34;bareul\u0026#34; -}} {{- $.Scratch.Set \u0026#34;bareul\u0026#34; slice}} {{- range seq (sub (len $tmp) 1) -}} {{- $.Scratch.Add \u0026#34;bareul\u0026#34; (index $tmp (sub . 1)) -}} {{- end -}} {{- else -}} \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; {{- end -}} {{- end -}} {{- end -}} {{- end }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#{{- $cleanedID -}}\u0026#34; aria-label=\u0026#34;{{- $header | plainify -}}\u0026#34;\u0026gt;{{- $header | safeHTML -}}\u0026lt;/a\u0026gt; {{- else }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#{{- $cleanedID -}}\u0026#34; aria-label=\u0026#34;{{- $header | plainify -}}\u0026#34;\u0026gt;{{- $header | safeHTML -}}\u0026lt;/a\u0026gt; {{- end -}} {{- end -}} \u0026lt;!-- {{- $firstHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers 0) 1) 0)) -}} --\u0026gt; {{- $firstHeaderLevel := $largest }} {{- $lastHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers (sub (len $headers) 1)) 1) 0)) }} \u0026lt;/li\u0026gt; {{- range seq (sub $lastHeaderLevel $firstHeaderLevel) -}} {{- if in ($.Scratch.Get \u0026#34;bareul\u0026#34;) (add . $firstHeaderLevel) }} \u0026lt;/ul\u0026gt; {{- else }} \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; {{- end -}} {{- end }} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/details\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/aside\u0026gt; \u0026lt;script\u0026gt; let activeElement; let elements; window.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, function (event) { checkTocPosition(); elements = document.querySelectorAll(\u0026#39;h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]\u0026#39;); // Make the first header active activeElement = elements[0]; const id = encodeURI(activeElement.getAttribute(\u0026#39;id\u0026#39;)).toLowerCase(); document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.add(\u0026#39;active\u0026#39;); }, false); window.addEventListener(\u0026#39;resize\u0026#39;, function(event) { checkTocPosition(); }, false); window.addEventListener(\u0026#39;scroll\u0026#39;, () =\u0026gt; { // Check if there is an object in the top half of the screen or keep the last item active activeElement = Array.from(elements).find((element) =\u0026gt; { if ((getOffsetTop(element) - window.pageYOffset) \u0026gt; 0 \u0026amp;\u0026amp; (getOffsetTop(element) - window.pageYOffset) \u0026lt; window.innerHeight/2) { return element; } }) || activeElement elements.forEach(element =\u0026gt; { const id = encodeURI(element.getAttribute(\u0026#39;id\u0026#39;)).toLowerCase(); if (element === activeElement){ document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.add(\u0026#39;active\u0026#39;); } else { document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.remove(\u0026#39;active\u0026#39;); } }) }, false); const main = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--article-width\u0026#39;), 10); const toc = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--toc-width\u0026#39;), 10); const gap = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--gap\u0026#39;), 10); function checkTocPosition() { const width = document.body.scrollWidth; if (width - main - (toc * 2) - (gap * 4) \u0026gt; 0) { document.getElementById(\u0026#34;toc-container\u0026#34;).classList.add(\u0026#34;wide\u0026#34;); } else { document.getElementById(\u0026#34;toc-container\u0026#34;).classList.remove(\u0026#34;wide\u0026#34;); } } function getOffsetTop(element) { if (!element.getClientRects().length) { return 0; } let rect = element.getBoundingClientRect(); let win = element.ownerDocument.defaultView; return rect.top + win.pageYOffset; } \u0026lt;/script\u0026gt; {{- end }} 在项目目录assets/css/extended下添加blank.css文件，内容如下：\n:root { --nav-width: 1380px; --article-width: 650px; --toc-width: 300px; } .toc { margin: 0 2px 40px 2px; border: 1px solid var(--border); background: var(--entry); border-radius: var(--radius); padding: 0.4em; } .toc-container.wide { position: absolute; height: 100%; border-right: 1px solid var(--border); left: calc((var(--toc-width) + var(--gap)) * -1); top: calc(var(--gap) * 2); width: var(--toc-width); } .wide .toc { position: sticky; top: var(--gap); border: unset; background: unset; border-radius: unset; width: 100%; margin: 0 2px 40px 2px; } .toc details summary { cursor: zoom-in; margin-inline-start: 20px; padding: 12px 0; } .toc details[open] summary { font-weight: 500; } .toc-container.wide .toc .inner { margin: 0; } .active { font-size: 110%; font-weight: 600; } .toc ul { list-style-type: circle; } .toc .inner { margin: 0 0 0 20px; padding: 0px 15px 15px 20px; font-size: 16px; /*目录显示高度*/ max-height: 83vh; overflow-y: auto; } .toc .inner::-webkit-scrollbar-thumb { /*滚动条*/ background: var(--border); border: 7px solid var(--theme); border-radius: var(--radius); } .toc li ul { margin-inline-start: calc(var(--gap) * 0.5); list-style-type: none; } .toc li { list-style: none; font-size: 0.95rem; padding-bottom: 5px; } .toc li a:hover { color: var(--secondary); } 二、在GitHubPage部署网站 基本思路：建立两个仓库，一个网站仓库负责展示页面，另一个源仓库负责存储源码、更新内容并自动更新同步到网站仓库。\n1.建立网站仓库 在Github页面点击最上面的加号-New repository Repository name 填写 你的GitHub用户名.github.io，这样GitHub才会把它识别为网站仓库 选择Public 点击绿色的Create repository\n2.建立源仓库 同上建立仓库，随便命名，Public或Private都行 这里我命名为mywebsite\n3.GitHub 个人访问令牌 (Token) 生成\u0026amp;配置 点击右上角 头像 选择 Settings 左侧菜单选择 Developer settings 选择 Personal access tokens → Tokens (classic) 点击 Generate new token → Generate new token (classic) 设置 Token 信息： Token name：输入名称（如 mywebsite） Expiration：选择 No expiration（永不过期） 权限勾选： ✅ repo（全仓库权限） ✅ admin:repo_hook（仓库管理权限） 点击绿色按钮 Generate token 重要：立即复制生成的密钥并妥善保存（离开页面后将无法再次查看） 进入源仓库 点击settings 左侧Secrets and variables-Actions New repository secret 填写刚才的名称和密钥 Add sercet 4.配置workflow脚本 在本地网站根目录新建文件夹及文件.github/workflows/hugo.yaml 写入：\nname: github pages # 名字自取 on: push: branches: - main jobs: deploy: # 任务名自取 runs-on: ubuntu-latest\t# 在什么环境运行任务 steps: - uses: actions/checkout@v2\t# 引用actions/checkout这个action，与所在的github仓库同名 with: submodules: true # Fetch Hugo themes (true OR recursive) 获取submodule主题 fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo\t# 步骤名自取 uses: peaceiris/actions-hugo@v2\t# hugo官方提供的action，用于在任务环境中获取hugo with: hugo-version: \u0026#39;latest\u0026#39;\t# 获取最新版本的hugo extended: true - name: Build run: hugo --minify\t# 使用hugo构建静态网页 - name: Deploy uses: peaceiris/actions-gh-pages@v3\t# 一个自动发布github pages的action with: # github_token: ${{ secrets.GITHUB_TOKEN }} 该项适用于发布到源码相同repo的情况，不能用于发布到其他repo external_repository: Rook1eChan/Rook1eChan.github.io\t# 发布到哪个repo personal_token: ${{ secrets.MYWEBSITE2 }}\t# 发布到其他repo需要提供上面生成的personal access token publish_dir: ./public\t# 注意这里指的是要发布哪个文件夹的内容，而不是指发布到目的仓库的什么位置，因为hugo默认生成静态网页到public文件夹，所以这里发布public文件夹里的内容 publish_branch: main\t# 发布到哪个branch 只需要更改personal_token和external_repository\n5. SSH 密钥配置 检查是否已有 SSH Key\nWindows： 进入 C:\\Users\\你的用户名\\.ssh，查看是否存在 id_rsa（私钥）和 id_rsa.pub（公钥）文件。\n若有，说明已生成过 SSH Key，可直接使用。 若无，需重新生成。 Linux：\ncd ~/.ssh ls 检查是否存在 id_rsa 和 id_rsa.pub 文件。\n生成 SSH Key（若无） 运行以下命令（替换 xxx@xxx.com 为你的 GitHub 注册邮箱）：\nssh-keygen -t rsa -C \u0026#34;xxx@xxx.com\u0026#34; 连续按 3 次回车（使用默认路径，不设密码）。 生成的文件： id_rsa：私钥（切勿泄露）。 id_rsa.pub：公钥（需添加到 GitHub）。 将公钥添加到 GitHub 复制公钥内容（id_rsa.pub）： 登录 GitHub → 点击头像 → Settings → SSH and GPG Keys → New SSH Key。 测试 SSH 连接 在终端运行： ssh -T git@github.com 若显示 Hi 你的用户名!，说明配置成功。 之后clone或push时都选择SSH地址，而不是https地址。\n6.上传 本地网站根目录使用cmd，git@github.com:XXX/mywebsite.git改为源仓库地址：\ngit init git add . git remote add origin git@github.com:XXX/mywebsite.git git commit -m \u0026#34;Update\u0026#34; git push -u origin main 然后在源仓库的Action下，能看到正在Deploy，变绿色说明成功。此时网站仓库已自动更新了内容。\n进入网站仓库-settings-Pages-Build and deployment选择Deploy from a branch 刚才workflow脚本里写的是main，这里就选择main\n然后进入xxx.github.io，就可以看到你的网站了！🎉\n三、如何更新网站内容 不要在github和本地同时更改内容！不然会导致内容不同步，无法push。\n最好是一直都在本地更改，然后push到源仓库。\n1.本地更改后，比如新建了笔记，在网站根目录使用cmd： 不要复制注释！\ngit init //初始化git文件夹 git add . //添加变更内容 git remote add origin git@github.com:XXX/mywebsite.git //最后一项改为源仓库的地址，如果使用ssh连接的就复制ssh地址 git commit -m \u0026#34;new\u0026#34; //设置本次操作的名称，new可以随便改 git push -u origin main //把本地文件push到github上，增量更新 常见问题：\ngit push -u origin main的时候报错： error: src refspec master does not match any error: failed to push some refs to \u0026lsquo;github.com:Rook1eChan/mywebsite.git\u0026rsquo;\n使用git branch -a，查看branch的名称是不是main。如果是master，就把main改为master。\nTo github.com:Rook1eChan/mywebsite.git ! [rejected] main -\u0026gt; main (fetch first) error: failed to push some refs to \u0026lsquo;github.com:Rook1eChan/mywebsite.git\u0026rsquo; hint: Updates were rejected because the remote contains work that you do not\n说明你的GitHub和本地不同步。不建议强制合并，可以把GitHub整个repo clone到本地另一个文件，把变化的文件手动更改，再把新文件夹push上去。\n感谢GitHub、Hugo和Deekseek。\n","permalink":"https://Rook1eChan.github.io/posts/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%BB%BA%E7%AB%8Bgithub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugopapermod/","summary":"\u003cp\u003egithub提供给每个用户一个网址，用户可以建立自己的静态网站。\u003c/p\u003e\n\u003ch2 id=\"一hugo\"\u003e一、Hugo\u003c/h2\u003e\n\u003cp\u003ehugo是一个快速搭建网站的工具，由go语言编写。\u003c/p\u003e\n\u003ch3 id=\"1安装hugo\"\u003e1.安装hugo\u003c/h3\u003e\n\u003cp\u003e到hugo的github标签页\u003ca href=\"https://github.com/gohugoio/hugo/tags\"\u003eTags · gohugoio/hugo\u003c/a\u003e选择一个版本，下载对应的安装包。比如\u003ccode\u003ehugo_extended_withdeploy_0.147.0_windows-amd64.zip\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e解压后，在根目录打开cmd，输入\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-cmd\" data-lang=\"cmd\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehugo new site YourSiteName\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e为你的网站建立文件夹。\u003ccode\u003eYourSiteName\u003c/code\u003e更改为你的网站的名字。\n根目录会出现YourSiteName文件夹。\u003c/p\u003e\n\u003cp\u003e3.将根目录的hugo.exe复制到YourSiteName里。\n在YourSiteName文件夹里打开cmd，输入\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-cmd\" data-lang=\"cmd\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehugo server -D\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e会返回如下信息：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-cmd\" data-lang=\"cmd\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                   \u003cspan class=\"p\"\u003e|\u003c/span\u003e EN\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e-------------------+-----\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Pages            \u003cspan class=\"p\"\u003e|\u003c/span\u003e 11\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Paginator pages  \u003cspan class=\"p\"\u003e|\u003c/span\u003e  0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Non-page files   \u003cspan class=\"p\"\u003e|\u003c/span\u003e  0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Static files     \u003cspan class=\"p\"\u003e|\u003c/span\u003e  0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Processed images \u003cspan class=\"p\"\u003e|\u003c/span\u003e  0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Aliases          \u003cspan class=\"p\"\u003e|\u003c/span\u003e  2\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Cleaned          \u003cspan class=\"p\"\u003e|\u003c/span\u003e  0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eBuilt in 79 ms\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eEnvironment: \u003cspan class=\"s2\"\u003e\u0026#34;development\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eServing pages from disk\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eRunning in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eWeb Server is available at http://localhost:1313/ (bind address 127.0.0.1)\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ePress Ctrl+C to stop\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e在浏览器中输入\u003ccode\u003ehttp://localhost:1313/\u003c/code\u003e，显示Page Not Found，说明服务器正常运行，但是此时网站还没有页面。\u003c/p\u003e","title":"从0开始建立Github个人博客(hugo\u0026PaperMod)"},{"content":"原文：Neural-IR Models.. Neural IR(Information Retrieval) is a… | by Muhammad Hammad Khan | Medium\n译文：【翻译】一文详解神经信息检索领域的最新进展 - 知乎\n神经信息检索(Neural Information Retrieval, Neural IR)是信息检索领域的一个重要研究课题。自从谷歌在2018年发布BERT以来，它在11个NLP任务上获得了最先进的结果，一举改变了整个NLP领域的研究范式。2019年1月，Nogueira和Cho在MS MARCO Passage Ranking测试集上首次使用BERT。从那时起，人们开始研究神经信息检索的范式，也提出了许多基于BERT的文本排序方法。这些方法用于多阶段搜索架构的重排阶段(Re-Ranker)。如下图所示。\nFigure1 展示了一个简化的多阶段搜索结构。第一步：倒排索引（Inverted Index）+BM25得分进行排序，得到topK文档，这一步也叫候选项生成（Candidates Generation）。第二步，通过基于BERT的上下文排序模型来确定前N个文档的最终排序。\n神经重排模型(Neural re-ranking models)一般可以分为以下四种，如Figure2所示：\n基于表征(representation-focused) 基于交互(interaction-focused) 全交互（也被称作交叉编码器,）(all-to-all interaction(cross encoder) ) 迟交互(late interaction) 1.基于表征——双塔模型(Bi-encoder Models) 双塔模型将Query和Doc分别表征为密集的向量嵌入，用向量相似度分数来估计Q和D的相关性。在训练时需要正负样本进行对比学习，因为如果只给模型看正样本，它会偷懒——把所有向量都变成一样的，这样“相似度”永远最高。负样本强迫模型学会区分相关和不相关的内容。\n在将模型训练好后，doc和query的表征可以独立进行，不用像交叉编码器那样每次都要把Query和Doc拼在一起重新计算。\n1.1密集段落检索器(Dense passage retriever, DPR) 论文：Dense Passage Retrieval for Open-Domain Question Answering EMNLP 2020, Facebook Research Code: github.com/facebookresearch/DPR 讲解博客：【IR 论文】DPR — 最早提出使用嵌入向量来检索文档的模型_dpr模型-CSDN博客\nDPR是一个应用于问答领域的双塔模型，旨在最大限度地提高查询与相关文档的相似度，同时最小化与非相关文档的相似度。DPR是RAG中R的经典方案。\n正样本往往数据集已给定，而负样本比较难选择。为此，DPR提出了一种Batch内负采样的技术，从同一批次的其他样本中选择样本作为负样本。这种方法是有效且高效的。\n1.2最近邻负对比估计 (Approximate nearest neighbour Negative Contrastive Estimation, ANCE) 该论文证明了强负样本能够加速模型收敛，提升模型性能。负样本分为易区别的和不易区别的，显然不易区别（即强负样本）的对模型学习帮助更大。本文使用ANN寻找强负样本。\nDPR和ANCE的结果表明，双塔编码器不如交叉编码器有效，因为丧失了查询与文档的交互。但是开销更小，因为文档可以单独预先处理。\n2.迟交互模型（Late Interaction Models） 2.1Contextualized Late Interaction over BERT, ColBERT v2论文：ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction ColBERT\u0026amp;ColBERTv2讲解：ColBERT和ColBERTv2:兼具Bi-encoder和cross-encoder优势的多向量排序模型-CSDN博客\nColBERT引入延迟交互机制，相比交叉编码器效率提升了很多。\n简单来说，将Query和Doc进行分词，每个token生成一个嵌入向量。计算Query中每个词与Doc所有词的向量相似度。Query每个词取最高分，所有最高分加起来得到Doc的最终得分。\n独立编码： Query向量：[如何, 治疗, 感冒] Doc向量：[感冒, 病毒, 休息, 维生素C] 计算MaxSim： 治疗 → 与Doc所有词相似度最高的是休息（得分0.7）。 感冒 → 匹配Doc中的感冒（得分1.0）。 总分：0.7 + 1.0 = 1.7 ColBERT既像双塔模型，可以单独计算Doc的向量并存储；又像交叉编码器，实现token级别的交互。\n双塔模型将整句话表征为一个向量，而ColBERT将每个token都表征为一个向量，显然后者更细粒度，对语义理解更深。但是存储Doc的每个token的向量所需的空间比传统的倒排索引大得多。这种大内存占用的特点使 ColBERT 在大型语料库情形下不占优势。\nColBERTv2在ColBERT基础上使用更先进的训练方法来微调模型，并通过残差压缩方法大幅减少存储成本。\n3.基于知识蒸馏的神经重排模型 蒸馏的主要用途是减小模型大小并降低整体推理成本。\n论文：Improving efficient neural ranking models with cross-architecture knowledge distillation.\n该作者使用MSMARCO数据集对教师模型进行微调，用它对所有训练三元组打分，构建一个新的数据集。最后，学生模型在这个新构建的数据集上使用Margin MSE Loss进行训练，该损失函数优化了查询与非相关文本及相关文本分数之间的边距(Margin)。\n论文：Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling\nTAS-B方法，在训练时把Query按照话题进行聚簇，每个Batch内均匀采样Query，防止随机采样导致一个Batch内的话题都相似。\n就目前来看，RAG的检索远没有检索领域来的复杂，只是用了关键字检索，没有微调。\n","permalink":"https://Rook1eChan.github.io/posts/neural-ir-models/","summary":"\u003cp\u003e原文：\u003ca href=\"https://medium.com/@mhammadkhan/neural-re-ranking-models-c0a67278f626\"\u003eNeural-IR Models.. Neural IR(Information Retrieval) is a… | by Muhammad Hammad Khan | Medium\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e译文：\u003ca href=\"https://zhuanlan.zhihu.com/p/545429612\"\u003e【翻译】一文详解神经信息检索领域的最新进展 - 知乎\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e神经信息检索(Neural Information Retrieval, Neural IR)是信息检索领域的一个重要研究课题。自从谷歌在2018年发布BERT以来，它在11个NLP任务上获得了最先进的结果，一举改变了整个NLP领域的研究范式。2019年1月，Nogueira和Cho在MS MARCO Passage Ranking测试集上首次使用BERT。从那时起，人们开始研究神经信息检索的范式，也提出了许多基于BERT的文本排序方法。这些方法用于\u003cstrong\u003e多阶段搜索架构的重排阶段(Re-Ranker)\u003c/strong\u003e。如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"/Neural-IR-Models-1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure1 展示了一个简化的多阶段搜索结构。第一步：倒排索引（Inverted Index）+BM25得分进行排序，得到topK文档，这一步也叫候选项生成（Candidates Generation）。第二步，通过基于BERT的上下文排序模型来确定前N个文档的最终排序。\u003c/p\u003e\n\u003cp\u003e神经重排模型(Neural re-ranking models)一般可以分为以下四种，如Figure2所示：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e基于表征(representation-focused)\u003c/li\u003e\n\u003cli\u003e基于交互(interaction-focused)\u003c/li\u003e\n\u003cli\u003e全交互（也被称作交叉编码器,）(all-to-all interaction(cross encoder) )\u003c/li\u003e\n\u003cli\u003e迟交互(late interaction)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"/Neural-IR-Models-2.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"1基于表征双塔模型bi-encoder-models\"\u003e1.基于表征——双塔模型(Bi-encoder Models)\u003c/h2\u003e\n\u003cp\u003e双塔模型将Query和Doc分别表征为密集的向量嵌入，用向量相似度分数来估计Q和D的相关性。在训练时\u003cstrong\u003e需要正负样本进行对比学习\u003c/strong\u003e，因为如果只给模型看正样本，它会偷懒——把所有向量都变成一样的，这样“相似度”永远最高。负样本强迫模型学会区分相关和不相关的内容。\u003c/p\u003e\n\u003cp\u003e在将模型训练好后，doc和query的表征可以独立进行，不用像交叉编码器那样每次都要把Query和Doc拼在一起重新计算。\u003c/p\u003e\n\u003ch3 id=\"11密集段落检索器dense-passage-retriever-dpr\"\u003e1.1密集段落检索器(Dense passage retriever, DPR)\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e论文：\u003ca href=\"https://aclanthology.org/2020.emnlp-main.550\"\u003eDense Passage Retrieval for Open-Domain Question Answering\u003c/a\u003e\nEMNLP 2020, Facebook Research\nCode: \u003ca href=\"https://github.com/facebookresearch/DPR\"\u003egithub.com/facebookresearch/DPR\u003c/a\u003e\n讲解博客：\u003ca href=\"https://blog.csdn.net/qq_45668004/article/details/138256448\"\u003e【IR 论文】DPR — 最早提出使用嵌入向量来检索文档的模型_dpr模型-CSDN博客\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eDPR是一个应用于问答领域的双塔模型，旨在最大限度地提高查询与相关文档的相似度，同时最小化与非相关文档的相似度。DPR是RAG中R的经典方案。\u003c/p\u003e\n\u003cp\u003e正样本往往数据集已给定，而负样本比较难选择。为此，DPR提出了一种Batch内负采样的技术，从同一批次的其他样本中选择样本作为负样本。这种方法是有效且高效的。\u003c/p\u003e\n\u003ch3 id=\"12最近邻负对比估计-approximate-nearest-neighbour-negative-contrastive-estimation-ance\"\u003e1.2最近邻负对比估计 (Approximate nearest neighbour Negative Contrastive Estimation, ANCE)\u003c/h3\u003e\n\u003cp\u003e该论文证明了强负样本能够加速模型收敛，提升模型性能。负样本分为易区别的和不易区别的，显然不易区别（即强负样本）的对模型学习帮助更大。本文使用ANN寻找强负样本。\u003c/p\u003e","title":"Neural-IR Models（博客）"}]