[{"content":"你好！\n","permalink":"https://Rook1eChan.github.io/posts/welcome/","summary":"\u003cp\u003e你好！\u003c/p\u003e","title":"欢迎来到我的博客"},{"content":"？\n","permalink":"https://Rook1eChan.github.io/posts/new2/","summary":"\u003cp\u003e？\u003c/p\u003e","title":"New2"},{"content":"原文：Neural-IR Models.. Neural IR(Information Retrieval) is a… | by Muhammad Hammad Khan | Medium\n译文：【翻译】一文详解神经信息检索领域的最新进展 - 知乎\n神经信息检索(Neural Information Retrieval, Neural IR)是信息检索领域的一个重要研究课题。自从谷歌在2018年发布BERT以来，它在11个NLP任务上获得了最先进的结果，一举改变了整个NLP领域的研究范式。2019年1月，Nogueira和Cho在MS MARCO Passage Ranking测试集上首次使用BERT。从那时起，人们开始研究神经信息检索的范式，也提出了许多基于BERT的文本排序方法。这些方法用于多阶段搜索架构的重排阶段(Re-Ranker)。如下图所示。\nFigure1 展示了一个简化的多阶段搜索结构。第一步：倒排索引（Inverted Index）+BM25得分进行排序，得到topK文档，这一步也叫候选项生成（Candidates Generation）。第二步，通过基于BERT的上下文排序模型来确定前N个文档的最终排序。\n神经重排模型(Neural re-ranking models)一般可以分为以下四种，如Figure2所示：\n基于表征(representation-focused) 基于交互(interaction-focused) 全交互（也被称作交叉编码器,）(all-to-all interaction(cross encoder) ) 迟交互(late interaction) 1.基于表征——双塔模型(Bi-encoder Models) 双塔模型将Query和Doc分别表征为密集的向量嵌入，用向量相似度分数来估计Q和D的相关性。在训练时需要正负样本进行对比学习，因为如果只给模型看正样本，它会偷懒——把所有向量都变成一样的，这样“相似度”永远最高。负样本强迫模型学会区分相关和不相关的内容。\n在将模型训练好后，doc和query的表征可以独立进行，不用像交叉编码器那样每次都要把Query和Doc拼在一起重新计算。\n1.1密集段落检索器(Dense passage retriever, DPR) 论文：Dense Passage Retrieval for Open-Domain Question Answering EMNLP 2020, Facebook Research Code: github.com/facebookresearch/DPR 讲解博客：【IR 论文】DPR — 最早提出使用嵌入向量来检索文档的模型_dpr模型-CSDN博客\nDPR是一个应用于问答领域的双塔模型，旨在最大限度地提高查询与相关文档的相似度，同时最小化与非相关文档的相似度。DPR是RAG中R的经典方案。\n正样本往往数据集已给定，而负样本比较难选择。为此，DPR提出了一种Batch内负采样的技术，从同一批次的其他样本中选择样本作为负样本。这种方法是有效且高效的。\n1.2最近邻负对比估计 (Approximate nearest neighbour Negative Contrastive Estimation, ANCE) 该论文证明了强负样本能够加速模型收敛，提升模型性能。负样本分为易区别的和不易区别的，显然不易区别（即强负样本）的对模型学习帮助更大。本文使用ANN寻找强负样本。\nDPR和ANCE的结果表明，双塔编码器不如交叉编码器有效，因为丧失了查询与文档的交互。但是开销更小，因为文档可以单独预先处理。\n2.迟交互模型（Late Interaction Models） 2.1Contextualized Late Interaction over BERT, ColBERT v2论文：ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction ColBERT\u0026amp;ColBERTv2讲解：ColBERT和ColBERTv2:兼具Bi-encoder和cross-encoder优势的多向量排序模型-CSDN博客\nColBERT引入延迟交互机制，相比交叉编码器效率提升了很多。\n简单来说，将Query和Doc进行分词，每个token生成一个嵌入向量。计算Query中每个词与Doc所有词的向量相似度。Query每个词取最高分，所有最高分加起来得到Doc的最终得分。\n独立编码： Query向量：[如何, 治疗, 感冒] Doc向量：[感冒, 病毒, 休息, 维生素C] 计算MaxSim： 治疗 → 与Doc所有词相似度最高的是休息（得分0.7）。 感冒 → 匹配Doc中的感冒（得分1.0）。 总分：0.7 + 1.0 = 1.7 ColBERT既像双塔模型，可以单独计算Doc的向量并存储；又像交叉编码器，实现token级别的交互。\n双塔模型将整句话表征为一个向量，而ColBERT将每个token都表征为一个向量，显然后者更细粒度，对语义理解更深。但是存储Doc的每个token的向量所需的空间比传统的倒排索引大得多。这种大内存占用的特点使 ColBERT 在大型语料库情形下不占优势。\nColBERTv2在ColBERT基础上使用更先进的训练方法来微调模型，并通过残差压缩方法大幅减少存储成本。\n3.基于知识蒸馏的神经重排模型 蒸馏的主要用途是减小模型大小并降低整体推理成本。\n论文：Improving efficient neural ranking models with cross-architecture knowledge distillation.\n该作者使用MSMARCO数据集对教师模型进行微调，用它对所有训练三元组打分，构建一个新的数据集。最后，学生模型在这个新构建的数据集上使用Margin MSE Loss进行训练，该损失函数优化了查询与非相关文本及相关文本分数之间的边距(Margin)。\n论文：Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling\nTAS-B方法，在训练时把Query按照话题进行聚簇，每个Batch内均匀采样Query，防止随机采样导致一个Batch内的话题都相似。\n就目前来看，RAG的检索远没有检索领域来的复杂，只是用了关键字检索，没有微调。\n","permalink":"https://Rook1eChan.github.io/posts/neural-ir-models/","summary":"\u003cp\u003e原文：\u003ca href=\"https://medium.com/@mhammadkhan/neural-re-ranking-models-c0a67278f626\"\u003eNeural-IR Models.. Neural IR(Information Retrieval) is a… | by Muhammad Hammad Khan | Medium\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e译文：\u003ca href=\"https://zhuanlan.zhihu.com/p/545429612\"\u003e【翻译】一文详解神经信息检索领域的最新进展 - 知乎\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e神经信息检索(Neural Information Retrieval, Neural IR)是信息检索领域的一个重要研究课题。自从谷歌在2018年发布BERT以来，它在11个NLP任务上获得了最先进的结果，一举改变了整个NLP领域的研究范式。2019年1月，Nogueira和Cho在MS MARCO Passage Ranking测试集上首次使用BERT。从那时起，人们开始研究神经信息检索的范式，也提出了许多基于BERT的文本排序方法。这些方法用于\u003cstrong\u003e多阶段搜索架构的重排阶段(Re-Ranker)\u003c/strong\u003e。如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"/Neural-IR-Models-1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure1 展示了一个简化的多阶段搜索结构。第一步：倒排索引（Inverted Index）+BM25得分进行排序，得到topK文档，这一步也叫候选项生成（Candidates Generation）。第二步，通过基于BERT的上下文排序模型来确定前N个文档的最终排序。\u003c/p\u003e\n\u003cp\u003e神经重排模型(Neural re-ranking models)一般可以分为以下四种，如Figure2所示：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e基于表征(representation-focused)\u003c/li\u003e\n\u003cli\u003e基于交互(interaction-focused)\u003c/li\u003e\n\u003cli\u003e全交互（也被称作交叉编码器,）(all-to-all interaction(cross encoder) )\u003c/li\u003e\n\u003cli\u003e迟交互(late interaction)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"/Neural-IR-Models-2.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"1基于表征双塔模型bi-encoder-models\"\u003e1.基于表征——双塔模型(Bi-encoder Models)\u003c/h2\u003e\n\u003cp\u003e双塔模型将Query和Doc分别表征为密集的向量嵌入，用向量相似度分数来估计Q和D的相关性。在训练时\u003cstrong\u003e需要正负样本进行对比学习\u003c/strong\u003e，因为如果只给模型看正样本，它会偷懒——把所有向量都变成一样的，这样“相似度”永远最高。负样本强迫模型学会区分相关和不相关的内容。\u003c/p\u003e\n\u003cp\u003e在将模型训练好后，doc和query的表征可以独立进行，不用像交叉编码器那样每次都要把Query和Doc拼在一起重新计算。\u003c/p\u003e\n\u003ch3 id=\"11密集段落检索器dense-passage-retriever-dpr\"\u003e1.1密集段落检索器(Dense passage retriever, DPR)\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e论文：\u003ca href=\"https://aclanthology.org/2020.emnlp-main.550\"\u003eDense Passage Retrieval for Open-Domain Question Answering\u003c/a\u003e\nEMNLP 2020, Facebook Research\nCode: \u003ca href=\"https://github.com/facebookresearch/DPR\"\u003egithub.com/facebookresearch/DPR\u003c/a\u003e\n讲解博客：\u003ca href=\"https://blog.csdn.net/qq_45668004/article/details/138256448\"\u003e【IR 论文】DPR — 最早提出使用嵌入向量来检索文档的模型_dpr模型-CSDN博客\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eDPR是一个应用于问答领域的双塔模型，旨在最大限度地提高查询与相关文档的相似度，同时最小化与非相关文档的相似度。DPR是RAG中R的经典方案。\u003c/p\u003e\n\u003cp\u003e正样本往往数据集已给定，而负样本比较难选择。为此，DPR提出了一种Batch内负采样的技术，从同一批次的其他样本中选择样本作为负样本。这种方法是有效且高效的。\u003c/p\u003e\n\u003ch3 id=\"12最近邻负对比估计-approximate-nearest-neighbour-negative-contrastive-estimation-ance\"\u003e1.2最近邻负对比估计 (Approximate nearest neighbour Negative Contrastive Estimation, ANCE)\u003c/h3\u003e\n\u003cp\u003e该论文证明了强负样本能够加速模型收敛，提升模型性能。负样本分为易区别的和不易区别的，显然不易区别（即强负样本）的对模型学习帮助更大。本文使用ANN寻找强负样本。\u003c/p\u003e","title":"Neural-IR Models（博客）"},{"content":"","permalink":"https://Rook1eChan.github.io/posts/note2/","summary":"","title":"Note2"},{"content":"你好！\n","permalink":"https://Rook1eChan.github.io/posts/note1/","summary":"\u003cp\u003e你好！\u003c/p\u003e","title":"Note1"}]