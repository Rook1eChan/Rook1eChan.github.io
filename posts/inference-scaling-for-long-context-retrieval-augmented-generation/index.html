<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>Inference Scaling for Long-Context Retrieval Augmented Generation | Chan's Blog</title>
<meta name=keywords content><meta name=description content="ICLR2025，来自Google DeepMind团队的工作
https://arxiv.org/abs/2410.04343v2
0.目标
先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。
目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。
1.贡献

提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。
得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。
根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。

2.相关工作
2.1长上下文LLMs
早期采用稀疏/低秩核来减少内存需求。

I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019."><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/"><meta property="og:site_name" content="Chan's Blog"><meta property="og:title" content="Inference Scaling for Long-Context Retrieval Augmented Generation"><meta property="og:description" content="ICLR2025，来自Google DeepMind团队的工作
https://arxiv.org/abs/2410.04343v2
0.目标 先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。
目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。
1.贡献 提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。 得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。 根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。 2.相关工作 2.1长上下文LLMs 早期采用稀疏/低秩核来减少内存需求。
I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019."><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-07T23:04:00+08:00"><meta property="article:modified_time" content="2025-05-07T23:04:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Inference Scaling for Long-Context Retrieval Augmented Generation"><meta name=twitter:description content="ICLR2025，来自Google DeepMind团队的工作
https://arxiv.org/abs/2410.04343v2
0.目标
先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。
目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。
1.贡献

提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。
得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。
根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。

2.相关工作
2.1长上下文LLMs
早期采用稀疏/低秩核来减少内存需求。

I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Inference Scaling for Long-Context Retrieval Augmented Generation","item":"https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Inference Scaling for Long-Context Retrieval Augmented Generation","name":"Inference Scaling for Long-Context Retrieval Augmented Generation","description":"ICLR2025，来自Google DeepMind团队的工作\nhttps://arxiv.org/abs/2410.04343v2\n0.目标 先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。\n目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。\n1.贡献 提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。 得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。 根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。 2.相关工作 2.1长上下文LLMs 早期采用稀疏/低秩核来减少内存需求。\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\nK. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.\nN. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.\n","keywords":[],"articleBody":"ICLR2025，来自Google DeepMind团队的工作\nhttps://arxiv.org/abs/2410.04343v2\n0.目标 先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。\n目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。\n1.贡献 提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。 得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。 根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。 2.相关工作 2.1长上下文LLMs 早期采用稀疏/低秩核来减少内存需求。\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\nK. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.\nN. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.\nM. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020\n此外递归和状态空间模型（SSMs）被提出，作为基于transformer模型的有效替代方案。\nM. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024\nA. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, L. Derczynski, et al. RWKV: Reinventing rnns for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14048–14077, 2023a.\n对于因果LLMs，外推和插值方法已被证明在扩展上下文窗口长度方面非常有效。\nS. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023b.\nO. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\nY. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590–14604, 2023.\n最近在高效注意力机制方面的进展，使得LLMs能够训练和推理包含数百万个标记的输入序列。\nT. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact attention with IO-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022\nS. A. Jacobs, M. Tanaka, C. Zhang, M. Zhang, L. Song, S. Rajbhandari, and Y. He. DeepSpeed Ulysses:System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023.\nH. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nM. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n2.2上下文学习 上下文学习（ICL）提供了一种计算效率高的方法，通过依赖少量任务示例来提高模型在推理时的表现。\n为了进一步提升ICL性能，现有研究集中在预训练策略上，优化语言模型以实现上下文学习。\n此外，选择性使用少量示例也被证明有助于提高下游任务的表现。\n值得注意的是，重新格式化或找到最佳顺序的场景示例也能提高ICL的有效性。\n随着长情境语言模型的出现，在ICL中扩展示例数量成为可能。\n例如，Agarwal等人表明，多样本ICL可以减轻LLM中的预训练偏差，从而提高各种任务的ICL性能。\n2.3RAG RAG通过引入外部知识来提高语言模型的表现。\n和naiveRAG相比，优化检索阶段可以有效提升上下文相关性，提高生成质量。\nREPLUG使用语言模型作为监督来学习一个密集检索器模型。\n此外，编码文档可以增加知识检索并提高生成能力。\nIzacard和Grave（2021）利用融合解码器架构来编码多个问题-段落对，同时保持模型效率。\n或者，有选择地利用文档中的知识可以提高语言模型对无关上下文的鲁棒性。例如，RAFT提出使用负文档训练语言模型以提高生成质量和相关性（Zhang等人，2024）。与我们的工作同时，提出了长文档检索和数据集扩展以优化RAG性能（Jiang等人，2024；Shao等人，2024）。\n3.基于RAG的推理扩展策略 我们使用有效上下文长度来测量推理计算，有效上下文长度定义为 LLM 输出最终答案之前，所有迭代的输入token总数。每次的输入受到LLM上下文窗口的限制。忽略输出的token和检索的开销。\n3.1DRAG DRAG基于NaiveRAG构建，将检索到的topk文档和上下文示例集成到输入中。颠倒文档顺序，将排名较高的文档放在离查询近的位置。\n3.2IterDRAG 将查询分解为更简单的子查询，每个子查询执行检索并生成中间答案。解决完所有子查询后，将所有上下文、子查询、中间答案组合在一起，生成最终答案。\n由于现有的数据集不带有子查询和中间答案，所以让大模型使用约束解码（constrained decoding）并遵循self-ask格式生成example。\n生成example的步骤：\n在每一轮中，生成子查询，然后将查询到的文档交错放入prompt，再生成中间答案。\n最终答案生成。或者达到最大轮数后，强制生成最终答案。\n文档、子查询-中间答案、最终答案一起构成example。\n推理过程中，上下文示例添加到初始文档之前。\nIterRAG还将学习：1.将问题分解为简单可控的子问题 2.提取子问题的相关信息\n这一方法有助于提高RAG回答复杂问题的能力。\n具体操作见附录H。\n4.RAG性能和推理计算规模 4.1给定预算（最大有效上下文长度）下的最佳性能 限制最大输入token即$L_{max}$时，可以通过调整推理参数 $\\theta$ 来优化计算资源的使用。\n在 DRAG 中，可以调整 检索文档数量 ( k ) 和 上下文示例数量 ( m )； 在 IterDRAG 中，额外引入了 检索与生成的迭代次数 ( n )。 对于每个输入查询及其真实答案 $ (x_i, y_i) \\in \\mathcal{X} $，我们可以应用参数为 $ \\theta $ 的 RAG 推理策略 $ f $，得到预测结果 $ \\hat{y}_i = f(x_i; \\theta) $，并计算评估指标 $ P(y_i, \\hat{y}_i) $。\n为了研究 RAG 性能与推理计算量之间的关系，我们在不同的计算预算 $ L_{\\text{max}} $ 下采样，并通过枚举不同的 $ \\theta \\in \\Theta $ 来寻找该预算下的最优平均性能 $ P^*(L_{\\text{max}}) $：\n$P^*(L_{\\text{max}}) := \\max_{\\theta \\in \\Theta} \\left\\{ \\frac{1}{|\\mathcal{X}|} \\sum_i P(y_i, f(x_i; \\theta)) \\ \\Bigg| \\ \\forall i, l(x_i; \\theta) \\leq L_{\\text{max}} \\right\\}. \\quad$\n$ \\mathcal{X} $：测试集，包含输入查询 $ x_i $ 和真实答案 $ y_i $ 的配对 $ (x_i, y_i) $。 $ \\theta $：RAG 推理参数，包括： $ k $：检索的文档数量， $ m $：上下文示例（in-context examples）数量， $ n $：生成迭代次数（DRAG 中 $ n=1 $，IterDRAG 中 $ n \\geq 1 $）。 $ f(x_i; \\theta) $：使用参数 $ \\theta$ 的 RAG 策略对查询 $ x_i $ 的预测结果 $ \\hat{y}_i $。 $ P(y_i, \\hat{y}_i) $：评估指标（如准确率、F1 分数等），衡量预测答案 $ \\hat{y}_i $ 与真实答案 $ y_i $ 的匹配程度。 $ l(x_i; \\theta) $：对查询 $ x_i $ 使用参数 $ \\theta $ 时的实际上下文长度（即所有输入 tokens 的总和）。 $ L_{\\text{max}} $：预算约束，即允许的最大上下文长度。 在所有满足约束的 $ \\theta $ 中，选择使平均性能 $ \\frac{1}{|\\mathcal{X}|} \\sum_i P(\\cdot) $ 最大的参数组合。\n实验：\n评估Genmini 1.5 Flash（上下文窗口最高1M）在知识密集型问答数据集（Bamboogle、HotpotQA、MuSiQue 和 2WikiMultiHopQA）上的性能。\n评估指标为exact match（EM）、F1、Acc\n$L_{max} \\in \\{16k, 32k, 128k, 1M, 5M \\} $tokens\n对于DRAG，检索文档数量 $k \\in \\{0,1,2,5,10,20,50,100,200,500,1000\\}$，示例数量 $m \\in \\{0,2^0,2^1,...,2^8\\}$\n对于IterRAG，$n \\in \\{1,2,3,4,5\\}$\n模型：\nzero-shot QA（ZS QA）纯使用LLM自身知识 many-shots QA（MS QA）只加入m个示例 RAG 只使用k个文档 DRAG IterRAG 4.2总体性能 结论：在任意上下文长度、任意数据集中，都是DRAG、IterRAG效果较好；达不到指定的上下文长度时会被省略。而且最好的P值随上下文长度线性变化。\nDRAG、IterdDRAG的性能随上下文长度的扩展而不断提高，而其余方法很快达到峰值。DRAG 在较短的最大长度下表现出色，而 IterDRAG 在更长的有效上下文长度下更有效。\n4.3RAG推理缩放定律 随着有效上下文长度的扩大，最佳性能表现出线性增长。所以可以通过增加计算来提高 RAG 性能，从而在给定可用计算资源的情况下更准确地预测性能。 对于 $L_{max}$ 高于 $10^5$​，IterDRAG 继续通过交错检索和迭代生成进行有效扩展。说明其更适合长上下文推理。 上下文超过1M后，性能提升不明显。 4.4对于特定参数的缩放 文档和示例并非起到同样作用。对于固定配置，增加检索到的文档数量通常会带来更好的性能提升。但k和m也是有阈值的。 增加示例对IterRAG更有帮助，如示例从0-1时，IterRAG的性能明显提升；而示例对DRAG不明显。 5.长上下文 RAG 的推理计算分配 构建计算分配模型，目的是为了能根据 $L_{max}$ 求 $\\theta$。\n性能指标（𝑃）\n表示在数据集 $ X $ 上的表现（如准确率），建模为参数 $ \\theta $ 的函数。 资源参数（𝜃）\n定义为三维向量 $ \\theta := (k, m, n)^T $，包含： ( k )：使用的文档数量 ( m )：上下文示例（demonstrations/shots）的数量 ( n )：最大迭代/生成步数 信息量参数（𝑖）\n衡量输入内容的信息价值，定义为 $ i := (i_{\\text{doc}}, i_{\\text{shot}}, 0)^T $： $ i_{\\text{doc}} $：文档的信息量 通过“添加1篇文档 vs 零样本（zero-shot）的性能差异”计算。 $ i_{\\text{shot}} $：示例的信息量 通过“添加1个示例 vs 零样本的性能差异”计算。 $ i_{\\text{iter}} $ 被忽略（设为0），因实验中发现增加生成步数对性能无显著提升。 性能模型公式\n$P(\\theta) \\approx \\sigma((a + b \\odot i)^T \\log(\\theta) + c)$\n符号说明： $ \\odot $：逐元素相乘（Hadamard积）。 $ a, b \\in \\mathbb{R}^3 $：待估计参数，分别表示资源的基础效应和与信息量的交互效应。 $ c $：常数偏置项。 a、b、c都是在特定情况下拟合的。 $ \\log(\\theta) $：对资源向量逐元素取对数。 $ \\sigma $：Sigmoid函数。 $L_{max}$ 相当于 $\\theta$ 的限制条件；拟合出abc；目标是最大化 $P(\\theta)$ ，就可以得到最好的 $\\theta$。\n","wordCount":"866","inLanguage":"en","datePublished":"2025-05-07T23:04:00+08:00","dateModified":"2025-05-07T23:04:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/"},"publisher":{"@type":"Organization","name":"Chan's Blog","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/apple-touch-icon.png"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="Chan's Blog (Alt + H)"><img src=https://Rook1eChan.github.io/apple-touch-icon.png alt aria-label=logo height=35>Chan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Inference Scaling for Long-Context Retrieval Augmented Generation</h1><div class=post-meta><span title='2025-05-07 23:04:00 +0800 +0800'>May 7, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0%e7%9b%ae%e6%a0%87 aria-label=0.目标>0.目标</a></li><li><a href=#1%e8%b4%a1%e7%8c%ae aria-label=1.贡献>1.贡献</a></li><li><a href=#2%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c aria-label=2.相关工作>2.相关工作</a><ul><li><a href=#21%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87llms aria-label=2.1长上下文LLMs>2.1长上下文LLMs</a></li><li><a href=#22%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0 aria-label=2.2上下文学习>2.2上下文学习</a></li><li><a href=#23rag aria-label=2.3RAG>2.3RAG</a></li></ul></li><li><a href=#3%e5%9f%ba%e4%ba%8erag%e7%9a%84%e6%8e%a8%e7%90%86%e6%89%a9%e5%b1%95%e7%ad%96%e7%95%a5 aria-label=3.基于RAG的推理扩展策略>3.基于RAG的推理扩展策略</a><ul><li><a href=#31drag aria-label=3.1DRAG>3.1DRAG</a></li><li><a href=#32iterdrag aria-label=3.2IterDRAG>3.2IterDRAG</a></li></ul></li><li><a href=#4rag%e6%80%a7%e8%83%bd%e5%92%8c%e6%8e%a8%e7%90%86%e8%ae%a1%e7%ae%97%e8%a7%84%e6%a8%a1 aria-label=4.RAG性能和推理计算规模>4.RAG性能和推理计算规模</a><ul><li><a href=#41%e7%bb%99%e5%ae%9a%e9%a2%84%e7%ae%97%e6%9c%80%e5%a4%a7%e6%9c%89%e6%95%88%e4%b8%8a%e4%b8%8b%e6%96%87%e9%95%bf%e5%ba%a6%e4%b8%8b%e7%9a%84%e6%9c%80%e4%bd%b3%e6%80%a7%e8%83%bd aria-label=4.1给定预算（最大有效上下文长度）下的最佳性能>4.1给定预算（最大有效上下文长度）下的最佳性能</a></li><li><a href=#42%e6%80%bb%e4%bd%93%e6%80%a7%e8%83%bd aria-label=4.2总体性能>4.2总体性能</a></li><li><a href=#43rag%e6%8e%a8%e7%90%86%e7%bc%a9%e6%94%be%e5%ae%9a%e5%be%8b aria-label=4.3RAG推理缩放定律>4.3RAG推理缩放定律</a></li><li><a href=#44%e5%af%b9%e4%ba%8e%e7%89%b9%e5%ae%9a%e5%8f%82%e6%95%b0%e7%9a%84%e7%bc%a9%e6%94%be aria-label=4.4对于特定参数的缩放>4.4对于特定参数的缩放</a></li></ul></li><li><a href=#5%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87-rag-%e7%9a%84%e6%8e%a8%e7%90%86%e8%ae%a1%e7%ae%97%e5%88%86%e9%85%8d aria-label="5.长上下文 RAG 的推理计算分配">5.长上下文 RAG 的推理计算分配</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>ICLR2025，来自Google DeepMind团队的工作</p><p><a href=https://arxiv.org/abs/2410.04343v2>https://arxiv.org/abs/2410.04343v2</a></p><h2 id=0目标>0.目标<a hidden class=anchor aria-hidden=true href=#0目标>#</a></h2><p>先前对于RAG推理扩展的研究主要集中于提供更多的知识，但只增加知识的数量是不够的。当前的LLM在处理长上下文时仍存在挑战。比如，在超长序列中定位有效信息的能力有限、最佳性能往往是在没有充分利用上下文的情况下实现的、超过一定阈值（文档数量）的检索会使性能停滞甚至下降。</p><p>目标是找到上下文长度与最优配置之间的关系，能够预测最佳推理参数，最大限度提高RAG性能。其中DRAG的参数为检索到的文档数量和示例数量。IterDRAG的参数为生成次数。</p><h2 id=1贡献>1.贡献<a hidden class=anchor aria-hidden=true href=#1贡献>#</a></h2><ul><li>提出两种RAG方法：DRAG（基于演示的RAG，为LLM提供多个RAG示例）和IterDRAG（基于迭代演示的RAG，将输入查询分解为更简单的子查询，迭代检索）。并证明了这两种方法优于仅提供知识的RAG。</li><li>得到了RAG的推理缩放定律：在最佳配置下，RAG性能随有效上下文长度线性变化。</li><li>根据定律对RAG性能与不同推理参数建模，推导出计算分配模型，为长上下文RAG的优化提供了指导。</li></ul><h2 id=2相关工作>2.相关工作<a hidden class=anchor aria-hidden=true href=#2相关工作>#</a></h2><h3 id=21长上下文llms>2.1长上下文LLMs<a hidden class=anchor aria-hidden=true href=#21长上下文llms>#</a></h3><p>早期采用稀疏/低秩核来减少内存需求。</p><blockquote><p>I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. <em>arXiv preprint</em> <em>arXiv:2004.05150</em>, 2020.</p><p>K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. <em>arXiv preprint arXiv:2009.14794</em>,2020.</p><p>N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In <em>International Conference</em> <em>on Learning Representations</em>, 2019.</p><p>M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. <em>Advances in neural information processing</em> <em>systems</em>, 33:17283–17297, 2020</p></blockquote><p>此外递归和状态空间模型（SSMs）被提出，作为基于transformer模型的有效替代方案。</p><blockquote><p>M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended long short-term memory. <em>arXiv preprint arXiv:2405.04517</em>, 2024</p><p>A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. <em>arXiv preprint</em> <em>arXiv:2312.00752</em>, 2023.</p><p>B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, L. Derczynski, et al. RWKV: Reinventing rnns for the transformer era. In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 14048–14077, 2023a.</p></blockquote><p>对于因果LLMs，外推和插值方法已被证明在扩展上下文窗口长度方面非常有效。</p><blockquote><p>S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. <em>arXiv preprint arXiv:2306.15595</em>, 2023.</p><p>B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. <em>arXiv preprint arXiv:2309.00071</em>, 2023b.</p><p>O. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. <em>arXiv preprint arXiv:2108.12409</em>, 2021.</p><p>Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A length-extrapolatable transformer. In <em>Proceedings of the 61st Annual Meeting of the Association for</em> <em>Computational Linguistics (Volume 1: Long Papers)</em>, pages 14590–14604, 2023.</p></blockquote><p>最近在高效注意力机制方面的进展，使得LLMs能够训练和推理包含数百万个标记的输入序列。</p><blockquote><p>T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact attention with IO-awareness. <em>Advances in Neural Information Processing Systems</em>, 35:16344–16359, 2022</p><p>S. A. Jacobs, M. Tanaka, C. Zhang, M. Zhang, L. Song, S. Rajbhandari, and Y. He. DeepSpeed Ulysses:System optimizations for enabling training of extreme long sequence transformer models. <em>arXiv</em> <em>preprint arXiv:2309.14509</em>, 2023.</p><p>H. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite context. <em>arXiv preprint arXiv:2310.01889</em>, 2023.</p><p>J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. <em>arXiv preprint arXiv:2303.08774</em>, 2023.</p><p>M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. <em>arXiv preprint arXiv:2403.05530</em>, 2024.</p><p>G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. <em>arXiv preprint arXiv:2312.11805</em>, 2023.</p></blockquote><h3 id=22上下文学习>2.2上下文学习<a hidden class=anchor aria-hidden=true href=#22上下文学习>#</a></h3><p>上下文学习（ICL）提供了一种计算效率高的方法，通过依赖少量任务示例来提高模型在推理时的表现。</p><p>为了进一步提升ICL性能，现有研究集中在预训练策略上，优化语言模型以实现上下文学习。</p><p>此外，选择性使用少量示例也被证明有助于提高下游任务的表现。</p><p>值得注意的是，重新格式化或找到最佳顺序的场景示例也能提高ICL的有效性。</p><p>随着长情境语言模型的出现，在ICL中扩展示例数量成为可能。</p><p>例如，Agarwal等人表明，多样本ICL可以减轻LLM中的预训练偏差，从而提高各种任务的ICL性能。</p><h3 id=23rag>2.3RAG<a hidden class=anchor aria-hidden=true href=#23rag>#</a></h3><p>RAG通过引入外部知识来提高语言模型的表现。</p><p>和naiveRAG相比，优化检索阶段可以有效提升上下文相关性，提高生成质量。</p><p>REPLUG使用语言模型作为监督来学习一个密集检索器模型。</p><p>此外，编码文档可以增加知识检索并提高生成能力。</p><p>Izacard和Grave（2021）利用融合解码器架构来编码多个问题-段落对，同时保持模型效率。</p><p>或者，有选择地利用文档中的知识可以提高语言模型对无关上下文的鲁棒性。例如，RAFT提出使用负文档训练语言模型以提高生成质量和相关性（Zhang等人，2024）。与我们的工作同时，提出了长文档检索和数据集扩展以优化RAG性能（Jiang等人，2024；Shao等人，2024）。</p><h2 id=3基于rag的推理扩展策略>3.基于RAG的推理扩展策略<a hidden class=anchor aria-hidden=true href=#3基于rag的推理扩展策略>#</a></h2><p>我们使用有效上下文长度来测量推理计算，有效上下文长度定义为 LLM 输出最终答案之前，所有迭代的输入token总数。每次的输入受到LLM上下文窗口的限制。忽略输出的token和检索的开销。</p><h3 id=31drag>3.1DRAG<a hidden class=anchor aria-hidden=true href=#31drag>#</a></h3><p>DRAG基于NaiveRAG构建，将检索到的topk文档和上下文示例集成到输入中。颠倒文档顺序，<u>将排名较高的文档放在离查询近的位置</u>。</p><h3 id=32iterdrag>3.2IterDRAG<a hidden class=anchor aria-hidden=true href=#32iterdrag>#</a></h3><p>将查询分解为更简单的子查询，每个子查询执行检索并生成中间答案。解决完所有子查询后，将所有上下文、子查询、中间答案组合在一起，生成最终答案。</p><blockquote><p>由于现有的数据集不带有子查询和中间答案，所以让大模型使用约束解码（constrained decoding）并遵循self-ask格式生成example。</p><p>生成example的步骤：</p><p>在每一轮中，生成子查询，然后将查询到的文档<u>交错</u>放入prompt，再生成中间答案。</p><p>最终答案生成。或者达到最大轮数后，强制生成最终答案。</p><p>文档、子查询-中间答案、最终答案一起构成example。</p></blockquote><p>推理过程中，上下文示例添加到初始文档之前。</p><p>IterRAG还将学习：1.将问题分解为简单可控的子问题 2.提取子问题的相关信息</p><p>这一方法有助于提高RAG回答复杂问题的能力。</p><p>具体操作见附录H。</p><h2 id=4rag性能和推理计算规模>4.RAG性能和推理计算规模<a hidden class=anchor aria-hidden=true href=#4rag性能和推理计算规模>#</a></h2><h3 id=41给定预算最大有效上下文长度下的最佳性能>4.1给定预算（最大有效上下文长度）下的最佳性能<a hidden class=anchor aria-hidden=true href=#41给定预算最大有效上下文长度下的最佳性能>#</a></h3><p>限制最大输入token即$L_{max}$时，可以通过调整推理参数 $\theta$ 来优化计算资源的使用。</p><ul><li>在 <strong>DRAG</strong> 中，可以调整 <strong>检索文档数量 ( k )</strong> 和 <strong>上下文示例数量 ( m )</strong>；</li><li>在 <strong>IterDRAG</strong> 中，额外引入了 <strong>检索与生成的迭代次数 ( n )</strong>。</li></ul><p>对于每个输入查询及其真实答案 $ (x_i, y_i) \in \mathcal{X} $，我们可以应用参数为 $ \theta $ 的 RAG 推理策略 $ f $，得到预测结果 $ \hat{y}_i = f(x_i; \theta) $，并计算评估指标 $ P(y_i, \hat{y}_i) $。</p><p>为了研究 RAG 性能与推理计算量之间的关系，我们在不同的计算预算 $ L_{\text{max}} $ 下采样，并通过枚举不同的 $ \theta \in \Theta $ 来寻找该预算下的最优平均性能 $ P^*(L_{\text{max}}) $：</p><p>$P^*(L_{\text{max}}) := \max_{\theta \in \Theta} \left\{ \frac{1}{|\mathcal{X}|} \sum_i P(y_i, f(x_i; \theta)) \ \Bigg| \ \forall i, l(x_i; \theta) \leq L_{\text{max}} \right\}.
\quad$</p><ul><li>$ \mathcal{X} $：测试集，包含输入查询 $ x_i $ 和真实答案 $ y_i $ 的配对 $ (x_i, y_i) $。</li><li>$ \theta $：RAG 推理参数，包括：<ul><li>$ k $：检索的文档数量，</li><li>$ m $：上下文示例（in-context examples）数量，</li><li>$ n $：生成迭代次数（DRAG 中 $ n=1 $，IterDRAG 中 $ n \geq 1 $）。</li></ul></li><li>$ f(x_i; \theta) $：使用参数 $ \theta$ 的 RAG 策略对查询 $ x_i $ 的预测结果 $ \hat{y}_i $。</li><li>$ P(y_i, \hat{y}_i) $：评估指标（如准确率、F1 分数等），衡量预测答案 $ \hat{y}_i $ 与真实答案 $ y_i $ 的匹配程度。</li><li>$ l(x_i; \theta) $：对查询 $ x_i $ 使用参数 $ \theta $ 时的实际上下文长度（即所有输入 tokens 的总和）。</li><li>$ L_{\text{max}} $：预算约束，即允许的最大上下文长度。</li></ul><p>在所有满足约束的 $ \theta $ 中，选择使平均性能 $ \frac{1}{|\mathcal{X}|} \sum_i P(\cdot) $ 最大的参数组合。</p><p>实验：</p><p>评估Genmini 1.5 Flash（上下文窗口最高1M）在知识密集型问答数据集（Bamboogle、HotpotQA、MuSiQue 和 2WikiMultiHopQA）上的性能。</p><p>评估指标为exact match（EM）、F1、Acc</p><p>$L_{max} \in \{16k, 32k, 128k, 1M, 5M \} $tokens</p><p>对于DRAG，检索文档数量 $k \in \{0,1,2,5,10,20,50,100,200,500,1000\}$，示例数量 $m \in \{0,2^0,2^1,...,2^8\}$</p><p>对于IterRAG，$n \in \{1,2,3,4,5\}$</p><p>模型：</p><ol><li>zero-shot QA（ZS QA）纯使用LLM自身知识</li><li>many-shots QA（MS QA）只加入m个示例</li><li>RAG 只使用k个文档</li><li>DRAG</li><li>IterRAG</li></ol><h3 id=42总体性能>4.2总体性能<a hidden class=anchor aria-hidden=true href=#42总体性能>#</a></h3><p>结论：在任意上下文长度、任意数据集中，都是DRAG、IterRAG效果较好；达不到指定的上下文长度时会被省略。而且最好的P值随上下文长度线性变化。</p><p>DRAG、IterdDRAG的性能随上下文长度的扩展而不断提高，而其余方法很快达到峰值。DRAG 在较短的最大长度下表现出色，而 IterDRAG 在更长的有效上下文长度下更有效。</p><h3 id=43rag推理缩放定律>4.3RAG推理缩放定律<a hidden class=anchor aria-hidden=true href=#43rag推理缩放定律>#</a></h3><ol><li>随着有效上下文长度的扩大，最佳性能表现出线性增长。所以可以通过增加计算来提高 RAG 性能，从而在给定可用计算资源的情况下更准确地预测性能。</li><li>对于 $L_{max}$ 高于 $10^5$​，IterDRAG 继续通过交错检索和迭代生成进行有效扩展。说明其更适合长上下文推理。</li><li>上下文超过1M后，性能提升不明显。</li></ol><h3 id=44对于特定参数的缩放>4.4对于特定参数的缩放<a hidden class=anchor aria-hidden=true href=#44对于特定参数的缩放>#</a></h3><ol><li>文档和示例并非起到同样作用。对于固定配置，增加检索到的文档数量通常会带来更好的性能提升。但k和m也是有阈值的。</li><li>增加示例对IterRAG更有帮助，如示例从0-1时，IterRAG的性能明显提升；而示例对DRAG不明显。</li></ol><h2 id=5长上下文-rag-的推理计算分配>5.长上下文 RAG 的推理计算分配<a hidden class=anchor aria-hidden=true href=#5长上下文-rag-的推理计算分配>#</a></h2><p>构建计算分配模型，目的是为了能根据 $L_{max}$ 求 $\theta$。</p><ol><li><p><strong>性能指标（𝑃）</strong></p><ul><li>表示在数据集 $ X $ 上的表现（如准确率），建模为参数 $ \theta $ 的函数。</li></ul></li><li><p><strong>资源参数（𝜃）</strong></p><ul><li>定义为三维向量 $ \theta := (k, m, n)^T $，包含：<ul><li>( k )：使用的文档数量</li><li>( m )：上下文示例（demonstrations/shots）的数量</li><li>( n )：最大迭代/生成步数</li></ul></li></ul></li><li><p><strong>信息量参数（𝑖）</strong></p><ul><li>衡量输入内容的信息价值，定义为 $ i := (i_{\text{doc}}, i_{\text{shot}}, 0)^T $：<ul><li>$ i_{\text{doc}} $：<strong>文档的信息量</strong><ul><li>通过“添加1篇文档 vs 零样本（zero-shot）的性能差异”计算。</li></ul></li><li>$ i_{\text{shot}} $：<strong>示例的信息量</strong><ul><li>通过“添加1个示例 vs 零样本的性能差异”计算。</li></ul></li><li>$ i_{\text{iter}} $ 被忽略（设为0），因实验中发现增加生成步数对性能无显著提升。</li></ul></li></ul></li><li><p><strong>性能模型公式</strong></p><p>$P(\theta) \approx \sigma((a + b \odot i)^T \log(\theta) + c)$</p><ul><li><strong>符号说明</strong>：<ul><li>$ \odot $：逐元素相乘（Hadamard积）。</li><li>$ a, b \in \mathbb{R}^3 $：待估计参数，分别表示资源的基础效应和与信息量的交互效应。</li><li>$ c $：常数偏置项。 a、b、c都是在特定情况下拟合的。</li><li>$ \log(\theta) $：对资源向量逐元素取对数。</li><li>$ \sigma $：Sigmoid函数。</li></ul></li></ul></li></ol><p>$L_{max}$ 相当于 $\theta$ 的限制条件；拟合出abc；目标是最大化 $P(\theta)$ ，就可以得到最好的 $\theta$。</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>Chan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>