<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>Inference Scaling for Long-Context Retrieval Augmented Generation | Chan's Blog</title>
<meta name=keywords content><meta name=description content="ICLR2025ï¼Œæ¥è‡ªGoogle DeepMindå›¢é˜Ÿçš„å·¥ä½œ
https://arxiv.org/abs/2410.04343v2
0.ç›®æ ‡
å…ˆå‰å¯¹äºRAGæ¨ç†æ‰©å±•çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæä¾›æ›´å¤šçš„çŸ¥è¯†ï¼Œä½†åªå¢åŠ çŸ¥è¯†çš„æ•°é‡æ˜¯ä¸å¤Ÿçš„ã€‚å½“å‰çš„LLMåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ¯”å¦‚ï¼Œåœ¨è¶…é•¿åºåˆ—ä¸­å®šä½æœ‰æ•ˆä¿¡æ¯çš„èƒ½åŠ›æœ‰é™ã€æœ€ä½³æ€§èƒ½å¾€å¾€æ˜¯åœ¨æ²¡æœ‰å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å®ç°çš„ã€è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ˆæ–‡æ¡£æ•°é‡ï¼‰çš„æ£€ç´¢ä¼šä½¿æ€§èƒ½åœæ»ç”šè‡³ä¸‹é™ã€‚
ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æœ€ä¼˜é…ç½®ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿé¢„æµ‹æœ€ä½³æ¨ç†å‚æ•°ï¼Œæœ€å¤§é™åº¦æé«˜RAGæ€§èƒ½ã€‚å…¶ä¸­DRAGçš„å‚æ•°ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡å’Œç¤ºä¾‹æ•°é‡ã€‚IterDRAGçš„å‚æ•°ä¸ºç”Ÿæˆæ¬¡æ•°ã€‚
1.è´¡çŒ®

æå‡ºä¸¤ç§RAGæ–¹æ³•ï¼šDRAGï¼ˆåŸºäºæ¼”ç¤ºçš„RAGï¼Œä¸ºLLMæä¾›å¤šä¸ªRAGç¤ºä¾‹ï¼‰å’ŒIterDRAGï¼ˆåŸºäºè¿­ä»£æ¼”ç¤ºçš„RAGï¼Œå°†è¾“å…¥æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œè¿­ä»£æ£€ç´¢ï¼‰ã€‚å¹¶è¯æ˜äº†è¿™ä¸¤ç§æ–¹æ³•ä¼˜äºä»…æä¾›çŸ¥è¯†çš„RAGã€‚
å¾—åˆ°äº†RAGçš„æ¨ç†ç¼©æ”¾å®šå¾‹ï¼šåœ¨æœ€ä½³é…ç½®ä¸‹ï¼ŒRAGæ€§èƒ½éšæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚
æ ¹æ®å®šå¾‹å¯¹RAGæ€§èƒ½ä¸ä¸åŒæ¨ç†å‚æ•°å»ºæ¨¡ï¼Œæ¨å¯¼å‡ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡RAGçš„ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚

2.ç›¸å…³å·¥ä½œ
2.1é•¿ä¸Šä¸‹æ–‡LLMs
æ—©æœŸé‡‡ç”¨ç¨€ç–/ä½ç§©æ ¸æ¥å‡å°‘å†…å­˜éœ€æ±‚ã€‚

I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019."><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/"><meta property="og:site_name" content="Chan's Blog"><meta property="og:title" content="Inference Scaling for Long-Context Retrieval Augmented Generation"><meta property="og:description" content="ICLR2025ï¼Œæ¥è‡ªGoogle DeepMindå›¢é˜Ÿçš„å·¥ä½œ
https://arxiv.org/abs/2410.04343v2
0.ç›®æ ‡ å…ˆå‰å¯¹äºRAGæ¨ç†æ‰©å±•çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæä¾›æ›´å¤šçš„çŸ¥è¯†ï¼Œä½†åªå¢åŠ çŸ¥è¯†çš„æ•°é‡æ˜¯ä¸å¤Ÿçš„ã€‚å½“å‰çš„LLMåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ¯”å¦‚ï¼Œåœ¨è¶…é•¿åºåˆ—ä¸­å®šä½æœ‰æ•ˆä¿¡æ¯çš„èƒ½åŠ›æœ‰é™ã€æœ€ä½³æ€§èƒ½å¾€å¾€æ˜¯åœ¨æ²¡æœ‰å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å®ç°çš„ã€è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ˆæ–‡æ¡£æ•°é‡ï¼‰çš„æ£€ç´¢ä¼šä½¿æ€§èƒ½åœæ»ç”šè‡³ä¸‹é™ã€‚
ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æœ€ä¼˜é…ç½®ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿé¢„æµ‹æœ€ä½³æ¨ç†å‚æ•°ï¼Œæœ€å¤§é™åº¦æé«˜RAGæ€§èƒ½ã€‚å…¶ä¸­DRAGçš„å‚æ•°ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡å’Œç¤ºä¾‹æ•°é‡ã€‚IterDRAGçš„å‚æ•°ä¸ºç”Ÿæˆæ¬¡æ•°ã€‚
1.è´¡çŒ® æå‡ºä¸¤ç§RAGæ–¹æ³•ï¼šDRAGï¼ˆåŸºäºæ¼”ç¤ºçš„RAGï¼Œä¸ºLLMæä¾›å¤šä¸ªRAGç¤ºä¾‹ï¼‰å’ŒIterDRAGï¼ˆåŸºäºè¿­ä»£æ¼”ç¤ºçš„RAGï¼Œå°†è¾“å…¥æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œè¿­ä»£æ£€ç´¢ï¼‰ã€‚å¹¶è¯æ˜äº†è¿™ä¸¤ç§æ–¹æ³•ä¼˜äºä»…æä¾›çŸ¥è¯†çš„RAGã€‚ å¾—åˆ°äº†RAGçš„æ¨ç†ç¼©æ”¾å®šå¾‹ï¼šåœ¨æœ€ä½³é…ç½®ä¸‹ï¼ŒRAGæ€§èƒ½éšæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚ æ ¹æ®å®šå¾‹å¯¹RAGæ€§èƒ½ä¸ä¸åŒæ¨ç†å‚æ•°å»ºæ¨¡ï¼Œæ¨å¯¼å‡ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡RAGçš„ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚ 2.ç›¸å…³å·¥ä½œ 2.1é•¿ä¸Šä¸‹æ–‡LLMs æ—©æœŸé‡‡ç”¨ç¨€ç–/ä½ç§©æ ¸æ¥å‡å°‘å†…å­˜éœ€æ±‚ã€‚
I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019."><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-07T23:04:00+08:00"><meta property="article:modified_time" content="2025-05-07T23:04:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Inference Scaling for Long-Context Retrieval Augmented Generation"><meta name=twitter:description content="ICLR2025ï¼Œæ¥è‡ªGoogle DeepMindå›¢é˜Ÿçš„å·¥ä½œ
https://arxiv.org/abs/2410.04343v2
0.ç›®æ ‡
å…ˆå‰å¯¹äºRAGæ¨ç†æ‰©å±•çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæä¾›æ›´å¤šçš„çŸ¥è¯†ï¼Œä½†åªå¢åŠ çŸ¥è¯†çš„æ•°é‡æ˜¯ä¸å¤Ÿçš„ã€‚å½“å‰çš„LLMåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ¯”å¦‚ï¼Œåœ¨è¶…é•¿åºåˆ—ä¸­å®šä½æœ‰æ•ˆä¿¡æ¯çš„èƒ½åŠ›æœ‰é™ã€æœ€ä½³æ€§èƒ½å¾€å¾€æ˜¯åœ¨æ²¡æœ‰å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å®ç°çš„ã€è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ˆæ–‡æ¡£æ•°é‡ï¼‰çš„æ£€ç´¢ä¼šä½¿æ€§èƒ½åœæ»ç”šè‡³ä¸‹é™ã€‚
ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æœ€ä¼˜é…ç½®ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿé¢„æµ‹æœ€ä½³æ¨ç†å‚æ•°ï¼Œæœ€å¤§é™åº¦æé«˜RAGæ€§èƒ½ã€‚å…¶ä¸­DRAGçš„å‚æ•°ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡å’Œç¤ºä¾‹æ•°é‡ã€‚IterDRAGçš„å‚æ•°ä¸ºç”Ÿæˆæ¬¡æ•°ã€‚
1.è´¡çŒ®

æå‡ºä¸¤ç§RAGæ–¹æ³•ï¼šDRAGï¼ˆåŸºäºæ¼”ç¤ºçš„RAGï¼Œä¸ºLLMæä¾›å¤šä¸ªRAGç¤ºä¾‹ï¼‰å’ŒIterDRAGï¼ˆåŸºäºè¿­ä»£æ¼”ç¤ºçš„RAGï¼Œå°†è¾“å…¥æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œè¿­ä»£æ£€ç´¢ï¼‰ã€‚å¹¶è¯æ˜äº†è¿™ä¸¤ç§æ–¹æ³•ä¼˜äºä»…æä¾›çŸ¥è¯†çš„RAGã€‚
å¾—åˆ°äº†RAGçš„æ¨ç†ç¼©æ”¾å®šå¾‹ï¼šåœ¨æœ€ä½³é…ç½®ä¸‹ï¼ŒRAGæ€§èƒ½éšæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚
æ ¹æ®å®šå¾‹å¯¹RAGæ€§èƒ½ä¸ä¸åŒæ¨ç†å‚æ•°å»ºæ¨¡ï¼Œæ¨å¯¼å‡ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡RAGçš„ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚

2.ç›¸å…³å·¥ä½œ
2.1é•¿ä¸Šä¸‹æ–‡LLMs
æ—©æœŸé‡‡ç”¨ç¨€ç–/ä½ç§©æ ¸æ¥å‡å°‘å†…å­˜éœ€æ±‚ã€‚

I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.
N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Inference Scaling for Long-Context Retrieval Augmented Generation","item":"https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Inference Scaling for Long-Context Retrieval Augmented Generation","name":"Inference Scaling for Long-Context Retrieval Augmented Generation","description":"ICLR2025ï¼Œæ¥è‡ªGoogle DeepMindå›¢é˜Ÿçš„å·¥ä½œ\nhttps://arxiv.org/abs/2410.04343v2\n0.ç›®æ ‡ å…ˆå‰å¯¹äºRAGæ¨ç†æ‰©å±•çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæä¾›æ›´å¤šçš„çŸ¥è¯†ï¼Œä½†åªå¢åŠ çŸ¥è¯†çš„æ•°é‡æ˜¯ä¸å¤Ÿçš„ã€‚å½“å‰çš„LLMåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ¯”å¦‚ï¼Œåœ¨è¶…é•¿åºåˆ—ä¸­å®šä½æœ‰æ•ˆä¿¡æ¯çš„èƒ½åŠ›æœ‰é™ã€æœ€ä½³æ€§èƒ½å¾€å¾€æ˜¯åœ¨æ²¡æœ‰å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å®ç°çš„ã€è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ˆæ–‡æ¡£æ•°é‡ï¼‰çš„æ£€ç´¢ä¼šä½¿æ€§èƒ½åœæ»ç”šè‡³ä¸‹é™ã€‚\nç›®æ ‡æ˜¯æ‰¾åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æœ€ä¼˜é…ç½®ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿé¢„æµ‹æœ€ä½³æ¨ç†å‚æ•°ï¼Œæœ€å¤§é™åº¦æé«˜RAGæ€§èƒ½ã€‚å…¶ä¸­DRAGçš„å‚æ•°ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡å’Œç¤ºä¾‹æ•°é‡ã€‚IterDRAGçš„å‚æ•°ä¸ºç”Ÿæˆæ¬¡æ•°ã€‚\n1.è´¡çŒ® æå‡ºä¸¤ç§RAGæ–¹æ³•ï¼šDRAGï¼ˆåŸºäºæ¼”ç¤ºçš„RAGï¼Œä¸ºLLMæä¾›å¤šä¸ªRAGç¤ºä¾‹ï¼‰å’ŒIterDRAGï¼ˆåŸºäºè¿­ä»£æ¼”ç¤ºçš„RAGï¼Œå°†è¾“å…¥æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œè¿­ä»£æ£€ç´¢ï¼‰ã€‚å¹¶è¯æ˜äº†è¿™ä¸¤ç§æ–¹æ³•ä¼˜äºä»…æä¾›çŸ¥è¯†çš„RAGã€‚ å¾—åˆ°äº†RAGçš„æ¨ç†ç¼©æ”¾å®šå¾‹ï¼šåœ¨æœ€ä½³é…ç½®ä¸‹ï¼ŒRAGæ€§èƒ½éšæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚ æ ¹æ®å®šå¾‹å¯¹RAGæ€§èƒ½ä¸ä¸åŒæ¨ç†å‚æ•°å»ºæ¨¡ï¼Œæ¨å¯¼å‡ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡RAGçš„ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚ 2.ç›¸å…³å·¥ä½œ 2.1é•¿ä¸Šä¸‹æ–‡LLMs æ—©æœŸé‡‡ç”¨ç¨€ç–/ä½ç§©æ ¸æ¥å‡å°‘å†…å­˜éœ€æ±‚ã€‚\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\nK. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.\nN. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.\n","keywords":[],"articleBody":"ICLR2025ï¼Œæ¥è‡ªGoogle DeepMindå›¢é˜Ÿçš„å·¥ä½œ\nhttps://arxiv.org/abs/2410.04343v2\n0.ç›®æ ‡ å…ˆå‰å¯¹äºRAGæ¨ç†æ‰©å±•çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæä¾›æ›´å¤šçš„çŸ¥è¯†ï¼Œä½†åªå¢åŠ çŸ¥è¯†çš„æ•°é‡æ˜¯ä¸å¤Ÿçš„ã€‚å½“å‰çš„LLMåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ¯”å¦‚ï¼Œåœ¨è¶…é•¿åºåˆ—ä¸­å®šä½æœ‰æ•ˆä¿¡æ¯çš„èƒ½åŠ›æœ‰é™ã€æœ€ä½³æ€§èƒ½å¾€å¾€æ˜¯åœ¨æ²¡æœ‰å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å®ç°çš„ã€è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ˆæ–‡æ¡£æ•°é‡ï¼‰çš„æ£€ç´¢ä¼šä½¿æ€§èƒ½åœæ»ç”šè‡³ä¸‹é™ã€‚\nç›®æ ‡æ˜¯æ‰¾åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æœ€ä¼˜é…ç½®ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿé¢„æµ‹æœ€ä½³æ¨ç†å‚æ•°ï¼Œæœ€å¤§é™åº¦æé«˜RAGæ€§èƒ½ã€‚å…¶ä¸­DRAGçš„å‚æ•°ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡å’Œç¤ºä¾‹æ•°é‡ã€‚IterDRAGçš„å‚æ•°ä¸ºç”Ÿæˆæ¬¡æ•°ã€‚\n1.è´¡çŒ® æå‡ºä¸¤ç§RAGæ–¹æ³•ï¼šDRAGï¼ˆåŸºäºæ¼”ç¤ºçš„RAGï¼Œä¸ºLLMæä¾›å¤šä¸ªRAGç¤ºä¾‹ï¼‰å’ŒIterDRAGï¼ˆåŸºäºè¿­ä»£æ¼”ç¤ºçš„RAGï¼Œå°†è¾“å…¥æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œè¿­ä»£æ£€ç´¢ï¼‰ã€‚å¹¶è¯æ˜äº†è¿™ä¸¤ç§æ–¹æ³•ä¼˜äºä»…æä¾›çŸ¥è¯†çš„RAGã€‚ å¾—åˆ°äº†RAGçš„æ¨ç†ç¼©æ”¾å®šå¾‹ï¼šåœ¨æœ€ä½³é…ç½®ä¸‹ï¼ŒRAGæ€§èƒ½éšæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚ æ ¹æ®å®šå¾‹å¯¹RAGæ€§èƒ½ä¸ä¸åŒæ¨ç†å‚æ•°å»ºæ¨¡ï¼Œæ¨å¯¼å‡ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡RAGçš„ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚ 2.ç›¸å…³å·¥ä½œ 2.1é•¿ä¸Šä¸‹æ–‡LLMs æ—©æœŸé‡‡ç”¨ç¨€ç–/ä½ç§©æ ¸æ¥å‡å°‘å†…å­˜éœ€æ±‚ã€‚\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\nK. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,2020.\nN. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.\nM. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283â€“17297, 2020\næ­¤å¤–é€’å½’å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰è¢«æå‡ºï¼Œä½œä¸ºåŸºäºtransformeræ¨¡å‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚\nM. Beck, K. PÃ¶ppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024\nA. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, L. Derczynski, et al. RWKV: Reinventing rnns for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14048â€“14077, 2023a.\nå¯¹äºå› æœLLMsï¼Œå¤–æ¨å’Œæ’å€¼æ–¹æ³•å·²è¢«è¯æ˜åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æ–¹é¢éå¸¸æœ‰æ•ˆã€‚\nS. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023b.\nO. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\nY. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590â€“14604, 2023.\næœ€è¿‘åœ¨é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶æ–¹é¢çš„è¿›å±•ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿè®­ç»ƒå’Œæ¨ç†åŒ…å«æ•°ç™¾ä¸‡ä¸ªæ ‡è®°çš„è¾“å…¥åºåˆ—ã€‚\nT. Dao, D. Fu, S. Ermon, A. Rudra, and C. RÃ©. Flashattention: Fast and memory-efficient exact attention with IO-awareness. Advances in Neural Information Processing Systems, 35:16344â€“16359, 2022\nS. A. Jacobs, M. Tanaka, C. Zhang, M. Zhang, L. Song, S. Rajbhandari, and Y. He. DeepSpeed Ulysses:System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023.\nH. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nM. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n2.2ä¸Šä¸‹æ–‡å­¦ä¹  ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„æ–¹æ³•ï¼Œé€šè¿‡ä¾èµ–å°‘é‡ä»»åŠ¡ç¤ºä¾‹æ¥æé«˜æ¨¡å‹åœ¨æ¨ç†æ—¶çš„è¡¨ç°ã€‚\nä¸ºäº†è¿›ä¸€æ­¥æå‡ICLæ€§èƒ½ï¼Œç°æœ‰ç ”ç©¶é›†ä¸­åœ¨é¢„è®­ç»ƒç­–ç•¥ä¸Šï¼Œä¼˜åŒ–è¯­è¨€æ¨¡å‹ä»¥å®ç°ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚\næ­¤å¤–ï¼Œé€‰æ‹©æ€§ä½¿ç”¨å°‘é‡ç¤ºä¾‹ä¹Ÿè¢«è¯æ˜æœ‰åŠ©äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé‡æ–°æ ¼å¼åŒ–æˆ–æ‰¾åˆ°æœ€ä½³é¡ºåºçš„åœºæ™¯ç¤ºä¾‹ä¹Ÿèƒ½æé«˜ICLçš„æœ‰æ•ˆæ€§ã€‚\néšç€é•¿æƒ…å¢ƒè¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œåœ¨ICLä¸­æ‰©å±•ç¤ºä¾‹æ•°é‡æˆä¸ºå¯èƒ½ã€‚\nä¾‹å¦‚ï¼ŒAgarwalç­‰äººè¡¨æ˜ï¼Œå¤šæ ·æœ¬ICLå¯ä»¥å‡è½»LLMä¸­çš„é¢„è®­ç»ƒåå·®ï¼Œä»è€Œæé«˜å„ç§ä»»åŠ¡çš„ICLæ€§èƒ½ã€‚\n2.3RAG RAGé€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æ¥æé«˜è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚\nå’ŒnaiveRAGç›¸æ¯”ï¼Œä¼˜åŒ–æ£€ç´¢é˜¶æ®µå¯ä»¥æœ‰æ•ˆæå‡ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚\nREPLUGä½¿ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºç›‘ç£æ¥å­¦ä¹ ä¸€ä¸ªå¯†é›†æ£€ç´¢å™¨æ¨¡å‹ã€‚\næ­¤å¤–ï¼Œç¼–ç æ–‡æ¡£å¯ä»¥å¢åŠ çŸ¥è¯†æ£€ç´¢å¹¶æé«˜ç”Ÿæˆèƒ½åŠ›ã€‚\nIzacardå’ŒGraveï¼ˆ2021ï¼‰åˆ©ç”¨èåˆè§£ç å™¨æ¶æ„æ¥ç¼–ç å¤šä¸ªé—®é¢˜-æ®µè½å¯¹ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ•ˆç‡ã€‚\næˆ–è€…ï¼Œæœ‰é€‰æ‹©åœ°åˆ©ç”¨æ–‡æ¡£ä¸­çš„çŸ¥è¯†å¯ä»¥æé«˜è¯­è¨€æ¨¡å‹å¯¹æ— å…³ä¸Šä¸‹æ–‡çš„é²æ£’æ€§ã€‚ä¾‹å¦‚ï¼ŒRAFTæå‡ºä½¿ç”¨è´Ÿæ–‡æ¡£è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥æé«˜ç”Ÿæˆè´¨é‡å’Œç›¸å…³æ€§ï¼ˆZhangç­‰äººï¼Œ2024ï¼‰ã€‚ä¸æˆ‘ä»¬çš„å·¥ä½œåŒæ—¶ï¼Œæå‡ºäº†é•¿æ–‡æ¡£æ£€ç´¢å’Œæ•°æ®é›†æ‰©å±•ä»¥ä¼˜åŒ–RAGæ€§èƒ½ï¼ˆJiangç­‰äººï¼Œ2024ï¼›Shaoç­‰äººï¼Œ2024ï¼‰ã€‚\n3.åŸºäºRAGçš„æ¨ç†æ‰©å±•ç­–ç•¥ æˆ‘ä»¬ä½¿ç”¨æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦æ¥æµ‹é‡æ¨ç†è®¡ç®—ï¼Œæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦å®šä¹‰ä¸º LLM è¾“å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ï¼Œæ‰€æœ‰è¿­ä»£çš„è¾“å…¥tokenæ€»æ•°ã€‚æ¯æ¬¡çš„è¾“å…¥å—åˆ°LLMä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ã€‚å¿½ç•¥è¾“å‡ºçš„tokenå’Œæ£€ç´¢çš„å¼€é”€ã€‚\n3.1DRAG DRAGåŸºäºNaiveRAGæ„å»ºï¼Œå°†æ£€ç´¢åˆ°çš„topkæ–‡æ¡£å’Œä¸Šä¸‹æ–‡ç¤ºä¾‹é›†æˆåˆ°è¾“å…¥ä¸­ã€‚é¢ å€’æ–‡æ¡£é¡ºåºï¼Œå°†æ’åè¾ƒé«˜çš„æ–‡æ¡£æ”¾åœ¨ç¦»æŸ¥è¯¢è¿‘çš„ä½ç½®ã€‚\n3.2IterDRAG å°†æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œæ¯ä¸ªå­æŸ¥è¯¢æ‰§è¡Œæ£€ç´¢å¹¶ç”Ÿæˆä¸­é—´ç­”æ¡ˆã€‚è§£å†³å®Œæ‰€æœ‰å­æŸ¥è¯¢åï¼Œå°†æ‰€æœ‰ä¸Šä¸‹æ–‡ã€å­æŸ¥è¯¢ã€ä¸­é—´ç­”æ¡ˆç»„åˆåœ¨ä¸€èµ·ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚\nç”±äºç°æœ‰çš„æ•°æ®é›†ä¸å¸¦æœ‰å­æŸ¥è¯¢å’Œä¸­é—´ç­”æ¡ˆï¼Œæ‰€ä»¥è®©å¤§æ¨¡å‹ä½¿ç”¨çº¦æŸè§£ç ï¼ˆconstrained decodingï¼‰å¹¶éµå¾ªself-askæ ¼å¼ç”Ÿæˆexampleã€‚\nç”Ÿæˆexampleçš„æ­¥éª¤ï¼š\nåœ¨æ¯ä¸€è½®ä¸­ï¼Œç”Ÿæˆå­æŸ¥è¯¢ï¼Œç„¶åå°†æŸ¥è¯¢åˆ°çš„æ–‡æ¡£äº¤é”™æ”¾å…¥promptï¼Œå†ç”Ÿæˆä¸­é—´ç­”æ¡ˆã€‚\næœ€ç»ˆç­”æ¡ˆç”Ÿæˆã€‚æˆ–è€…è¾¾åˆ°æœ€å¤§è½®æ•°åï¼Œå¼ºåˆ¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚\næ–‡æ¡£ã€å­æŸ¥è¯¢-ä¸­é—´ç­”æ¡ˆã€æœ€ç»ˆç­”æ¡ˆä¸€èµ·æ„æˆexampleã€‚\næ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸Šä¸‹æ–‡ç¤ºä¾‹æ·»åŠ åˆ°åˆå§‹æ–‡æ¡£ä¹‹å‰ã€‚\nIterRAGè¿˜å°†å­¦ä¹ ï¼š1.å°†é—®é¢˜åˆ†è§£ä¸ºç®€å•å¯æ§çš„å­é—®é¢˜ 2.æå–å­é—®é¢˜çš„ç›¸å…³ä¿¡æ¯\nè¿™ä¸€æ–¹æ³•æœ‰åŠ©äºæé«˜RAGå›ç­”å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚\nå…·ä½“æ“ä½œè§é™„å½•Hã€‚\n4.RAGæ€§èƒ½å’Œæ¨ç†è®¡ç®—è§„æ¨¡ 4.1ç»™å®šé¢„ç®—ï¼ˆæœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ä¸‹çš„æœ€ä½³æ€§èƒ½ é™åˆ¶æœ€å¤§è¾“å…¥tokenå³$L_{max}$æ—¶ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´æ¨ç†å‚æ•° $\\theta$ æ¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ã€‚\nåœ¨ DRAG ä¸­ï¼Œå¯ä»¥è°ƒæ•´ æ£€ç´¢æ–‡æ¡£æ•°é‡ ( k ) å’Œ ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡ ( m )ï¼› åœ¨ IterDRAG ä¸­ï¼Œé¢å¤–å¼•å…¥äº† æ£€ç´¢ä¸ç”Ÿæˆçš„è¿­ä»£æ¬¡æ•° ( n )ã€‚ å¯¹äºæ¯ä¸ªè¾“å…¥æŸ¥è¯¢åŠå…¶çœŸå®ç­”æ¡ˆ $ (x_i, y_i) \\in \\mathcal{X} $ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨å‚æ•°ä¸º $ \\theta $ çš„ RAG æ¨ç†ç­–ç•¥ $ f $ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ $ \\hat{y}_i = f(x_i; \\theta) $ï¼Œå¹¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡ $ P(y_i, \\hat{y}_i) $ã€‚\nä¸ºäº†ç ”ç©¶ RAG æ€§èƒ½ä¸æ¨ç†è®¡ç®—é‡ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„è®¡ç®—é¢„ç®— $ L_{\\text{max}} $ ä¸‹é‡‡æ ·ï¼Œå¹¶é€šè¿‡æšä¸¾ä¸åŒçš„ $ \\theta \\in \\Theta $ æ¥å¯»æ‰¾è¯¥é¢„ç®—ä¸‹çš„æœ€ä¼˜å¹³å‡æ€§èƒ½ $ P^*(L_{\\text{max}}) $ï¼š\n$P^*(L_{\\text{max}}) := \\max_{\\theta \\in \\Theta} \\left\\{ \\frac{1}{|\\mathcal{X}|} \\sum_i P(y_i, f(x_i; \\theta)) \\ \\Bigg| \\ \\forall i, l(x_i; \\theta) \\leq L_{\\text{max}} \\right\\}. \\quad$\n$ \\mathcal{X} $ï¼šæµ‹è¯•é›†ï¼ŒåŒ…å«è¾“å…¥æŸ¥è¯¢ $ x_i $ å’ŒçœŸå®ç­”æ¡ˆ $ y_i $ çš„é…å¯¹ $ (x_i, y_i) $ã€‚ $ \\theta $ï¼šRAG æ¨ç†å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š $ k $ï¼šæ£€ç´¢çš„æ–‡æ¡£æ•°é‡ï¼Œ $ m $ï¼šä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼ˆin-context examplesï¼‰æ•°é‡ï¼Œ $ n $ï¼šç”Ÿæˆè¿­ä»£æ¬¡æ•°ï¼ˆDRAG ä¸­ $ n=1 $ï¼ŒIterDRAG ä¸­ $ n \\geq 1 $ï¼‰ã€‚ $ f(x_i; \\theta) $ï¼šä½¿ç”¨å‚æ•° $ \\theta$ çš„ RAG ç­–ç•¥å¯¹æŸ¥è¯¢ $ x_i $ çš„é¢„æµ‹ç»“æœ $ \\hat{y}_i $ã€‚ $ P(y_i, \\hat{y}_i) $ï¼šè¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1 åˆ†æ•°ç­‰ï¼‰ï¼Œè¡¡é‡é¢„æµ‹ç­”æ¡ˆ $ \\hat{y}_i $ ä¸çœŸå®ç­”æ¡ˆ $ y_i $ çš„åŒ¹é…ç¨‹åº¦ã€‚ $ l(x_i; \\theta) $ï¼šå¯¹æŸ¥è¯¢ $ x_i $ ä½¿ç”¨å‚æ•° $ \\theta $ æ—¶çš„å®é™…ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå³æ‰€æœ‰è¾“å…¥ tokens çš„æ€»å’Œï¼‰ã€‚ $ L_{\\text{max}} $ï¼šé¢„ç®—çº¦æŸï¼Œå³å…è®¸çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ åœ¨æ‰€æœ‰æ»¡è¶³çº¦æŸçš„ $ \\theta $ ä¸­ï¼Œé€‰æ‹©ä½¿å¹³å‡æ€§èƒ½ $ \\frac{1}{|\\mathcal{X}|} \\sum_i P(\\cdot) $ æœ€å¤§çš„å‚æ•°ç»„åˆã€‚\nå®éªŒï¼š\nè¯„ä¼°Genmini 1.5 Flashï¼ˆä¸Šä¸‹æ–‡çª—å£æœ€é«˜1Mï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”æ•°æ®é›†ï¼ˆBamboogleã€HotpotQAã€MuSiQue å’Œ 2WikiMultiHopQAï¼‰ä¸Šçš„æ€§èƒ½ã€‚\nè¯„ä¼°æŒ‡æ ‡ä¸ºexact matchï¼ˆEMï¼‰ã€F1ã€Acc\n$L_{max} \\in \\{16k, 32k, 128k, 1M, 5M \\} $tokens\nå¯¹äºDRAGï¼Œæ£€ç´¢æ–‡æ¡£æ•°é‡ $k \\in \\{0,1,2,5,10,20,50,100,200,500,1000\\}$ï¼Œç¤ºä¾‹æ•°é‡ $m \\in \\{0,2^0,2^1,...,2^8\\}$\nå¯¹äºIterRAGï¼Œ$n \\in \\{1,2,3,4,5\\}$\næ¨¡å‹ï¼š\nzero-shot QAï¼ˆZS QAï¼‰çº¯ä½¿ç”¨LLMè‡ªèº«çŸ¥è¯† many-shots QAï¼ˆMS QAï¼‰åªåŠ å…¥mä¸ªç¤ºä¾‹ RAG åªä½¿ç”¨kä¸ªæ–‡æ¡£ DRAG IterRAG 4.2æ€»ä½“æ€§èƒ½ ç»“è®ºï¼šåœ¨ä»»æ„ä¸Šä¸‹æ–‡é•¿åº¦ã€ä»»æ„æ•°æ®é›†ä¸­ï¼Œéƒ½æ˜¯DRAGã€IterRAGæ•ˆæœè¾ƒå¥½ï¼›è¾¾ä¸åˆ°æŒ‡å®šçš„ä¸Šä¸‹æ–‡é•¿åº¦æ—¶ä¼šè¢«çœç•¥ã€‚è€Œä¸”æœ€å¥½çš„På€¼éšä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚\nDRAGã€IterdDRAGçš„æ€§èƒ½éšä¸Šä¸‹æ–‡é•¿åº¦çš„æ‰©å±•è€Œä¸æ–­æé«˜ï¼Œè€Œå…¶ä½™æ–¹æ³•å¾ˆå¿«è¾¾åˆ°å³°å€¼ã€‚DRAG åœ¨è¾ƒçŸ­çš„æœ€å¤§é•¿åº¦ä¸‹è¡¨ç°å‡ºè‰²ï¼Œè€Œ IterDRAG åœ¨æ›´é•¿çš„æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ä¸‹æ›´æœ‰æ•ˆã€‚\n4.3RAGæ¨ç†ç¼©æ”¾å®šå¾‹ éšç€æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çš„æ‰©å¤§ï¼Œæœ€ä½³æ€§èƒ½è¡¨ç°å‡ºçº¿æ€§å¢é•¿ã€‚æ‰€ä»¥å¯ä»¥é€šè¿‡å¢åŠ è®¡ç®—æ¥æé«˜ RAG æ€§èƒ½ï¼Œä»è€Œåœ¨ç»™å®šå¯ç”¨è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹æ›´å‡†ç¡®åœ°é¢„æµ‹æ€§èƒ½ã€‚ å¯¹äº $L_{max}$ é«˜äº $10^5$â€‹ï¼ŒIterDRAG ç»§ç»­é€šè¿‡äº¤é”™æ£€ç´¢å’Œè¿­ä»£ç”Ÿæˆè¿›è¡Œæœ‰æ•ˆæ‰©å±•ã€‚è¯´æ˜å…¶æ›´é€‚åˆé•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚ ä¸Šä¸‹æ–‡è¶…è¿‡1Måï¼Œæ€§èƒ½æå‡ä¸æ˜æ˜¾ã€‚ 4.4å¯¹äºç‰¹å®šå‚æ•°çš„ç¼©æ”¾ æ–‡æ¡£å’Œç¤ºä¾‹å¹¶éèµ·åˆ°åŒæ ·ä½œç”¨ã€‚å¯¹äºå›ºå®šé…ç½®ï¼Œå¢åŠ æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡é€šå¸¸ä¼šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½æå‡ã€‚ä½†kå’Œmä¹Ÿæ˜¯æœ‰é˜ˆå€¼çš„ã€‚ å¢åŠ ç¤ºä¾‹å¯¹IterRAGæ›´æœ‰å¸®åŠ©ï¼Œå¦‚ç¤ºä¾‹ä»0-1æ—¶ï¼ŒIterRAGçš„æ€§èƒ½æ˜æ˜¾æå‡ï¼›è€Œç¤ºä¾‹å¯¹DRAGä¸æ˜æ˜¾ã€‚ 5.é•¿ä¸Šä¸‹æ–‡ RAG çš„æ¨ç†è®¡ç®—åˆ†é… æ„å»ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œç›®çš„æ˜¯ä¸ºäº†èƒ½æ ¹æ® $L_{max}$ æ±‚ $\\theta$ã€‚\næ€§èƒ½æŒ‡æ ‡ï¼ˆğ‘ƒï¼‰\nè¡¨ç¤ºåœ¨æ•°æ®é›† $ X $ ä¸Šçš„è¡¨ç°ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰ï¼Œå»ºæ¨¡ä¸ºå‚æ•° $ \\theta $ çš„å‡½æ•°ã€‚ èµ„æºå‚æ•°ï¼ˆğœƒï¼‰\nå®šä¹‰ä¸ºä¸‰ç»´å‘é‡ $ \\theta := (k, m, n)^T $ï¼ŒåŒ…å«ï¼š ( k )ï¼šä½¿ç”¨çš„æ–‡æ¡£æ•°é‡ ( m )ï¼šä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼ˆdemonstrations/shotsï¼‰çš„æ•°é‡ ( n )ï¼šæœ€å¤§è¿­ä»£/ç”Ÿæˆæ­¥æ•° ä¿¡æ¯é‡å‚æ•°ï¼ˆğ‘–ï¼‰\nè¡¡é‡è¾“å…¥å†…å®¹çš„ä¿¡æ¯ä»·å€¼ï¼Œå®šä¹‰ä¸º $ i := (i_{\\text{doc}}, i_{\\text{shot}}, 0)^T $ï¼š $ i_{\\text{doc}} $ï¼šæ–‡æ¡£çš„ä¿¡æ¯é‡ é€šè¿‡â€œæ·»åŠ 1ç¯‡æ–‡æ¡£ vs é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰çš„æ€§èƒ½å·®å¼‚â€è®¡ç®—ã€‚ $ i_{\\text{shot}} $ï¼šç¤ºä¾‹çš„ä¿¡æ¯é‡ é€šè¿‡â€œæ·»åŠ 1ä¸ªç¤ºä¾‹ vs é›¶æ ·æœ¬çš„æ€§èƒ½å·®å¼‚â€è®¡ç®—ã€‚ $ i_{\\text{iter}} $ è¢«å¿½ç•¥ï¼ˆè®¾ä¸º0ï¼‰ï¼Œå› å®éªŒä¸­å‘ç°å¢åŠ ç”Ÿæˆæ­¥æ•°å¯¹æ€§èƒ½æ— æ˜¾è‘—æå‡ã€‚ æ€§èƒ½æ¨¡å‹å…¬å¼\n$P(\\theta) \\approx \\sigma((a + b \\odot i)^T \\log(\\theta) + c)$\nç¬¦å·è¯´æ˜ï¼š $ \\odot $ï¼šé€å…ƒç´ ç›¸ä¹˜ï¼ˆHadamardç§¯ï¼‰ã€‚ $ a, b \\in \\mathbb{R}^3 $ï¼šå¾…ä¼°è®¡å‚æ•°ï¼Œåˆ†åˆ«è¡¨ç¤ºèµ„æºçš„åŸºç¡€æ•ˆåº”å’Œä¸ä¿¡æ¯é‡çš„äº¤äº’æ•ˆåº”ã€‚ $ c $ï¼šå¸¸æ•°åç½®é¡¹ã€‚ aã€bã€céƒ½æ˜¯åœ¨ç‰¹å®šæƒ…å†µä¸‹æ‹Ÿåˆçš„ã€‚ $ \\log(\\theta) $ï¼šå¯¹èµ„æºå‘é‡é€å…ƒç´ å–å¯¹æ•°ã€‚ $ \\sigma $ï¼šSigmoidå‡½æ•°ã€‚ $L_{max}$ ç›¸å½“äº $\\theta$ çš„é™åˆ¶æ¡ä»¶ï¼›æ‹Ÿåˆå‡ºabcï¼›ç›®æ ‡æ˜¯æœ€å¤§åŒ– $P(\\theta)$ ï¼Œå°±å¯ä»¥å¾—åˆ°æœ€å¥½çš„ $\\theta$ã€‚\n","wordCount":"866","inLanguage":"en","datePublished":"2025-05-07T23:04:00+08:00","dateModified":"2025-05-07T23:04:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/inference-scaling-for-long-context-retrieval-augmented-generation/"},"publisher":{"@type":"Organization","name":"Chan's Blog","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/apple-touch-icon.png"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="Chan's Blog (Alt + H)"><img src=https://Rook1eChan.github.io/apple-touch-icon.png alt aria-label=logo height=35>Chan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Inference Scaling for Long-Context Retrieval Augmented Generation</h1><div class=post-meta><span title='2025-05-07 23:04:00 +0800 +0800'>May 7, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0%e7%9b%ae%e6%a0%87 aria-label=0.ç›®æ ‡>0.ç›®æ ‡</a></li><li><a href=#1%e8%b4%a1%e7%8c%ae aria-label=1.è´¡çŒ®>1.è´¡çŒ®</a></li><li><a href=#2%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c aria-label=2.ç›¸å…³å·¥ä½œ>2.ç›¸å…³å·¥ä½œ</a><ul><li><a href=#21%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87llms aria-label=2.1é•¿ä¸Šä¸‹æ–‡LLMs>2.1é•¿ä¸Šä¸‹æ–‡LLMs</a></li><li><a href=#22%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0 aria-label=2.2ä¸Šä¸‹æ–‡å­¦ä¹ >2.2ä¸Šä¸‹æ–‡å­¦ä¹ </a></li><li><a href=#23rag aria-label=2.3RAG>2.3RAG</a></li></ul></li><li><a href=#3%e5%9f%ba%e4%ba%8erag%e7%9a%84%e6%8e%a8%e7%90%86%e6%89%a9%e5%b1%95%e7%ad%96%e7%95%a5 aria-label=3.åŸºäºRAGçš„æ¨ç†æ‰©å±•ç­–ç•¥>3.åŸºäºRAGçš„æ¨ç†æ‰©å±•ç­–ç•¥</a><ul><li><a href=#31drag aria-label=3.1DRAG>3.1DRAG</a></li><li><a href=#32iterdrag aria-label=3.2IterDRAG>3.2IterDRAG</a></li></ul></li><li><a href=#4rag%e6%80%a7%e8%83%bd%e5%92%8c%e6%8e%a8%e7%90%86%e8%ae%a1%e7%ae%97%e8%a7%84%e6%a8%a1 aria-label=4.RAGæ€§èƒ½å’Œæ¨ç†è®¡ç®—è§„æ¨¡>4.RAGæ€§èƒ½å’Œæ¨ç†è®¡ç®—è§„æ¨¡</a><ul><li><a href=#41%e7%bb%99%e5%ae%9a%e9%a2%84%e7%ae%97%e6%9c%80%e5%a4%a7%e6%9c%89%e6%95%88%e4%b8%8a%e4%b8%8b%e6%96%87%e9%95%bf%e5%ba%a6%e4%b8%8b%e7%9a%84%e6%9c%80%e4%bd%b3%e6%80%a7%e8%83%bd aria-label=4.1ç»™å®šé¢„ç®—ï¼ˆæœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ä¸‹çš„æœ€ä½³æ€§èƒ½>4.1ç»™å®šé¢„ç®—ï¼ˆæœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ä¸‹çš„æœ€ä½³æ€§èƒ½</a></li><li><a href=#42%e6%80%bb%e4%bd%93%e6%80%a7%e8%83%bd aria-label=4.2æ€»ä½“æ€§èƒ½>4.2æ€»ä½“æ€§èƒ½</a></li><li><a href=#43rag%e6%8e%a8%e7%90%86%e7%bc%a9%e6%94%be%e5%ae%9a%e5%be%8b aria-label=4.3RAGæ¨ç†ç¼©æ”¾å®šå¾‹>4.3RAGæ¨ç†ç¼©æ”¾å®šå¾‹</a></li><li><a href=#44%e5%af%b9%e4%ba%8e%e7%89%b9%e5%ae%9a%e5%8f%82%e6%95%b0%e7%9a%84%e7%bc%a9%e6%94%be aria-label=4.4å¯¹äºç‰¹å®šå‚æ•°çš„ç¼©æ”¾>4.4å¯¹äºç‰¹å®šå‚æ•°çš„ç¼©æ”¾</a></li></ul></li><li><a href=#5%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87-rag-%e7%9a%84%e6%8e%a8%e7%90%86%e8%ae%a1%e7%ae%97%e5%88%86%e9%85%8d aria-label="5.é•¿ä¸Šä¸‹æ–‡ RAG çš„æ¨ç†è®¡ç®—åˆ†é…">5.é•¿ä¸Šä¸‹æ–‡ RAG çš„æ¨ç†è®¡ç®—åˆ†é…</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>ICLR2025ï¼Œæ¥è‡ªGoogle DeepMindå›¢é˜Ÿçš„å·¥ä½œ</p><p><a href=https://arxiv.org/abs/2410.04343v2>https://arxiv.org/abs/2410.04343v2</a></p><h2 id=0ç›®æ ‡>0.ç›®æ ‡<a hidden class=anchor aria-hidden=true href=#0ç›®æ ‡>#</a></h2><p>å…ˆå‰å¯¹äºRAGæ¨ç†æ‰©å±•çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæä¾›æ›´å¤šçš„çŸ¥è¯†ï¼Œä½†åªå¢åŠ çŸ¥è¯†çš„æ•°é‡æ˜¯ä¸å¤Ÿçš„ã€‚å½“å‰çš„LLMåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ¯”å¦‚ï¼Œåœ¨è¶…é•¿åºåˆ—ä¸­å®šä½æœ‰æ•ˆä¿¡æ¯çš„èƒ½åŠ›æœ‰é™ã€æœ€ä½³æ€§èƒ½å¾€å¾€æ˜¯åœ¨æ²¡æœ‰å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å®ç°çš„ã€è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ˆæ–‡æ¡£æ•°é‡ï¼‰çš„æ£€ç´¢ä¼šä½¿æ€§èƒ½åœæ»ç”šè‡³ä¸‹é™ã€‚</p><p>ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æœ€ä¼˜é…ç½®ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿé¢„æµ‹æœ€ä½³æ¨ç†å‚æ•°ï¼Œæœ€å¤§é™åº¦æé«˜RAGæ€§èƒ½ã€‚å…¶ä¸­DRAGçš„å‚æ•°ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡å’Œç¤ºä¾‹æ•°é‡ã€‚IterDRAGçš„å‚æ•°ä¸ºç”Ÿæˆæ¬¡æ•°ã€‚</p><h2 id=1è´¡çŒ®>1.è´¡çŒ®<a hidden class=anchor aria-hidden=true href=#1è´¡çŒ®>#</a></h2><ul><li>æå‡ºä¸¤ç§RAGæ–¹æ³•ï¼šDRAGï¼ˆåŸºäºæ¼”ç¤ºçš„RAGï¼Œä¸ºLLMæä¾›å¤šä¸ªRAGç¤ºä¾‹ï¼‰å’ŒIterDRAGï¼ˆåŸºäºè¿­ä»£æ¼”ç¤ºçš„RAGï¼Œå°†è¾“å…¥æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œè¿­ä»£æ£€ç´¢ï¼‰ã€‚å¹¶è¯æ˜äº†è¿™ä¸¤ç§æ–¹æ³•ä¼˜äºä»…æä¾›çŸ¥è¯†çš„RAGã€‚</li><li>å¾—åˆ°äº†RAGçš„æ¨ç†ç¼©æ”¾å®šå¾‹ï¼šåœ¨æœ€ä½³é…ç½®ä¸‹ï¼ŒRAGæ€§èƒ½éšæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚</li><li>æ ¹æ®å®šå¾‹å¯¹RAGæ€§èƒ½ä¸ä¸åŒæ¨ç†å‚æ•°å»ºæ¨¡ï¼Œæ¨å¯¼å‡ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡RAGçš„ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚</li></ul><h2 id=2ç›¸å…³å·¥ä½œ>2.ç›¸å…³å·¥ä½œ<a hidden class=anchor aria-hidden=true href=#2ç›¸å…³å·¥ä½œ>#</a></h2><h3 id=21é•¿ä¸Šä¸‹æ–‡llms>2.1é•¿ä¸Šä¸‹æ–‡LLMs<a hidden class=anchor aria-hidden=true href=#21é•¿ä¸Šä¸‹æ–‡llms>#</a></h3><p>æ—©æœŸé‡‡ç”¨ç¨€ç–/ä½ç§©æ ¸æ¥å‡å°‘å†…å­˜éœ€æ±‚ã€‚</p><blockquote><p>I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. <em>arXiv preprint</em> <em>arXiv:2004.05150</em>, 2020.</p><p>K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. <em>arXiv preprint arXiv:2009.14794</em>,2020.</p><p>N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In <em>International Conference</em> <em>on Learning Representations</em>, 2019.</p><p>M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. <em>Advances in neural information processing</em> <em>systems</em>, 33:17283â€“17297, 2020</p></blockquote><p>æ­¤å¤–é€’å½’å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰è¢«æå‡ºï¼Œä½œä¸ºåŸºäºtransformeræ¨¡å‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚</p><blockquote><p>M. Beck, K. PÃ¶ppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended long short-term memory. <em>arXiv preprint arXiv:2405.04517</em>, 2024</p><p>A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. <em>arXiv preprint</em> <em>arXiv:2312.00752</em>, 2023.</p><p>B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, L. Derczynski, et al. RWKV: Reinventing rnns for the transformer era. In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 14048â€“14077, 2023a.</p></blockquote><p>å¯¹äºå› æœLLMsï¼Œå¤–æ¨å’Œæ’å€¼æ–¹æ³•å·²è¢«è¯æ˜åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æ–¹é¢éå¸¸æœ‰æ•ˆã€‚</p><blockquote><p>S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. <em>arXiv preprint arXiv:2306.15595</em>, 2023.</p><p>B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. <em>arXiv preprint arXiv:2309.00071</em>, 2023b.</p><p>O. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. <em>arXiv preprint arXiv:2108.12409</em>, 2021.</p><p>Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A length-extrapolatable transformer. In <em>Proceedings of the 61st Annual Meeting of the Association for</em> <em>Computational Linguistics (Volume 1: Long Papers)</em>, pages 14590â€“14604, 2023.</p></blockquote><p>æœ€è¿‘åœ¨é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶æ–¹é¢çš„è¿›å±•ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿè®­ç»ƒå’Œæ¨ç†åŒ…å«æ•°ç™¾ä¸‡ä¸ªæ ‡è®°çš„è¾“å…¥åºåˆ—ã€‚</p><blockquote><p>T. Dao, D. Fu, S. Ermon, A. Rudra, and C. RÃ©. Flashattention: Fast and memory-efficient exact attention with IO-awareness. <em>Advances in Neural Information Processing Systems</em>, 35:16344â€“16359, 2022</p><p>S. A. Jacobs, M. Tanaka, C. Zhang, M. Zhang, L. Song, S. Rajbhandari, and Y. He. DeepSpeed Ulysses:System optimizations for enabling training of extreme long sequence transformer models. <em>arXiv</em> <em>preprint arXiv:2309.14509</em>, 2023.</p><p>H. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite context. <em>arXiv preprint arXiv:2310.01889</em>, 2023.</p><p>J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. <em>arXiv preprint arXiv:2303.08774</em>, 2023.</p><p>M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. <em>arXiv preprint arXiv:2403.05530</em>, 2024.</p><p>G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. <em>arXiv preprint arXiv:2312.11805</em>, 2023.</p></blockquote><h3 id=22ä¸Šä¸‹æ–‡å­¦ä¹ >2.2ä¸Šä¸‹æ–‡å­¦ä¹ <a hidden class=anchor aria-hidden=true href=#22ä¸Šä¸‹æ–‡å­¦ä¹ >#</a></h3><p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„æ–¹æ³•ï¼Œé€šè¿‡ä¾èµ–å°‘é‡ä»»åŠ¡ç¤ºä¾‹æ¥æé«˜æ¨¡å‹åœ¨æ¨ç†æ—¶çš„è¡¨ç°ã€‚</p><p>ä¸ºäº†è¿›ä¸€æ­¥æå‡ICLæ€§èƒ½ï¼Œç°æœ‰ç ”ç©¶é›†ä¸­åœ¨é¢„è®­ç»ƒç­–ç•¥ä¸Šï¼Œä¼˜åŒ–è¯­è¨€æ¨¡å‹ä»¥å®ç°ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</p><p>æ­¤å¤–ï¼Œé€‰æ‹©æ€§ä½¿ç”¨å°‘é‡ç¤ºä¾‹ä¹Ÿè¢«è¯æ˜æœ‰åŠ©äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚</p><p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé‡æ–°æ ¼å¼åŒ–æˆ–æ‰¾åˆ°æœ€ä½³é¡ºåºçš„åœºæ™¯ç¤ºä¾‹ä¹Ÿèƒ½æé«˜ICLçš„æœ‰æ•ˆæ€§ã€‚</p><p>éšç€é•¿æƒ…å¢ƒè¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œåœ¨ICLä¸­æ‰©å±•ç¤ºä¾‹æ•°é‡æˆä¸ºå¯èƒ½ã€‚</p><p>ä¾‹å¦‚ï¼ŒAgarwalç­‰äººè¡¨æ˜ï¼Œå¤šæ ·æœ¬ICLå¯ä»¥å‡è½»LLMä¸­çš„é¢„è®­ç»ƒåå·®ï¼Œä»è€Œæé«˜å„ç§ä»»åŠ¡çš„ICLæ€§èƒ½ã€‚</p><h3 id=23rag>2.3RAG<a hidden class=anchor aria-hidden=true href=#23rag>#</a></h3><p>RAGé€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æ¥æé«˜è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</p><p>å’ŒnaiveRAGç›¸æ¯”ï¼Œä¼˜åŒ–æ£€ç´¢é˜¶æ®µå¯ä»¥æœ‰æ•ˆæå‡ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚</p><p>REPLUGä½¿ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºç›‘ç£æ¥å­¦ä¹ ä¸€ä¸ªå¯†é›†æ£€ç´¢å™¨æ¨¡å‹ã€‚</p><p>æ­¤å¤–ï¼Œç¼–ç æ–‡æ¡£å¯ä»¥å¢åŠ çŸ¥è¯†æ£€ç´¢å¹¶æé«˜ç”Ÿæˆèƒ½åŠ›ã€‚</p><p>Izacardå’ŒGraveï¼ˆ2021ï¼‰åˆ©ç”¨èåˆè§£ç å™¨æ¶æ„æ¥ç¼–ç å¤šä¸ªé—®é¢˜-æ®µè½å¯¹ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ•ˆç‡ã€‚</p><p>æˆ–è€…ï¼Œæœ‰é€‰æ‹©åœ°åˆ©ç”¨æ–‡æ¡£ä¸­çš„çŸ¥è¯†å¯ä»¥æé«˜è¯­è¨€æ¨¡å‹å¯¹æ— å…³ä¸Šä¸‹æ–‡çš„é²æ£’æ€§ã€‚ä¾‹å¦‚ï¼ŒRAFTæå‡ºä½¿ç”¨è´Ÿæ–‡æ¡£è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥æé«˜ç”Ÿæˆè´¨é‡å’Œç›¸å…³æ€§ï¼ˆZhangç­‰äººï¼Œ2024ï¼‰ã€‚ä¸æˆ‘ä»¬çš„å·¥ä½œåŒæ—¶ï¼Œæå‡ºäº†é•¿æ–‡æ¡£æ£€ç´¢å’Œæ•°æ®é›†æ‰©å±•ä»¥ä¼˜åŒ–RAGæ€§èƒ½ï¼ˆJiangç­‰äººï¼Œ2024ï¼›Shaoç­‰äººï¼Œ2024ï¼‰ã€‚</p><h2 id=3åŸºäºragçš„æ¨ç†æ‰©å±•ç­–ç•¥>3.åŸºäºRAGçš„æ¨ç†æ‰©å±•ç­–ç•¥<a hidden class=anchor aria-hidden=true href=#3åŸºäºragçš„æ¨ç†æ‰©å±•ç­–ç•¥>#</a></h2><p>æˆ‘ä»¬ä½¿ç”¨æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦æ¥æµ‹é‡æ¨ç†è®¡ç®—ï¼Œæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦å®šä¹‰ä¸º LLM è¾“å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ï¼Œæ‰€æœ‰è¿­ä»£çš„è¾“å…¥tokenæ€»æ•°ã€‚æ¯æ¬¡çš„è¾“å…¥å—åˆ°LLMä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ã€‚å¿½ç•¥è¾“å‡ºçš„tokenå’Œæ£€ç´¢çš„å¼€é”€ã€‚</p><h3 id=31drag>3.1DRAG<a hidden class=anchor aria-hidden=true href=#31drag>#</a></h3><p>DRAGåŸºäºNaiveRAGæ„å»ºï¼Œå°†æ£€ç´¢åˆ°çš„topkæ–‡æ¡£å’Œä¸Šä¸‹æ–‡ç¤ºä¾‹é›†æˆåˆ°è¾“å…¥ä¸­ã€‚é¢ å€’æ–‡æ¡£é¡ºåºï¼Œ<u>å°†æ’åè¾ƒé«˜çš„æ–‡æ¡£æ”¾åœ¨ç¦»æŸ¥è¯¢è¿‘çš„ä½ç½®</u>ã€‚</p><h3 id=32iterdrag>3.2IterDRAG<a hidden class=anchor aria-hidden=true href=#32iterdrag>#</a></h3><p>å°†æŸ¥è¯¢åˆ†è§£ä¸ºæ›´ç®€å•çš„å­æŸ¥è¯¢ï¼Œæ¯ä¸ªå­æŸ¥è¯¢æ‰§è¡Œæ£€ç´¢å¹¶ç”Ÿæˆä¸­é—´ç­”æ¡ˆã€‚è§£å†³å®Œæ‰€æœ‰å­æŸ¥è¯¢åï¼Œå°†æ‰€æœ‰ä¸Šä¸‹æ–‡ã€å­æŸ¥è¯¢ã€ä¸­é—´ç­”æ¡ˆç»„åˆåœ¨ä¸€èµ·ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚</p><blockquote><p>ç”±äºç°æœ‰çš„æ•°æ®é›†ä¸å¸¦æœ‰å­æŸ¥è¯¢å’Œä¸­é—´ç­”æ¡ˆï¼Œæ‰€ä»¥è®©å¤§æ¨¡å‹ä½¿ç”¨çº¦æŸè§£ç ï¼ˆconstrained decodingï¼‰å¹¶éµå¾ªself-askæ ¼å¼ç”Ÿæˆexampleã€‚</p><p>ç”Ÿæˆexampleçš„æ­¥éª¤ï¼š</p><p>åœ¨æ¯ä¸€è½®ä¸­ï¼Œç”Ÿæˆå­æŸ¥è¯¢ï¼Œç„¶åå°†æŸ¥è¯¢åˆ°çš„æ–‡æ¡£<u>äº¤é”™</u>æ”¾å…¥promptï¼Œå†ç”Ÿæˆä¸­é—´ç­”æ¡ˆã€‚</p><p>æœ€ç»ˆç­”æ¡ˆç”Ÿæˆã€‚æˆ–è€…è¾¾åˆ°æœ€å¤§è½®æ•°åï¼Œå¼ºåˆ¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚</p><p>æ–‡æ¡£ã€å­æŸ¥è¯¢-ä¸­é—´ç­”æ¡ˆã€æœ€ç»ˆç­”æ¡ˆä¸€èµ·æ„æˆexampleã€‚</p></blockquote><p>æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸Šä¸‹æ–‡ç¤ºä¾‹æ·»åŠ åˆ°åˆå§‹æ–‡æ¡£ä¹‹å‰ã€‚</p><p>IterRAGè¿˜å°†å­¦ä¹ ï¼š1.å°†é—®é¢˜åˆ†è§£ä¸ºç®€å•å¯æ§çš„å­é—®é¢˜ 2.æå–å­é—®é¢˜çš„ç›¸å…³ä¿¡æ¯</p><p>è¿™ä¸€æ–¹æ³•æœ‰åŠ©äºæé«˜RAGå›ç­”å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚</p><p>å…·ä½“æ“ä½œè§é™„å½•Hã€‚</p><h2 id=4ragæ€§èƒ½å’Œæ¨ç†è®¡ç®—è§„æ¨¡>4.RAGæ€§èƒ½å’Œæ¨ç†è®¡ç®—è§„æ¨¡<a hidden class=anchor aria-hidden=true href=#4ragæ€§èƒ½å’Œæ¨ç†è®¡ç®—è§„æ¨¡>#</a></h2><h3 id=41ç»™å®šé¢„ç®—æœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ä¸‹çš„æœ€ä½³æ€§èƒ½>4.1ç»™å®šé¢„ç®—ï¼ˆæœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ä¸‹çš„æœ€ä½³æ€§èƒ½<a hidden class=anchor aria-hidden=true href=#41ç»™å®šé¢„ç®—æœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ä¸‹çš„æœ€ä½³æ€§èƒ½>#</a></h3><p>é™åˆ¶æœ€å¤§è¾“å…¥tokenå³$L_{max}$æ—¶ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´æ¨ç†å‚æ•° $\theta$ æ¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ã€‚</p><ul><li>åœ¨ <strong>DRAG</strong> ä¸­ï¼Œå¯ä»¥è°ƒæ•´ <strong>æ£€ç´¢æ–‡æ¡£æ•°é‡ ( k )</strong> å’Œ <strong>ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡ ( m )</strong>ï¼›</li><li>åœ¨ <strong>IterDRAG</strong> ä¸­ï¼Œé¢å¤–å¼•å…¥äº† <strong>æ£€ç´¢ä¸ç”Ÿæˆçš„è¿­ä»£æ¬¡æ•° ( n )</strong>ã€‚</li></ul><p>å¯¹äºæ¯ä¸ªè¾“å…¥æŸ¥è¯¢åŠå…¶çœŸå®ç­”æ¡ˆ $ (x_i, y_i) \in \mathcal{X} $ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨å‚æ•°ä¸º $ \theta $ çš„ RAG æ¨ç†ç­–ç•¥ $ f $ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ $ \hat{y}_i = f(x_i; \theta) $ï¼Œå¹¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡ $ P(y_i, \hat{y}_i) $ã€‚</p><p>ä¸ºäº†ç ”ç©¶ RAG æ€§èƒ½ä¸æ¨ç†è®¡ç®—é‡ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„è®¡ç®—é¢„ç®— $ L_{\text{max}} $ ä¸‹é‡‡æ ·ï¼Œå¹¶é€šè¿‡æšä¸¾ä¸åŒçš„ $ \theta \in \Theta $ æ¥å¯»æ‰¾è¯¥é¢„ç®—ä¸‹çš„æœ€ä¼˜å¹³å‡æ€§èƒ½ $ P^*(L_{\text{max}}) $ï¼š</p><p>$P^*(L_{\text{max}}) := \max_{\theta \in \Theta} \left\{ \frac{1}{|\mathcal{X}|} \sum_i P(y_i, f(x_i; \theta)) \ \Bigg| \ \forall i, l(x_i; \theta) \leq L_{\text{max}} \right\}.
\quad$</p><ul><li>$ \mathcal{X} $ï¼šæµ‹è¯•é›†ï¼ŒåŒ…å«è¾“å…¥æŸ¥è¯¢ $ x_i $ å’ŒçœŸå®ç­”æ¡ˆ $ y_i $ çš„é…å¯¹ $ (x_i, y_i) $ã€‚</li><li>$ \theta $ï¼šRAG æ¨ç†å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š<ul><li>$ k $ï¼šæ£€ç´¢çš„æ–‡æ¡£æ•°é‡ï¼Œ</li><li>$ m $ï¼šä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼ˆin-context examplesï¼‰æ•°é‡ï¼Œ</li><li>$ n $ï¼šç”Ÿæˆè¿­ä»£æ¬¡æ•°ï¼ˆDRAG ä¸­ $ n=1 $ï¼ŒIterDRAG ä¸­ $ n \geq 1 $ï¼‰ã€‚</li></ul></li><li>$ f(x_i; \theta) $ï¼šä½¿ç”¨å‚æ•° $ \theta$ çš„ RAG ç­–ç•¥å¯¹æŸ¥è¯¢ $ x_i $ çš„é¢„æµ‹ç»“æœ $ \hat{y}_i $ã€‚</li><li>$ P(y_i, \hat{y}_i) $ï¼šè¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1 åˆ†æ•°ç­‰ï¼‰ï¼Œè¡¡é‡é¢„æµ‹ç­”æ¡ˆ $ \hat{y}_i $ ä¸çœŸå®ç­”æ¡ˆ $ y_i $ çš„åŒ¹é…ç¨‹åº¦ã€‚</li><li>$ l(x_i; \theta) $ï¼šå¯¹æŸ¥è¯¢ $ x_i $ ä½¿ç”¨å‚æ•° $ \theta $ æ—¶çš„å®é™…ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå³æ‰€æœ‰è¾“å…¥ tokens çš„æ€»å’Œï¼‰ã€‚</li><li>$ L_{\text{max}} $ï¼šé¢„ç®—çº¦æŸï¼Œå³å…è®¸çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚</li></ul><p>åœ¨æ‰€æœ‰æ»¡è¶³çº¦æŸçš„ $ \theta $ ä¸­ï¼Œé€‰æ‹©ä½¿å¹³å‡æ€§èƒ½ $ \frac{1}{|\mathcal{X}|} \sum_i P(\cdot) $ æœ€å¤§çš„å‚æ•°ç»„åˆã€‚</p><p>å®éªŒï¼š</p><p>è¯„ä¼°Genmini 1.5 Flashï¼ˆä¸Šä¸‹æ–‡çª—å£æœ€é«˜1Mï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”æ•°æ®é›†ï¼ˆBamboogleã€HotpotQAã€MuSiQue å’Œ 2WikiMultiHopQAï¼‰ä¸Šçš„æ€§èƒ½ã€‚</p><p>è¯„ä¼°æŒ‡æ ‡ä¸ºexact matchï¼ˆEMï¼‰ã€F1ã€Acc</p><p>$L_{max} \in \{16k, 32k, 128k, 1M, 5M \} $tokens</p><p>å¯¹äºDRAGï¼Œæ£€ç´¢æ–‡æ¡£æ•°é‡ $k \in \{0,1,2,5,10,20,50,100,200,500,1000\}$ï¼Œç¤ºä¾‹æ•°é‡ $m \in \{0,2^0,2^1,...,2^8\}$</p><p>å¯¹äºIterRAGï¼Œ$n \in \{1,2,3,4,5\}$</p><p>æ¨¡å‹ï¼š</p><ol><li>zero-shot QAï¼ˆZS QAï¼‰çº¯ä½¿ç”¨LLMè‡ªèº«çŸ¥è¯†</li><li>many-shots QAï¼ˆMS QAï¼‰åªåŠ å…¥mä¸ªç¤ºä¾‹</li><li>RAG åªä½¿ç”¨kä¸ªæ–‡æ¡£</li><li>DRAG</li><li>IterRAG</li></ol><h3 id=42æ€»ä½“æ€§èƒ½>4.2æ€»ä½“æ€§èƒ½<a hidden class=anchor aria-hidden=true href=#42æ€»ä½“æ€§èƒ½>#</a></h3><p>ç»“è®ºï¼šåœ¨ä»»æ„ä¸Šä¸‹æ–‡é•¿åº¦ã€ä»»æ„æ•°æ®é›†ä¸­ï¼Œéƒ½æ˜¯DRAGã€IterRAGæ•ˆæœè¾ƒå¥½ï¼›è¾¾ä¸åˆ°æŒ‡å®šçš„ä¸Šä¸‹æ–‡é•¿åº¦æ—¶ä¼šè¢«çœç•¥ã€‚è€Œä¸”æœ€å¥½çš„På€¼éšä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å˜åŒ–ã€‚</p><p>DRAGã€IterdDRAGçš„æ€§èƒ½éšä¸Šä¸‹æ–‡é•¿åº¦çš„æ‰©å±•è€Œä¸æ–­æé«˜ï¼Œè€Œå…¶ä½™æ–¹æ³•å¾ˆå¿«è¾¾åˆ°å³°å€¼ã€‚DRAG åœ¨è¾ƒçŸ­çš„æœ€å¤§é•¿åº¦ä¸‹è¡¨ç°å‡ºè‰²ï¼Œè€Œ IterDRAG åœ¨æ›´é•¿çš„æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ä¸‹æ›´æœ‰æ•ˆã€‚</p><h3 id=43ragæ¨ç†ç¼©æ”¾å®šå¾‹>4.3RAGæ¨ç†ç¼©æ”¾å®šå¾‹<a hidden class=anchor aria-hidden=true href=#43ragæ¨ç†ç¼©æ”¾å®šå¾‹>#</a></h3><ol><li>éšç€æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çš„æ‰©å¤§ï¼Œæœ€ä½³æ€§èƒ½è¡¨ç°å‡ºçº¿æ€§å¢é•¿ã€‚æ‰€ä»¥å¯ä»¥é€šè¿‡å¢åŠ è®¡ç®—æ¥æé«˜ RAG æ€§èƒ½ï¼Œä»è€Œåœ¨ç»™å®šå¯ç”¨è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹æ›´å‡†ç¡®åœ°é¢„æµ‹æ€§èƒ½ã€‚</li><li>å¯¹äº $L_{max}$ é«˜äº $10^5$â€‹ï¼ŒIterDRAG ç»§ç»­é€šè¿‡äº¤é”™æ£€ç´¢å’Œè¿­ä»£ç”Ÿæˆè¿›è¡Œæœ‰æ•ˆæ‰©å±•ã€‚è¯´æ˜å…¶æ›´é€‚åˆé•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚</li><li>ä¸Šä¸‹æ–‡è¶…è¿‡1Måï¼Œæ€§èƒ½æå‡ä¸æ˜æ˜¾ã€‚</li></ol><h3 id=44å¯¹äºç‰¹å®šå‚æ•°çš„ç¼©æ”¾>4.4å¯¹äºç‰¹å®šå‚æ•°çš„ç¼©æ”¾<a hidden class=anchor aria-hidden=true href=#44å¯¹äºç‰¹å®šå‚æ•°çš„ç¼©æ”¾>#</a></h3><ol><li>æ–‡æ¡£å’Œç¤ºä¾‹å¹¶éèµ·åˆ°åŒæ ·ä½œç”¨ã€‚å¯¹äºå›ºå®šé…ç½®ï¼Œå¢åŠ æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡é€šå¸¸ä¼šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½æå‡ã€‚ä½†kå’Œmä¹Ÿæ˜¯æœ‰é˜ˆå€¼çš„ã€‚</li><li>å¢åŠ ç¤ºä¾‹å¯¹IterRAGæ›´æœ‰å¸®åŠ©ï¼Œå¦‚ç¤ºä¾‹ä»0-1æ—¶ï¼ŒIterRAGçš„æ€§èƒ½æ˜æ˜¾æå‡ï¼›è€Œç¤ºä¾‹å¯¹DRAGä¸æ˜æ˜¾ã€‚</li></ol><h2 id=5é•¿ä¸Šä¸‹æ–‡-rag-çš„æ¨ç†è®¡ç®—åˆ†é…>5.é•¿ä¸Šä¸‹æ–‡ RAG çš„æ¨ç†è®¡ç®—åˆ†é…<a hidden class=anchor aria-hidden=true href=#5é•¿ä¸Šä¸‹æ–‡-rag-çš„æ¨ç†è®¡ç®—åˆ†é…>#</a></h2><p>æ„å»ºè®¡ç®—åˆ†é…æ¨¡å‹ï¼Œç›®çš„æ˜¯ä¸ºäº†èƒ½æ ¹æ® $L_{max}$ æ±‚ $\theta$ã€‚</p><ol><li><p><strong>æ€§èƒ½æŒ‡æ ‡ï¼ˆğ‘ƒï¼‰</strong></p><ul><li>è¡¨ç¤ºåœ¨æ•°æ®é›† $ X $ ä¸Šçš„è¡¨ç°ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰ï¼Œå»ºæ¨¡ä¸ºå‚æ•° $ \theta $ çš„å‡½æ•°ã€‚</li></ul></li><li><p><strong>èµ„æºå‚æ•°ï¼ˆğœƒï¼‰</strong></p><ul><li>å®šä¹‰ä¸ºä¸‰ç»´å‘é‡ $ \theta := (k, m, n)^T $ï¼ŒåŒ…å«ï¼š<ul><li>( k )ï¼šä½¿ç”¨çš„æ–‡æ¡£æ•°é‡</li><li>( m )ï¼šä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼ˆdemonstrations/shotsï¼‰çš„æ•°é‡</li><li>( n )ï¼šæœ€å¤§è¿­ä»£/ç”Ÿæˆæ­¥æ•°</li></ul></li></ul></li><li><p><strong>ä¿¡æ¯é‡å‚æ•°ï¼ˆğ‘–ï¼‰</strong></p><ul><li>è¡¡é‡è¾“å…¥å†…å®¹çš„ä¿¡æ¯ä»·å€¼ï¼Œå®šä¹‰ä¸º $ i := (i_{\text{doc}}, i_{\text{shot}}, 0)^T $ï¼š<ul><li>$ i_{\text{doc}} $ï¼š<strong>æ–‡æ¡£çš„ä¿¡æ¯é‡</strong><ul><li>é€šè¿‡â€œæ·»åŠ 1ç¯‡æ–‡æ¡£ vs é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰çš„æ€§èƒ½å·®å¼‚â€è®¡ç®—ã€‚</li></ul></li><li>$ i_{\text{shot}} $ï¼š<strong>ç¤ºä¾‹çš„ä¿¡æ¯é‡</strong><ul><li>é€šè¿‡â€œæ·»åŠ 1ä¸ªç¤ºä¾‹ vs é›¶æ ·æœ¬çš„æ€§èƒ½å·®å¼‚â€è®¡ç®—ã€‚</li></ul></li><li>$ i_{\text{iter}} $ è¢«å¿½ç•¥ï¼ˆè®¾ä¸º0ï¼‰ï¼Œå› å®éªŒä¸­å‘ç°å¢åŠ ç”Ÿæˆæ­¥æ•°å¯¹æ€§èƒ½æ— æ˜¾è‘—æå‡ã€‚</li></ul></li></ul></li><li><p><strong>æ€§èƒ½æ¨¡å‹å…¬å¼</strong></p><p>$P(\theta) \approx \sigma((a + b \odot i)^T \log(\theta) + c)$</p><ul><li><strong>ç¬¦å·è¯´æ˜</strong>ï¼š<ul><li>$ \odot $ï¼šé€å…ƒç´ ç›¸ä¹˜ï¼ˆHadamardç§¯ï¼‰ã€‚</li><li>$ a, b \in \mathbb{R}^3 $ï¼šå¾…ä¼°è®¡å‚æ•°ï¼Œåˆ†åˆ«è¡¨ç¤ºèµ„æºçš„åŸºç¡€æ•ˆåº”å’Œä¸ä¿¡æ¯é‡çš„äº¤äº’æ•ˆåº”ã€‚</li><li>$ c $ï¼šå¸¸æ•°åç½®é¡¹ã€‚ aã€bã€céƒ½æ˜¯åœ¨ç‰¹å®šæƒ…å†µä¸‹æ‹Ÿåˆçš„ã€‚</li><li>$ \log(\theta) $ï¼šå¯¹èµ„æºå‘é‡é€å…ƒç´ å–å¯¹æ•°ã€‚</li><li>$ \sigma $ï¼šSigmoidå‡½æ•°ã€‚</li></ul></li></ul></li></ol><p>$L_{max}$ ç›¸å½“äº $\theta$ çš„é™åˆ¶æ¡ä»¶ï¼›æ‹Ÿåˆå‡ºabcï¼›ç›®æ ‡æ˜¯æœ€å¤§åŒ– $P(\theta)$ ï¼Œå°±å¯ä»¥å¾—åˆ°æœ€å¥½çš„ $\theta$ã€‚</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>Chan's Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>