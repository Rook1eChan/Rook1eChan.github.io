<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>在Autodl中使用LLaMA-Factory进行微调 | 陈</title><meta name=keywords content><meta name=description content='LLaMA-Factory使用教程
一、环境准备
1.1创建虚拟环境
conda create -n lf python==3.11
conda init
然后重开cmd
conda activate lf

1.2下载相关的包
conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia
验证GPU版本的Pytorch是否成功
python -c "import torch; print(torch.cuda.is_available())"

1.3下载llama factory
sudo apt install git
开科学上网
git clone https://github.com/hiyouga/LLaMA-Factory.git

1.4安装依赖
python -m pip install --upgrade pip
pip install -r requirements.txt
pip install -e ".[torch,metrics]"
如果下载有问题，可以尝试清华源
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e ".[torch,metrics]"

1.5清理pip
pip cache purge

二、下载模型
2.1从modelscope下载模型权重文件
pip install modelscope
可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面'><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/llama-factory/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/llama-factory/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/llama-factory/"><meta property="og:site_name" content="陈"><meta property="og:title" content="在Autodl中使用LLaMA-Factory进行微调"><meta property="og:description" content='LLaMA-Factory使用教程
一、环境准备 1.1创建虚拟环境 conda create -n lf python==3.11 conda init 然后重开cmd
conda activate lf 1.2下载相关的包 conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia 验证GPU版本的Pytorch是否成功
python -c "import torch; print(torch.cuda.is_available())" 1.3下载llama factory sudo apt install git 开科学上网
git clone https://github.com/hiyouga/LLaMA-Factory.git 1.4安装依赖 python -m pip install --upgrade pip pip install -r requirements.txt pip install -e ".[torch,metrics]" 如果下载有问题，可以尝试清华源
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e ".[torch,metrics]" 1.5清理pip pip cache purge 二、下载模型 2.1从modelscope下载模型权重文件 pip install modelscope 可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-17T10:07:00+08:00"><meta property="article:modified_time" content="2025-07-17T10:07:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="在Autodl中使用LLaMA-Factory进行微调"><meta name=twitter:description content='LLaMA-Factory使用教程
一、环境准备
1.1创建虚拟环境
conda create -n lf python==3.11
conda init
然后重开cmd
conda activate lf

1.2下载相关的包
conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia
验证GPU版本的Pytorch是否成功
python -c "import torch; print(torch.cuda.is_available())"

1.3下载llama factory
sudo apt install git
开科学上网
git clone https://github.com/hiyouga/LLaMA-Factory.git

1.4安装依赖
python -m pip install --upgrade pip
pip install -r requirements.txt
pip install -e ".[torch,metrics]"
如果下载有问题，可以尝试清华源
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e ".[torch,metrics]"

1.5清理pip
pip cache purge

二、下载模型
2.1从modelscope下载模型权重文件
pip install modelscope
可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"在Autodl中使用LLaMA-Factory进行微调","item":"https://Rook1eChan.github.io/posts/llama-factory/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"在Autodl中使用LLaMA-Factory进行微调","name":"在Autodl中使用LLaMA-Factory进行微调","description":"LLaMA-Factory使用教程\n一、环境准备 1.1创建虚拟环境 conda create -n lf python==3.11 conda init 然后重开cmd\nconda activate lf 1.2下载相关的包 conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia 验证GPU版本的Pytorch是否成功\npython -c \u0026#34;import torch; print(torch.cuda.is_available())\u0026#34; 1.3下载llama factory sudo apt install git 开科学上网\ngit clone https://github.com/hiyouga/LLaMA-Factory.git 1.4安装依赖 python -m pip install --upgrade pip pip install -r requirements.txt pip install -e \u0026#34;.[torch,metrics]\u0026#34; 如果下载有问题，可以尝试清华源\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e \u0026#34;.[torch,metrics]\u0026#34; 1.5清理pip pip cache purge 二、下载模型 2.1从modelscope下载模型权重文件 pip install modelscope 可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面\n","keywords":[],"articleBody":"LLaMA-Factory使用教程\n一、环境准备 1.1创建虚拟环境 conda create -n lf python==3.11 conda init 然后重开cmd\nconda activate lf 1.2下载相关的包 conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia 验证GPU版本的Pytorch是否成功\npython -c \"import torch; print(torch.cuda.is_available())\" 1.3下载llama factory sudo apt install git 开科学上网\ngit clone https://github.com/hiyouga/LLaMA-Factory.git 1.4安装依赖 python -m pip install --upgrade pip pip install -r requirements.txt pip install -e \".[torch,metrics]\" 如果下载有问题，可以尝试清华源\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e \".[torch,metrics]\" 1.5清理pip pip cache purge 二、下载模型 2.1从modelscope下载模型权重文件 pip install modelscope 可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面\nmodelscope download --model Qwen/Qwen3-0.6B 更建议下载到指定文件夹，便于管理\nmodelscope download --model 'Qwen/Qwen3-4B-Base' --local_dir '/root/autodl-tmp/qwen3-4b-base/' 三、运行webui 3.1运行webui llamafactory-cli webui 或者 python src/webui.py 在autodl中可能会报错：To create a public link, set share=True in launch().\n打开interface.py文件，路径为：LLaMA-Factory/src/llamafactory/webui/interface.py\n将run_web_ui的create_ui().queue().launch的share参数修改为True\n如果还不行，可以尝试以下方法：\n从https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64下载文件 将文件重命名为：frpc_linux_amd64_v0.2 然后把文件放在/root/miniconda3/envs/lf/lib/python3.11/site-packages/gradio/，这个方法我没用 或者\ncp /root/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3 /root/miniconda3/envs/lf/lib/python3.11/site-packages/gradio/ 我是将frpc文件拷贝了一份到另一个文件夹，然后给一个权限 chmod +x frpc_linux_amd64_v0.3 3.2SSH 对于autodl来说，要想连接到页面还要经过ssh（如果是本地，直接点击最后的网页链接，应该就可以运行了）\n在autodl控制台——你这台机子——快捷工具——自定义服务——下载autodl ssh tool——打开\n输入该机子的登陆指令和密码\n代理到本地端口：6006\n代理到远程端口：1080\n然后，本地cmd运行\nssh -CNg -L 7860:127.0.0.1:7860 root@connect.bjb1.seetacloud.com -p 54073 54073换为该机子的端口，然后输入机子的密码\n然后访问127.0.0.1:7860即可进入webui\n四、对话页面开启 开启对话页面，来验证下载的模型是否正常\n在cmd输入\nCUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat --model_name_or_path /root/autodl-tmp/qwen3-0.6b --template qwen model_name_or_path：模型下载到默认地址，就填hf官方命名如Qwen3/Qwen3-0.6B；下载到指定地址就填绝对地址如/root/autodl-tmp/qwen3-0.6b\ntemplate：根据模型来选。如果模型出现不停的输出、胡言乱语现象就设置为default再试试\n然后访问 http://127.0.0.1:7860/\n五、微调 上手进行微调\n5.1简单实验 首先进行identity的实验\n使用系统自带的identity.json数据集，对应文件已经在data目录下\n如果使用自己的数据，要放到data下并在dataset_info.json中注册\n我们希望微调后，模型能输出“我是Alex，我由Rook1e制造”\n这里我们把identity.json里改掉，你也可以改成你的名字\nsed -i 's/{{name}}/Alex/g' data/identity.json sed -i 's/{{author}}/Rook1echan/g' data/identity.json 再次打开该文件发现所有的部分都发生了替换\n然后进入webui\n对于indentity这个简单实验的相关参数选择，没写的保持默认：\n参数名 选择 语言 zh 模型名称 用啥选啥 模型路径 下到指定文件夹就写文件夹绝对路径，不然就写hf模型标识符 模型下载源 modelscope 微调方法 lora 选择Train 训练阶段 SFT 数据集 identit 训练轮数 100（差不多到100loss就不下降了） 截断长度 256（数据不是很长） 批处理大小 建议设置不同数量多试验几次，让GPU到80%最好，充分利用也不至于OOM，我这里是4090 24G，设置15 其它参数设置-启用思考 不知道有啥印象，我关掉了，应该会推理快一点 然后点击开始就行了，下面能看到loss的曲线图\n5.2使用sft后的模型 qwen3-4b-base:\nCUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat --model_name_or_path /root/autodl-tmp/qwen3-4b-base --adapter_name_or_path /root/autodl-tmp/LLaMA-Factory/saves/Qwen3-4B-Base/lora/train_2025-09-18-19-41-50 --finetuning_type lora --template default 多了adapter_name_or_path（check point位置）和finetuning_type参数\n注意，由于我们使用的是单词对话模型，所以在chat里每次对话完后要清除历史！不然就会胡言乱语。。。\n经过实验，qwen3需要–template default才不会说胡话\n","wordCount":"210","inLanguage":"en","datePublished":"2025-07-17T10:07:00+08:00","dateModified":"2025-07-17T10:07:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/llama-factory/"},"publisher":{"@type":"Organization","name":"陈","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/icon.png"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="陈 (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>陈</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">在Autodl中使用LLaMA-Factory进行微调</h1><div class=post-meta><span title='2025-07-17 10:07:00 +0800 +0800'>July 17, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%b8%80%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87 aria-label=一、环境准备>一、环境准备</a><ul><li><a href=#11%e5%88%9b%e5%bb%ba%e8%99%9a%e6%8b%9f%e7%8e%af%e5%a2%83 aria-label=1.1创建虚拟环境>1.1创建虚拟环境</a></li><li><a href=#12%e4%b8%8b%e8%bd%bd%e7%9b%b8%e5%85%b3%e7%9a%84%e5%8c%85 aria-label=1.2下载相关的包>1.2下载相关的包</a></li><li><a href=#13%e4%b8%8b%e8%bd%bdllama-factory aria-label="1.3下载llama factory">1.3下载llama factory</a></li><li><a href=#14%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96 aria-label=1.4安装依赖>1.4安装依赖</a></li><li><a href=#15%e6%b8%85%e7%90%86pip aria-label=1.5清理pip>1.5清理pip</a></li></ul></li><li><a href=#%e4%ba%8c%e4%b8%8b%e8%bd%bd%e6%a8%a1%e5%9e%8b aria-label=二、下载模型>二、下载模型</a><ul><li><a href=#21%e4%bb%8emodelscope%e4%b8%8b%e8%bd%bd%e6%a8%a1%e5%9e%8b%e6%9d%83%e9%87%8d%e6%96%87%e4%bb%b6 aria-label=2.1从modelscope下载模型权重文件>2.1从modelscope下载模型权重文件</a></li></ul></li><li><a href=#%e4%b8%89%e8%bf%90%e8%a1%8cwebui aria-label=三、运行webui>三、运行webui</a><ul><li><a href=#31%e8%bf%90%e8%a1%8cwebui aria-label=3.1运行webui>3.1运行webui</a></li><li><a href=#32ssh aria-label=3.2SSH>3.2SSH</a></li></ul></li><li><a href=#%e5%9b%9b%e5%af%b9%e8%af%9d%e9%a1%b5%e9%9d%a2%e5%bc%80%e5%90%af aria-label=四、对话页面开启>四、对话页面开启</a></li><li><a href=#%e4%ba%94%e5%be%ae%e8%b0%83 aria-label=五、微调>五、微调</a><ul><li><a href=#51%e7%ae%80%e5%8d%95%e5%ae%9e%e9%aa%8c aria-label=5.1简单实验>5.1简单实验</a></li><li><a href=#52%e4%bd%bf%e7%94%a8sft%e5%90%8e%e7%9a%84%e6%a8%a1%e5%9e%8b aria-label=5.2使用sft后的模型>5.2使用sft后的模型</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>LLaMA-Factory使用教程</p><h1 id=一环境准备>一、环境准备<a hidden class=anchor aria-hidden=true href=#一环境准备>#</a></h1><h2 id=11创建虚拟环境>1.1创建虚拟环境<a hidden class=anchor aria-hidden=true href=#11创建虚拟环境>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda create -n lf <span class=nv>python</span><span class=o>==</span>3.11
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda init
</span></span></code></pre></div><p>然后重开cmd</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda activate lf
</span></span></code></pre></div><br><h2 id=12下载相关的包>1.2下载相关的包<a hidden class=anchor aria-hidden=true href=#12下载相关的包>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda install <span class=nv>pytorch</span><span class=o>==</span>2.4.0 <span class=nv>torchvision</span><span class=o>==</span>0.19.0 <span class=nv>torchaudio</span><span class=o>==</span>2.4.0 pytorch-cuda<span class=o>=</span>12.1 -c pytorch -c nvidia
</span></span></code></pre></div><p>验证GPU版本的Pytorch是否成功</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python -c <span class=s2>&#34;import torch; print(torch.cuda.is_available())&#34;</span>
</span></span></code></pre></div><br><h2 id=13下载llama-factory>1.3下载llama factory<a hidden class=anchor aria-hidden=true href=#13下载llama-factory>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt install git
</span></span></code></pre></div><p>开科学上网</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/hiyouga/LLaMA-Factory.git
</span></span></code></pre></div><br><h2 id=14安装依赖>1.4安装依赖<a hidden class=anchor aria-hidden=true href=#14安装依赖>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python -m pip install --upgrade pip
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install -r requirements.txt
</span></span><span class=line><span class=cl>pip install -e <span class=s2>&#34;.[torch,metrics]&#34;</span>
</span></span></code></pre></div><p>如果下载有问题，可以尝试清华源</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e <span class=s2>&#34;.[torch,metrics]&#34;</span>
</span></span></code></pre></div><br><h2 id=15清理pip>1.5清理pip<a hidden class=anchor aria-hidden=true href=#15清理pip>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip cache purge
</span></span></code></pre></div><br><h1 id=二下载模型>二、下载模型<a hidden class=anchor aria-hidden=true href=#二下载模型>#</a></h1><h2 id=21从modelscope下载模型权重文件>2.1从modelscope下载模型权重文件<a hidden class=anchor aria-hidden=true href=#21从modelscope下载模型权重文件>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install modelscope
</span></span></code></pre></div><p>可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>modelscope download --model Qwen/Qwen3-0.6B
</span></span></code></pre></div><p>更建议下载到指定文件夹，便于管理</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>modelscope download --model <span class=s1>&#39;Qwen/Qwen3-4B-Base&#39;</span> --local_dir <span class=s1>&#39;/root/autodl-tmp/qwen3-4b-base/&#39;</span>
</span></span></code></pre></div><h1 id=三运行webui>三、运行webui<a hidden class=anchor aria-hidden=true href=#三运行webui>#</a></h1><h2 id=31运行webui>3.1运行webui<a hidden class=anchor aria-hidden=true href=#31运行webui>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llamafactory-cli webui
</span></span><span class=line><span class=cl>或者
</span></span><span class=line><span class=cl>python src/webui.py
</span></span></code></pre></div><p>在autodl中可能会报错：<code>To create a public link, set share=True in launch().</code></p><p>打开interface.py文件，路径为：LLaMA-Factory/src/llamafactory/webui/interface.py</p><p>将run_web_ui的create_ui().queue().launch的share参数修改为True</p><p>如果还不行，可以尝试以下方法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>从https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64下载文件
</span></span><span class=line><span class=cl>将文件重命名为：frpc_linux_amd64_v0.2
</span></span><span class=line><span class=cl>然后把文件放在/root/miniconda3/envs/lf/lib/python3.11/site-packages/gradio/，这个方法我没用
</span></span></code></pre></div><p>或者</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cp /root/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3 /root/miniconda3/envs/lf/lib/python3.11/site-packages/gradio/
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>我是将frpc文件拷贝了一份到另一个文件夹，然后给一个权限
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>chmod +x frpc_linux_amd64_v0.3
</span></span></code></pre></div><br><h2 id=32ssh>3.2SSH<a hidden class=anchor aria-hidden=true href=#32ssh>#</a></h2><p>对于autodl来说，要想连接到页面还要经过ssh（如果是本地，直接点击最后的网页链接，应该就可以运行了）</p><p>在autodl控制台——你这台机子——快捷工具——自定义服务——下载autodl ssh tool——打开</p><p>输入该机子的登陆指令和密码</p><p>代理到本地端口：6006</p><p>代理到远程端口：1080</p><p>然后，本地cmd运行</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh -CNg -L 7860:127.0.0.1:7860 root@connect.bjb1.seetacloud.com -p <span class=m>54073</span>
</span></span></code></pre></div><p>54073换为该机子的端口，然后输入机子的密码</p><p>然后访问127.0.0.1:7860即可进入webui</p><br><h1 id=四对话页面开启>四、对话页面开启<a hidden class=anchor aria-hidden=true href=#四对话页面开启>#</a></h1><p>开启对话页面，来验证下载的模型是否正常</p><p>在cmd输入</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span><span class=m>0</span> llamafactory-cli webchat --model_name_or_path /root/autodl-tmp/qwen3-0.6b --template qwen
</span></span></code></pre></div><p>model_name_or_path：模型下载到默认地址，就填hf官方命名如<code>Qwen3/Qwen3-0.6B</code>；下载到指定地址就填绝对地址如<code>/root/autodl-tmp/qwen3-0.6b</code></p><p>template：根据模型来选。如果模型出现不停的输出、胡言乱语现象就设置为default再试试</p><p>然后访问 http://127.0.0.1:7860/</p><br><h1 id=五微调>五、微调<a hidden class=anchor aria-hidden=true href=#五微调>#</a></h1><p>上手进行微调</p><h2 id=51简单实验>5.1简单实验<a hidden class=anchor aria-hidden=true href=#51简单实验>#</a></h2><p>首先进行identity的实验</p><p>使用系统自带的identity.json数据集，对应文件已经在data目录下</p><p>如果使用自己的数据，要放到data下并在dataset_info.json中注册</p><p>我们希望微调后，模型能输出“我是Alex，我由Rook1e制造”</p><p>这里我们把identity.json里改掉，你也可以改成你的名字</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>sed -i &#39;s/{{name}}/Alex/g&#39;  data/identity.json 
</span></span><span class=line><span class=cl>sed -i &#39;s/{{author}}/Rook1echan/g&#39;  data/identity.json 
</span></span></code></pre></div><p>再次打开该文件发现所有的部分都发生了替换</p><p>然后进入webui</p><p>对于indentity这个简单实验的相关参数选择，没写的保持默认：</p><table><thead><tr><th>参数名</th><th>选择</th></tr></thead><tbody><tr><td>语言</td><td>zh</td></tr><tr><td>模型名称</td><td>用啥选啥</td></tr><tr><td>模型路径</td><td>下到指定文件夹就写文件夹绝对路径，不然就写hf模型标识符</td></tr><tr><td>模型下载源</td><td>modelscope</td></tr><tr><td>微调方法</td><td>lora</td></tr><tr><td>选择Train</td><td></td></tr><tr><td>训练阶段</td><td>SFT</td></tr><tr><td>数据集</td><td>identit</td></tr><tr><td>训练轮数</td><td>100（差不多到100loss就不下降了）</td></tr><tr><td>截断长度</td><td>256（数据不是很长）</td></tr><tr><td>批处理大小</td><td>建议设置不同数量多试验几次，让GPU到80%最好，充分利用也不至于OOM，我这里是4090 24G，设置15</td></tr><tr><td>其它参数设置-启用思考</td><td>不知道有啥印象，我关掉了，应该会推理快一点</td></tr></tbody></table><p>然后点击开始就行了，下面能看到loss的曲线图</p><br><h2 id=52使用sft后的模型>5.2使用sft后的模型<a hidden class=anchor aria-hidden=true href=#52使用sft后的模型>#</a></h2><p>qwen3-4b-base:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-BASH data-lang=BASH><span class=line><span class=cl><span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span><span class=m>0</span> llamafactory-cli webchat --model_name_or_path /root/autodl-tmp/qwen3-4b-base --adapter_name_or_path /root/autodl-tmp/LLaMA-Factory/saves/Qwen3-4B-Base/lora/train_2025-09-18-19-41-50 --finetuning_type lora --template default
</span></span></code></pre></div><p>多了adapter_name_or_path（check point位置）和finetuning_type参数</p><p>注意，由于我们使用的是单词对话模型，所以在chat里每次对话完后要清除历史！不然就会胡言乱语。。。</p><p>经过实验，qwen3需要&ndash;template default才不会说胡话</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>陈</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>