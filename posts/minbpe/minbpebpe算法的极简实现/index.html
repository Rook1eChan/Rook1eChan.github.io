<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>minbpeï¼šBPEç®—æ³•çš„æç®€å®ç° | é™ˆ</title><meta name=keywords content><meta name=description content='minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°
github åœ°å€ï¼škarpathy/minbpe

1.BPE ç®—æ³•
BPE(Byte Pair Encoding) æ˜¯å¤§æ¨¡å‹çš„ tokenizer å¸¸ç”¨çš„ç®—æ³•ã€‚å®ƒå¯¹è¾“å…¥æ–‡æœ¬çš„å­—èŠ‚è¿›è¡Œç¼–ç ã€‚
è¯¥ç®—æ³•å›  GPT-2 çš„è®ºæ–‡å’Œä»£ç è€Œè¢«å¹¿æ³›ä½¿ç”¨äº LLMã€‚Sennrich et al. 2015è¢«è®¤ä¸ºæ˜¯ BPE åœ¨ NLP åº”ç”¨ä¸­çš„åŸå§‹å‚è€ƒã€‚
ç®€å•æ¥è¯´ï¼Œbpe æŠŠæ–‡æœ¬çœ‹ä½œ utf-8 ç¼–ç çš„å­—èŠ‚ï¼Œç„¶åå°†å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç›¸é‚»å­—èŠ‚åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ç¼–ç ã€‚å¦‚æ­¤åå¤æ“ä½œã€‚

2.minbpe ç®€ä»‹
2.1quickstart
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
text = "aaabdaaabac"

# è®­ç»ƒ
tokenizer.train(text, 256 + 3)  # 256tokens, 3merges

# ç¼–ç 
print(tokenizer.encode(text))

# è§£ç 
print(tokenizer.decode([258, 100, 258, 97, 99]))

# ä¿å­˜
tokenizer.save("toy")
# writes two files: toy.model (for loading) and toy.vocab (for viewing)
è‹±è¯­å­—æ¯ä¸€ä¸ªå­—æ¯å¯¹åº”ä¸€ä¸ªå­—èŠ‚ã€‚å¯¹äº"aaabdaaabac"ï¼Œå…ˆè®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åé€‰æ‹©æ¬¡æ•°æœ€å¤šçš„è¿›è¡Œåˆå¹¶ï¼ˆâ€œaaâ€ï¼Œ4æ¬¡ï¼‰ã€‚
â€œaâ€â€œaâ€åˆå¹¶ä¸ºâ€œaaâ€ï¼Œç¼–ç ä¸º256ã€‚
ç„¶åå†è®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œå†åˆå¹¶ã€‚
åœ¨ toy.vocab ä¸­å¯ä»¥çœ‹åˆ°æ‰€æœ‰å­—ç¬¦åŠå¯¹åº”çš„ç¼–ç ã€‚
toy.vocab

......
[a][a] -> [aa] 256
[aa][a] -> [aaa] 257
[aaa][b] -> [aaab] 258

2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ
# 1.è¯æ˜RegexTokenizerä¸GPT-4çš„åˆ†è¯å™¨æ€§èƒ½ä¸€è‡´
text = "hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰"

# pip install tiktoken
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

# 2.æ ‡è®°ç‰¹æ®Štoken
text = "<|endoftext|>hello world"

print(enc.encode(text, allowed_special="all"))
# [100257, 15339, 1917]

# ours
print(tokenizer.encode(text, allowed_special="all"))
# [100257, 15339, 1917]
è°ƒç”¨ encode æ—¶å¿…é¡»æ˜¾ç¤ºå£°æ˜å¤„ç†ç‰¹æ®Šæ ‡è®°ã€‚allowed_special å‚æ•°å¯ä»¥è®¾ç½®ä¸º"all"ã€&ldquo;none"æˆ–ä¸€ä¸ªç‰¹æ®Štokenåˆ—è¡¨ã€‚'><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/"><meta property="og:site_name" content="é™ˆ"><meta property="og:title" content="minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°"><meta property="og:description" content='minbpeï¼šBPEç®—æ³•çš„æç®€å®ç° github åœ°å€ï¼škarpathy/minbpe
1.BPE ç®—æ³• BPE(Byte Pair Encoding) æ˜¯å¤§æ¨¡å‹çš„ tokenizer å¸¸ç”¨çš„ç®—æ³•ã€‚å®ƒå¯¹è¾“å…¥æ–‡æœ¬çš„å­—èŠ‚è¿›è¡Œç¼–ç ã€‚
è¯¥ç®—æ³•å›  GPT-2 çš„è®ºæ–‡å’Œä»£ç è€Œè¢«å¹¿æ³›ä½¿ç”¨äº LLMã€‚Sennrich et al. 2015è¢«è®¤ä¸ºæ˜¯ BPE åœ¨ NLP åº”ç”¨ä¸­çš„åŸå§‹å‚è€ƒã€‚
ç®€å•æ¥è¯´ï¼Œbpe æŠŠæ–‡æœ¬çœ‹ä½œ utf-8 ç¼–ç çš„å­—èŠ‚ï¼Œç„¶åå°†å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç›¸é‚»å­—èŠ‚åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ç¼–ç ã€‚å¦‚æ­¤åå¤æ“ä½œã€‚
2.minbpe ç®€ä»‹ 2.1quickstart from minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = "aaabdaaabac" # è®­ç»ƒ tokenizer.train(text, 256 + 3) # 256tokens, 3merges # ç¼–ç  print(tokenizer.encode(text)) # è§£ç  print(tokenizer.decode([258, 100, 258, 97, 99])) # ä¿å­˜ tokenizer.save("toy") # writes two files: toy.model (for loading) and toy.vocab (for viewing) è‹±è¯­å­—æ¯ä¸€ä¸ªå­—æ¯å¯¹åº”ä¸€ä¸ªå­—èŠ‚ã€‚å¯¹äº"aaabdaaabac"ï¼Œå…ˆè®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åé€‰æ‹©æ¬¡æ•°æœ€å¤šçš„è¿›è¡Œåˆå¹¶ï¼ˆâ€œaaâ€ï¼Œ4æ¬¡ï¼‰ã€‚
â€œaâ€â€œaâ€åˆå¹¶ä¸ºâ€œaaâ€ï¼Œç¼–ç ä¸º256ã€‚
ç„¶åå†è®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œå†åˆå¹¶ã€‚
åœ¨ toy.vocab ä¸­å¯ä»¥çœ‹åˆ°æ‰€æœ‰å­—ç¬¦åŠå¯¹åº”çš„ç¼–ç ã€‚
toy.vocab ...... [a][a] -> [aa] 256 [aa][a] -> [aaa] 257 [aaa][b] -> [aaab] 258 2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ # 1.è¯æ˜RegexTokenizerä¸GPT-4çš„åˆ†è¯å™¨æ€§èƒ½ä¸€è‡´ text = "hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰" # pip install tiktoken import tiktoken enc = tiktoken.get_encoding("cl100k_base") print(enc.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] from minbpe import GPT4Tokenizer tokenizer = GPT4Tokenizer() print(tokenizer.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] # 2.æ ‡è®°ç‰¹æ®Štoken text = "<|endoftext|>hello world" print(enc.encode(text, allowed_special="all")) # [100257, 15339, 1917] # ours print(tokenizer.encode(text, allowed_special="all")) # [100257, 15339, 1917] è°ƒç”¨ encode æ—¶å¿…é¡»æ˜¾ç¤ºå£°æ˜å¤„ç†ç‰¹æ®Šæ ‡è®°ã€‚allowed_special å‚æ•°å¯ä»¥è®¾ç½®ä¸º"all"ã€â€œnone"æˆ–ä¸€ä¸ªç‰¹æ®Štokenåˆ—è¡¨ã€‚'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-23T14:46:23+08:00"><meta property="article:modified_time" content="2025-09-23T14:46:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°"><meta name=twitter:description content='minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°
github åœ°å€ï¼škarpathy/minbpe

1.BPE ç®—æ³•
BPE(Byte Pair Encoding) æ˜¯å¤§æ¨¡å‹çš„ tokenizer å¸¸ç”¨çš„ç®—æ³•ã€‚å®ƒå¯¹è¾“å…¥æ–‡æœ¬çš„å­—èŠ‚è¿›è¡Œç¼–ç ã€‚
è¯¥ç®—æ³•å›  GPT-2 çš„è®ºæ–‡å’Œä»£ç è€Œè¢«å¹¿æ³›ä½¿ç”¨äº LLMã€‚Sennrich et al. 2015è¢«è®¤ä¸ºæ˜¯ BPE åœ¨ NLP åº”ç”¨ä¸­çš„åŸå§‹å‚è€ƒã€‚
ç®€å•æ¥è¯´ï¼Œbpe æŠŠæ–‡æœ¬çœ‹ä½œ utf-8 ç¼–ç çš„å­—èŠ‚ï¼Œç„¶åå°†å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç›¸é‚»å­—èŠ‚åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ç¼–ç ã€‚å¦‚æ­¤åå¤æ“ä½œã€‚

2.minbpe ç®€ä»‹
2.1quickstart
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
text = "aaabdaaabac"

# è®­ç»ƒ
tokenizer.train(text, 256 + 3)  # 256tokens, 3merges

# ç¼–ç 
print(tokenizer.encode(text))

# è§£ç 
print(tokenizer.decode([258, 100, 258, 97, 99]))

# ä¿å­˜
tokenizer.save("toy")
# writes two files: toy.model (for loading) and toy.vocab (for viewing)
è‹±è¯­å­—æ¯ä¸€ä¸ªå­—æ¯å¯¹åº”ä¸€ä¸ªå­—èŠ‚ã€‚å¯¹äº"aaabdaaabac"ï¼Œå…ˆè®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åé€‰æ‹©æ¬¡æ•°æœ€å¤šçš„è¿›è¡Œåˆå¹¶ï¼ˆâ€œaaâ€ï¼Œ4æ¬¡ï¼‰ã€‚
â€œaâ€â€œaâ€åˆå¹¶ä¸ºâ€œaaâ€ï¼Œç¼–ç ä¸º256ã€‚
ç„¶åå†è®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œå†åˆå¹¶ã€‚
åœ¨ toy.vocab ä¸­å¯ä»¥çœ‹åˆ°æ‰€æœ‰å­—ç¬¦åŠå¯¹åº”çš„ç¼–ç ã€‚
toy.vocab

......
[a][a] -> [aa] 256
[aa][a] -> [aaa] 257
[aaa][b] -> [aaab] 258

2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ
# 1.è¯æ˜RegexTokenizerä¸GPT-4çš„åˆ†è¯å™¨æ€§èƒ½ä¸€è‡´
text = "hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰"

# pip install tiktoken
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

# 2.æ ‡è®°ç‰¹æ®Štoken
text = "<|endoftext|>hello world"

print(enc.encode(text, allowed_special="all"))
# [100257, 15339, 1917]

# ours
print(tokenizer.encode(text, allowed_special="all"))
# [100257, 15339, 1917]
è°ƒç”¨ encode æ—¶å¿…é¡»æ˜¾ç¤ºå£°æ˜å¤„ç†ç‰¹æ®Šæ ‡è®°ã€‚allowed_special å‚æ•°å¯ä»¥è®¾ç½®ä¸º"all"ã€&ldquo;none"æˆ–ä¸€ä¸ªç‰¹æ®Štokenåˆ—è¡¨ã€‚'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°","item":"https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°","name":"minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°","description":"minbpeï¼šBPEç®—æ³•çš„æç®€å®ç° github åœ°å€ï¼škarpathy/minbpe\n1.BPE ç®—æ³• BPE(Byte Pair Encoding) æ˜¯å¤§æ¨¡å‹çš„ tokenizer å¸¸ç”¨çš„ç®—æ³•ã€‚å®ƒå¯¹è¾“å…¥æ–‡æœ¬çš„å­—èŠ‚è¿›è¡Œç¼–ç ã€‚\nè¯¥ç®—æ³•å›  GPT-2 çš„è®ºæ–‡å’Œä»£ç è€Œè¢«å¹¿æ³›ä½¿ç”¨äº LLMã€‚Sennrich et al. 2015è¢«è®¤ä¸ºæ˜¯ BPE åœ¨ NLP åº”ç”¨ä¸­çš„åŸå§‹å‚è€ƒã€‚\nç®€å•æ¥è¯´ï¼Œbpe æŠŠæ–‡æœ¬çœ‹ä½œ utf-8 ç¼–ç çš„å­—èŠ‚ï¼Œç„¶åå°†å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç›¸é‚»å­—èŠ‚åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ç¼–ç ã€‚å¦‚æ­¤åå¤æ“ä½œã€‚\n2.minbpe ç®€ä»‹ 2.1quickstart from minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = \u0026#34;aaabdaaabac\u0026#34; # è®­ç»ƒ tokenizer.train(text, 256 + 3) # 256tokens, 3merges # ç¼–ç  print(tokenizer.encode(text)) # è§£ç  print(tokenizer.decode([258, 100, 258, 97, 99])) # ä¿å­˜ tokenizer.save(\u0026#34;toy\u0026#34;) # writes two files: toy.model (for loading) and toy.vocab (for viewing) è‹±è¯­å­—æ¯ä¸€ä¸ªå­—æ¯å¯¹åº”ä¸€ä¸ªå­—èŠ‚ã€‚å¯¹äº\u0026quot;aaabdaaabac\u0026quot;ï¼Œå…ˆè®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åé€‰æ‹©æ¬¡æ•°æœ€å¤šçš„è¿›è¡Œåˆå¹¶ï¼ˆâ€œaaâ€ï¼Œ4æ¬¡ï¼‰ã€‚\nâ€œaâ€â€œaâ€åˆå¹¶ä¸ºâ€œaaâ€ï¼Œç¼–ç ä¸º256ã€‚\nç„¶åå†è®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œå†åˆå¹¶ã€‚\nåœ¨ toy.vocab ä¸­å¯ä»¥çœ‹åˆ°æ‰€æœ‰å­—ç¬¦åŠå¯¹åº”çš„ç¼–ç ã€‚\ntoy.vocab ...... [a][a] -\u0026gt; [aa] 256 [aa][a] -\u0026gt; [aaa] 257 [aaa][b] -\u0026gt; [aaab] 258 2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ # 1.è¯æ˜RegexTokenizerä¸GPT-4çš„åˆ†è¯å™¨æ€§èƒ½ä¸€è‡´ text = \u0026#34;hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰\u0026#34; # pip install tiktoken import tiktoken enc = tiktoken.get_encoding(\u0026#34;cl100k_base\u0026#34;) print(enc.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] from minbpe import GPT4Tokenizer tokenizer = GPT4Tokenizer() print(tokenizer.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] # 2.æ ‡è®°ç‰¹æ®Štoken text = \u0026#34;\u0026lt;|endoftext|\u0026gt;hello world\u0026#34; print(enc.encode(text, allowed_special=\u0026#34;all\u0026#34;)) # [100257, 15339, 1917] # ours print(tokenizer.encode(text, allowed_special=\u0026#34;all\u0026#34;)) # [100257, 15339, 1917] è°ƒç”¨ encode æ—¶å¿…é¡»æ˜¾ç¤ºå£°æ˜å¤„ç†ç‰¹æ®Šæ ‡è®°ã€‚allowed_special å‚æ•°å¯ä»¥è®¾ç½®ä¸º\u0026quot;all\u0026quot;ã€\u0026ldquo;none\u0026quot;æˆ–ä¸€ä¸ªç‰¹æ®Štokenåˆ—è¡¨ã€‚\n","keywords":[],"articleBody":"minbpeï¼šBPEç®—æ³•çš„æç®€å®ç° github åœ°å€ï¼škarpathy/minbpe\n1.BPE ç®—æ³• BPE(Byte Pair Encoding) æ˜¯å¤§æ¨¡å‹çš„ tokenizer å¸¸ç”¨çš„ç®—æ³•ã€‚å®ƒå¯¹è¾“å…¥æ–‡æœ¬çš„å­—èŠ‚è¿›è¡Œç¼–ç ã€‚\nè¯¥ç®—æ³•å›  GPT-2 çš„è®ºæ–‡å’Œä»£ç è€Œè¢«å¹¿æ³›ä½¿ç”¨äº LLMã€‚Sennrich et al. 2015è¢«è®¤ä¸ºæ˜¯ BPE åœ¨ NLP åº”ç”¨ä¸­çš„åŸå§‹å‚è€ƒã€‚\nç®€å•æ¥è¯´ï¼Œbpe æŠŠæ–‡æœ¬çœ‹ä½œ utf-8 ç¼–ç çš„å­—èŠ‚ï¼Œç„¶åå°†å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç›¸é‚»å­—èŠ‚åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ç¼–ç ã€‚å¦‚æ­¤åå¤æ“ä½œã€‚\n2.minbpe ç®€ä»‹ 2.1quickstart from minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = \"aaabdaaabac\" # è®­ç»ƒ tokenizer.train(text, 256 + 3) # 256tokens, 3merges # ç¼–ç  print(tokenizer.encode(text)) # è§£ç  print(tokenizer.decode([258, 100, 258, 97, 99])) # ä¿å­˜ tokenizer.save(\"toy\") # writes two files: toy.model (for loading) and toy.vocab (for viewing) è‹±è¯­å­—æ¯ä¸€ä¸ªå­—æ¯å¯¹åº”ä¸€ä¸ªå­—èŠ‚ã€‚å¯¹äº\"aaabdaaabac\"ï¼Œå…ˆè®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åé€‰æ‹©æ¬¡æ•°æœ€å¤šçš„è¿›è¡Œåˆå¹¶ï¼ˆâ€œaaâ€ï¼Œ4æ¬¡ï¼‰ã€‚\nâ€œaâ€â€œaâ€åˆå¹¶ä¸ºâ€œaaâ€ï¼Œç¼–ç ä¸º256ã€‚\nç„¶åå†è®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œå†åˆå¹¶ã€‚\nåœ¨ toy.vocab ä¸­å¯ä»¥çœ‹åˆ°æ‰€æœ‰å­—ç¬¦åŠå¯¹åº”çš„ç¼–ç ã€‚\ntoy.vocab ...... [a][a] -\u003e [aa] 256 [aa][a] -\u003e [aaa] 257 [aaa][b] -\u003e [aaab] 258 2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ # 1.è¯æ˜RegexTokenizerä¸GPT-4çš„åˆ†è¯å™¨æ€§èƒ½ä¸€è‡´ text = \"hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰\" # pip install tiktoken import tiktoken enc = tiktoken.get_encoding(\"cl100k_base\") print(enc.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] from minbpe import GPT4Tokenizer tokenizer = GPT4Tokenizer() print(tokenizer.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] # 2.æ ‡è®°ç‰¹æ®Štoken text = \"\u003c|endoftext|\u003ehello world\" print(enc.encode(text, allowed_special=\"all\")) # [100257, 15339, 1917] # ours print(tokenizer.encode(text, allowed_special=\"all\")) # [100257, 15339, 1917] è°ƒç”¨ encode æ—¶å¿…é¡»æ˜¾ç¤ºå£°æ˜å¤„ç†ç‰¹æ®Šæ ‡è®°ã€‚allowed_special å‚æ•°å¯ä»¥è®¾ç½®ä¸º\"all\"ã€â€œnone\"æˆ–ä¸€ä¸ªç‰¹æ®Štokenåˆ—è¡¨ã€‚\né˜²æ­¢æ¶æ„æ³¨å…¥ é£é™©ç‰¹æ®Šä»¤ç‰Œï¼ˆå¦‚\u003c|endoftext|\u003eã€\u003c|user|\u003eç­‰ï¼‰é€šå¸¸åœ¨æ¨¡å‹ä¸­æœ‰ç‰¹æ®Šå«ä¹‰ï¼ˆå¦‚ç»ˆæ­¢åºåˆ—ã€è§’è‰²åŒºåˆ†ï¼‰ã€‚å¦‚æœåˆ†è¯å™¨é»˜è®¤è§£æç”¨æˆ·è¾“å…¥ä¸­çš„ç‰¹æ®Šä»¤ç‰Œï¼Œæ”»å‡»è€…å¯èƒ½ä¼šåœ¨ç”¨æˆ·è¾“å…¥ä¸­æ¶æ„æ’å…¥è¿™äº›ä»¤ç‰Œï¼Œè¯±å¯¼æ¨¡å‹æ‰§è¡Œéé¢„æœŸè¡Œä¸ºï¼ˆä¾‹å¦‚æå‰ç»ˆæ­¢ç”Ÿæˆã€åˆ‡æ¢è§’è‰²ï¼‰ã€‚æ˜¾å¼å£°æ˜allowed_specialå‚æ•°ï¼ˆå¦‚é™åˆ¶ä¸º \"none\" æˆ–æŒ‡å®šå®‰å…¨åˆ—è¡¨ï¼‰ï¼Œå¯ä»¥é¿å…å°†ç”¨æˆ·å¯æ§æ•°æ®ä¸­çš„ç‰¹æ®Šå­—ç¬¦è¯¯è§£æä¸ºåŠŸèƒ½æ€§ä»¤ç‰Œï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§è¾“å…¥éªŒè¯æœºåˆ¶ã€‚ é¿å…æ„å¤–çš„è¯­ä¹‰å¹²æ‰° å³ä½¿æ²¡æœ‰æ¶æ„æ”»å‡»ï¼Œç”¨æˆ·è¾“å…¥ä¸­ä¹Ÿå¯èƒ½åŒ…å«ä¸ç‰¹æ®Šä»¤ç‰Œç›¸åŒçš„å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚è‡ªç„¶æ–‡æœ¬ä¸­æ°å¥½å‡ºç°\u003c|end|\u003eï¼‰ã€‚å¦‚æœåˆ†è¯å™¨é»˜è®¤è§£æè¿™äº›å­—ç¬¦ä¸²ä¸ºç‰¹æ®Šä»¤ç‰Œï¼Œä¼šç ´ååŸå§‹æ–‡æœ¬çš„è¯­ä¹‰ï¼Œå¯¼è‡´æ¨¡å‹å¤„ç†å‡ºé”™ï¼ˆå¦‚é”™è¯¯æˆªæ–­ã€è§’è‰²æ··æ·†ï¼‰ã€‚é€šè¿‡æ˜¾å¼æ§åˆ¶ï¼Œå¼€å‘è€…å¯ä»¥ç¡®ä¿åªæœ‰é¢„æœŸçš„ç‰¹æ®Šä»¤ç‰Œè¢«è§£æï¼Œå…¶ä»–ç±»ä¼¼å­—ç¬¦ä¸²ä»…ä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†ã€‚ 3.è®­ç»ƒ minbpe å¯ä»¥åœ¨è¯­æ–™ä¸Šè®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ã€‚\n# ä»¿ç…§openaiï¼Œä½¿ç”¨æ­£åˆ™æ–¹æ³•æ¥æ‹†åˆ†æ–‡æœ¬ from minbpe import RegexTokenizer tokenizer = RegexTokenizer() tokenizer.train(very_long_training_string, vocab_size=32768) tokenizer.encode(\"hello world\") # string -\u003e tokens tokenizer.decode([1000, 2000, 3000]) # tokens -\u003e string tokenizer.save(\"tok32k\") # writes tok32k.model and tok32k.vocab tokenizer.load(\"tok32k.model\") # loads the model back from disk å¦‚æœè¦æ·»åŠ special tokensï¼š\n# å‰256ä¸ºåŸå§‹å­—èŠ‚tokenï¼Œæ¥ä¸‹æ¥32768-256ä¸ºåˆå¹¶çš„tokenï¼Œåé¢çš„32768æ˜¯ç‰¹æ®Štoken from minbpe import RegexTokenizer tokenizer = RegexTokenizer() tokenizer.train(very_long_training_string, vocab_size=32768) tokenizer.register_special_tokens({\"\u003c|endoftext|\u003e\": 32768}) tokenizer.encode(\"\u003c|endoftext|\u003ehello world\", allowed_special=\"all\") 3.é¡¹ç›®è§£æ 3.1é¡¹ç›®ç»“æ„ é¡¹ç›®ä¸­æœ‰ä¸¤ä¸ªåˆ†è¯å™¨ï¼Œå®ƒä»¬éƒ½å¯ä»¥æ‰§è¡Œåˆ†è¯å™¨çš„ä¸‰ä¸ªä¸»è¦åŠŸèƒ½ï¼š1ï¼‰åœ¨ç»™å®šæ–‡æœ¬ä¸Šè®­ç»ƒåˆ†è¯å™¨è¯æ±‡å¹¶æ‰§è¡Œåˆå¹¶ï¼Œ2ï¼‰å°†æ–‡æœ¬ç¼–ç ä¸ºtokensï¼Œ3ï¼‰å°†tokensè§£ç ä¸ºæ–‡æœ¬ã€‚ä»“åº“çš„æ–‡ä»¶å¦‚ä¸‹ï¼š\nminbpe/base.py: å®ç°äº† Tokenizer ç±»ï¼Œå®ƒæ˜¯åŸºç¡€ç±»ã€‚å®ƒåŒ…å« train ã€ encode å’Œ decode ã€save/loadåŠŸèƒ½ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è§çš„å·¥å…·å‡½æ•°ã€‚è¿™ä¸ªç±»ä¸æ˜¯ç›´æ¥ä½¿ç”¨çš„ï¼Œè€Œæ˜¯ç”¨æ¥ç»§æ‰¿çš„ã€‚ minbpe/basic.py: å®ç°äº† BasicTokenizer ï¼Œè¿™æ˜¯ BPE ç®—æ³•æœ€ç®€å•çš„å®ç°ï¼Œç›´æ¥åœ¨æ–‡æœ¬ä¸Šè¿è¡Œã€‚ minbpe/regex.py: å®ç°äº† RegexTokenizer ï¼Œé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è¿›ä¸€æ­¥åˆ†å‰²è¾“å…¥æ–‡æœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢„å¤„ç†é˜¶æ®µï¼Œåœ¨åˆ†è¯ä¹‹å‰æŒ‰ç±»åˆ«åˆ†å‰²è¾“å…¥æ–‡æœ¬ã€‚è¿™ç¡®ä¿äº†ä¸ä¼šåœ¨ç±»åˆ«è¾¹ç•Œå¤„è¿›è¡Œåˆå¹¶ã€‚è¿™ä¸ªæ–¹æ³•åœ¨ GPT-2 è®ºæ–‡ä¸­è¢«å¼•å…¥ï¼Œå¹¶ä¸”æˆªè‡³ GPT-4 ä»åœ¨ä½¿ç”¨ã€‚è¿™ä¸ªç±»è¿˜å¤„ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚ minbpe/gpt4.py: å®ç°äº† GPT4Tokenizer ã€‚è¿™ä¸ªç±»æ˜¯ RegexTokenizer çš„ä¸€ä¸ªè½»é‡çº§åŒ…è£…å™¨ï¼Œç²¾ç¡®åœ°é‡ç°äº† tiktoken åº“ä¸­ GPT-4 çš„åˆ†è¯ã€‚ æœ€åï¼Œè„šæœ¬ train.py åœ¨è¾“å…¥æ–‡æœ¬ tests/taylorswift.txt ä¸Šè®­ç»ƒäº†ä¸¤ä¸ªä¸»è¦çš„åˆ†è¯å™¨ï¼Œå¹¶å°†è¯æ±‡è¡¨ä¿å­˜åˆ°ç£ç›˜ä»¥ä¾›å¯è§†åŒ–ã€‚\n3.2ä»£ç åˆ†æ class ä»£ç åˆ†æ(https://zerolovesea.github.io/2024/03/09/MiniBPE%EF%BC%9A%E6%8E%A2%E7%A9%B6Github%E4%B8%8A%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84BPE%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81/#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95): def __init__(self): super().__init__() BasicTokenizer åœ¨ encode æ—¶ï¼Œå…ˆå°†æ–‡æœ¬è½¬æ¢ä¸ºutf-8ç¼–ç ï¼Œç„¶åè®¡ç®—statsï¼Œå³æ–‡æœ¬ä¸­ç›¸é‚»å­—èŠ‚çš„é¢‘ç‡ã€‚\npair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\"))) åœ¨ merges ä¸­ å¯»æ‰¾ stats ï¼Œæ‰¾ä¸åˆ°åˆ™è®¾ä¸ºinfã€‚ç„¶åå–æœ€å°å€¼ï¼Œå› ä¸ºå€¼è¶Šå°ï¼Œè¯´æ˜è¯¥tokenè¶Šå¤„äº merge çš„æ—©æœŸï¼Œè¿™æ ·èƒ½å®ç°é€æ¸èšåˆã€‚\nRegexTokenizer åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œæ”¹è¿›ï¼š\nä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†è¾“å…¥æ–‡æœ¬æ‹†åˆ†ä¸º text_chunksï¼ˆä¾‹å¦‚ï¼Œå°† \"Hello, world!\" æ‹†åˆ†ä¸º [\"Hello\", \",\", \" world\", \"!\"]ï¼‰ã€‚ å°†æ¯ä¸ªæ–‡æœ¬å—ç¼–ç ä¸º UTF-8 å­—èŠ‚åºåˆ—ã€‚ä¾‹å¦‚ï¼Œ\"Hello\" ç¼–ç ä¸ºå­—èŠ‚ b'Hello'ï¼Œå†è½¬ä¸º [72, 101, 108, 108, 111]ã€‚ åœ¨æ¯ä¸ªæ–‡æœ¬å—å†…éƒ¨è®¡ç®—statsï¼Œè¿™ä¸€ç‚¹å’Œbasicä¸åŒã€‚è¿™æ ·è®¡ç®—ç›¸é‚»å­—ç¬¦çš„é¢‘ç‡æ—¶ä¸ä¼šè·¨å•è¯ã€‚ special token çš„å¤„ç†ï¼š\nregister_special_tokens å°†æŒ‡å®šçš„ special token åŠ å…¥ vocab ä¸­ã€‚ åœ¨ encode æ—¶ï¼Œå¦‚æœ allowed_special=\"all\"ï¼Œå…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬ä¸­çš„ special token åˆ†ç¦»å‡ºæ¥ï¼Œå¾—åˆ° special_chunksã€‚ å¯¹äºä¸å« special token çš„ partï¼Œä½¿ç”¨ encode_ordinary åˆ†å‰²ã€è½¬æ¢ä¸ºç¼–ç ã€‚ special token ç›´æ¥è½¬æ¢ä¸ºç¼–ç ï¼Œå®ç°ä¼˜å…ˆå¤„ç†ã€‚ å¦‚æœ allowed_special=\"none\"ï¼Œåˆ™æ•´ä¸ªæ–‡æœ¬ä½¿ç”¨ encode_ordinary å¤„ç†ã€‚ ","wordCount":"355","inLanguage":"en","datePublished":"2025-09-23T14:46:23+08:00","dateModified":"2025-09-23T14:46:23+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/"},"publisher":{"@type":"Organization","name":"é™ˆ","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="é™ˆ (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>é™ˆ</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°</h1><div class=post-meta><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#minbpebpe%e7%ae%97%e6%b3%95%e7%9a%84%e6%9e%81%e7%ae%80%e5%ae%9e%e7%8e%b0 aria-label=minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°>minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°</a></li><li><a href=#1bpe-%e7%ae%97%e6%b3%95 aria-label="1.BPE ç®—æ³•">1.BPE ç®—æ³•</a></li><li><a href=#2minbpe-%e7%ae%80%e4%bb%8b aria-label="2.minbpe ç®€ä»‹">2.minbpe ç®€ä»‹</a><ul><li><a href=#21quickstart aria-label=2.1quickstart>2.1quickstart</a></li><li><a href=#22minbpe%e5%92%8cgpt-4%e5%88%86%e8%af%8d%e5%99%a8%e5%8a%9f%e8%83%bd%e7%9b%b8%e5%90%8c aria-label=2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ>2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ</a></li><li><a href=#3%e8%ae%ad%e7%bb%83 aria-label=3.è®­ç»ƒ>3.è®­ç»ƒ</a></li></ul></li><li><a href=#3%e9%a1%b9%e7%9b%ae%e8%a7%a3%e6%9e%90 aria-label=3.é¡¹ç›®è§£æ>3.é¡¹ç›®è§£æ</a><ul><li><a href=#31%e9%a1%b9%e7%9b%ae%e7%bb%93%e6%9e%84 aria-label=3.1é¡¹ç›®ç»“æ„>3.1é¡¹ç›®ç»“æ„</a></li><li><a href=#32%e4%bb%a3%e7%a0%81%e5%88%86%e6%9e%90 aria-label=3.2ä»£ç åˆ†æ>3.2ä»£ç åˆ†æ</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=minbpebpeç®—æ³•çš„æç®€å®ç°>minbpeï¼šBPEç®—æ³•çš„æç®€å®ç°<a hidden class=anchor aria-hidden=true href=#minbpebpeç®—æ³•çš„æç®€å®ç°>#</a></h1><p>github åœ°å€ï¼š<a href=https://github.com/karpathy/minbpe>karpathy/minbpe</a></p><br><h1 id=1bpe-ç®—æ³•>1.BPE ç®—æ³•<a hidden class=anchor aria-hidden=true href=#1bpe-ç®—æ³•>#</a></h1><p>BPE(Byte Pair Encoding) æ˜¯å¤§æ¨¡å‹çš„ tokenizer å¸¸ç”¨çš„ç®—æ³•ã€‚å®ƒå¯¹è¾“å…¥æ–‡æœ¬çš„å­—èŠ‚è¿›è¡Œç¼–ç ã€‚</p><p>è¯¥ç®—æ³•å›  GPT-2 çš„<a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>è®ºæ–‡</a>å’Œ<a href=https://github.com/openai/gpt-2>ä»£ç </a>è€Œè¢«å¹¿æ³›ä½¿ç”¨äº LLMã€‚<a href=https://arxiv.org/abs/1508.07909>Sennrich et al. 2015</a>è¢«è®¤ä¸ºæ˜¯ BPE åœ¨ NLP åº”ç”¨ä¸­çš„åŸå§‹å‚è€ƒã€‚</p><p>ç®€å•æ¥è¯´ï¼Œbpe æŠŠæ–‡æœ¬çœ‹ä½œ utf-8 ç¼–ç çš„å­—èŠ‚ï¼Œç„¶åå°†å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç›¸é‚»å­—èŠ‚åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ç¼–ç ã€‚å¦‚æ­¤åå¤æ“ä½œã€‚</p><br><h1 id=2minbpe-ç®€ä»‹>2.minbpe ç®€ä»‹<a hidden class=anchor aria-hidden=true href=#2minbpe-ç®€ä»‹>#</a></h1><h2 id=21quickstart>2.1quickstart<a hidden class=anchor aria-hidden=true href=#21quickstart>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>BasicTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BasicTokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;aaabdaaabac&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># è®­ç»ƒ</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=mi>256</span> <span class=o>+</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 256tokens, 3merges</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ç¼–ç </span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># è§£ç </span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=mi>258</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>258</span><span class=p>,</span> <span class=mi>97</span><span class=p>,</span> <span class=mi>99</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ä¿å­˜</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;toy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># writes two files: toy.model (for loading) and toy.vocab (for viewing)</span>
</span></span></code></pre></div><p>è‹±è¯­å­—æ¯ä¸€ä¸ªå­—æ¯å¯¹åº”ä¸€ä¸ªå­—èŠ‚ã€‚å¯¹äº"aaabdaaabac"ï¼Œå…ˆè®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åé€‰æ‹©æ¬¡æ•°æœ€å¤šçš„è¿›è¡Œåˆå¹¶ï¼ˆâ€œaaâ€ï¼Œ4æ¬¡ï¼‰ã€‚</p><p>â€œaâ€â€œaâ€åˆå¹¶ä¸ºâ€œaaâ€ï¼Œç¼–ç ä¸º256ã€‚</p><p>ç„¶åå†è®¡ç®—ç›¸é‚»ä¸¤å­—èŠ‚çš„å‡ºç°æ¬¡æ•°ï¼Œå†åˆå¹¶ã€‚</p><p>åœ¨ toy.vocab ä¸­å¯ä»¥çœ‹åˆ°æ‰€æœ‰å­—ç¬¦åŠå¯¹åº”çš„ç¼–ç ã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>toy.vocab
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>......
</span></span><span class=line><span class=cl>[a][a] -&gt; [aa] 256
</span></span><span class=line><span class=cl>[aa][a] -&gt; [aaa] 257
</span></span><span class=line><span class=cl>[aaa][b] -&gt; [aaab] 258
</span></span></code></pre></div><br><h2 id=22minbpeå’Œgpt-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ>2.2minbpeå’ŒGPT-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ<a hidden class=anchor aria-hidden=true href=#22minbpeå’Œgpt-4åˆ†è¯å™¨åŠŸèƒ½ç›¸åŒ>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1.è¯æ˜RegexTokenizerä¸GPT-4çš„åˆ†è¯å™¨æ€§èƒ½ä¸€è‡´</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># pip install tiktoken</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl><span class=n>enc</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>GPT4Tokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>GPT4Tokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2.æ ‡è®°ç‰¹æ®Štoken</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;&lt;|endoftext|&gt;hello world&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=s2>&#34;all&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [100257, 15339, 1917]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ours</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=s2>&#34;all&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [100257, 15339, 1917]</span>
</span></span></code></pre></div><p>è°ƒç”¨ encode æ—¶å¿…é¡»æ˜¾ç¤ºå£°æ˜å¤„ç†ç‰¹æ®Šæ ‡è®°ã€‚<code>allowed_special</code> å‚æ•°å¯ä»¥è®¾ç½®ä¸º"all"ã€&ldquo;none"æˆ–ä¸€ä¸ªç‰¹æ®Štokenåˆ—è¡¨ã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>é˜²æ­¢æ¶æ„æ³¨å…¥
</span></span><span class=line><span class=cl>é£é™©ç‰¹æ®Šä»¤ç‰Œï¼ˆå¦‚&lt;|endoftext|&gt;ã€&lt;|user|&gt;ç­‰ï¼‰é€šå¸¸åœ¨æ¨¡å‹ä¸­æœ‰ç‰¹æ®Šå«ä¹‰ï¼ˆå¦‚ç»ˆæ­¢åºåˆ—ã€è§’è‰²åŒºåˆ†ï¼‰ã€‚å¦‚æœåˆ†è¯å™¨é»˜è®¤è§£æç”¨æˆ·è¾“å…¥ä¸­çš„ç‰¹æ®Šä»¤ç‰Œï¼Œæ”»å‡»è€…å¯èƒ½ä¼šåœ¨ç”¨æˆ·è¾“å…¥ä¸­æ¶æ„æ’å…¥è¿™äº›ä»¤ç‰Œï¼Œè¯±å¯¼æ¨¡å‹æ‰§è¡Œéé¢„æœŸè¡Œä¸ºï¼ˆä¾‹å¦‚æå‰ç»ˆæ­¢ç”Ÿæˆã€åˆ‡æ¢è§’è‰²ï¼‰ã€‚æ˜¾å¼å£°æ˜allowed_specialå‚æ•°ï¼ˆå¦‚é™åˆ¶ä¸º &#34;none&#34; æˆ–æŒ‡å®šå®‰å…¨åˆ—è¡¨ï¼‰ï¼Œå¯ä»¥é¿å…å°†ç”¨æˆ·å¯æ§æ•°æ®ä¸­çš„ç‰¹æ®Šå­—ç¬¦è¯¯è§£æä¸ºåŠŸèƒ½æ€§ä»¤ç‰Œï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§è¾“å…¥éªŒè¯æœºåˆ¶ã€‚
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>é¿å…æ„å¤–çš„è¯­ä¹‰å¹²æ‰°
</span></span><span class=line><span class=cl>å³ä½¿æ²¡æœ‰æ¶æ„æ”»å‡»ï¼Œç”¨æˆ·è¾“å…¥ä¸­ä¹Ÿå¯èƒ½åŒ…å«ä¸ç‰¹æ®Šä»¤ç‰Œç›¸åŒçš„å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚è‡ªç„¶æ–‡æœ¬ä¸­æ°å¥½å‡ºç°&lt;|end|&gt;ï¼‰ã€‚å¦‚æœåˆ†è¯å™¨é»˜è®¤è§£æè¿™äº›å­—ç¬¦ä¸²ä¸ºç‰¹æ®Šä»¤ç‰Œï¼Œä¼šç ´ååŸå§‹æ–‡æœ¬çš„è¯­ä¹‰ï¼Œå¯¼è‡´æ¨¡å‹å¤„ç†å‡ºé”™ï¼ˆå¦‚é”™è¯¯æˆªæ–­ã€è§’è‰²æ··æ·†ï¼‰ã€‚é€šè¿‡æ˜¾å¼æ§åˆ¶ï¼Œå¼€å‘è€…å¯ä»¥ç¡®ä¿åªæœ‰é¢„æœŸçš„ç‰¹æ®Šä»¤ç‰Œè¢«è§£æï¼Œå…¶ä»–ç±»ä¼¼å­—ç¬¦ä¸²ä»…ä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†ã€‚
</span></span></code></pre></div><br><h2 id=3è®­ç»ƒ>3.è®­ç»ƒ<a hidden class=anchor aria-hidden=true href=#3è®­ç»ƒ>#</a></h2><p>minbpe å¯ä»¥åœ¨è¯­æ–™ä¸Šè®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ä»¿ç…§openaiï¼Œä½¿ç”¨æ­£åˆ™æ–¹æ³•æ¥æ‹†åˆ†æ–‡æœ¬</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>RegexTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>RegexTokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>very_long_training_string</span><span class=p>,</span> <span class=n>vocab_size</span><span class=o>=</span><span class=mi>32768</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;hello world&#34;</span><span class=p>)</span> <span class=c1># string -&gt; tokens</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=mi>1000</span><span class=p>,</span> <span class=mi>2000</span><span class=p>,</span> <span class=mi>3000</span><span class=p>])</span> <span class=c1># tokens -&gt; string</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;tok32k&#34;</span><span class=p>)</span> <span class=c1># writes tok32k.model and tok32k.vocab</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;tok32k.model&#34;</span><span class=p>)</span> <span class=c1># loads the model back from disk</span>
</span></span></code></pre></div><p>å¦‚æœè¦æ·»åŠ <code>special tokens</code>ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># å‰256ä¸ºåŸå§‹å­—èŠ‚tokenï¼Œæ¥ä¸‹æ¥32768-256ä¸ºåˆå¹¶çš„tokenï¼Œåé¢çš„32768æ˜¯ç‰¹æ®Štoken</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>RegexTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>RegexTokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>very_long_training_string</span><span class=p>,</span> <span class=n>vocab_size</span><span class=o>=</span><span class=mi>32768</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>register_special_tokens</span><span class=p>({</span><span class=s2>&#34;&lt;|endoftext|&gt;&#34;</span><span class=p>:</span> <span class=mi>32768</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;&lt;|endoftext|&gt;hello world&#34;</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=s2>&#34;all&#34;</span><span class=p>)</span>
</span></span></code></pre></div><br><h1 id=3é¡¹ç›®è§£æ>3.é¡¹ç›®è§£æ<a hidden class=anchor aria-hidden=true href=#3é¡¹ç›®è§£æ>#</a></h1><h2 id=31é¡¹ç›®ç»“æ„>3.1é¡¹ç›®ç»“æ„<a hidden class=anchor aria-hidden=true href=#31é¡¹ç›®ç»“æ„>#</a></h2><p>é¡¹ç›®ä¸­æœ‰ä¸¤ä¸ªåˆ†è¯å™¨ï¼Œå®ƒä»¬éƒ½å¯ä»¥æ‰§è¡Œåˆ†è¯å™¨çš„ä¸‰ä¸ªä¸»è¦åŠŸèƒ½ï¼š1ï¼‰åœ¨ç»™å®šæ–‡æœ¬ä¸Šè®­ç»ƒåˆ†è¯å™¨è¯æ±‡å¹¶æ‰§è¡Œåˆå¹¶ï¼Œ2ï¼‰å°†æ–‡æœ¬ç¼–ç ä¸ºtokensï¼Œ3ï¼‰å°†tokensè§£ç ä¸ºæ–‡æœ¬ã€‚ä»“åº“çš„æ–‡ä»¶å¦‚ä¸‹ï¼š</p><ol><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/base.py>minbpe/base.py</a>: å®ç°äº† <code>Tokenizer</code> ç±»ï¼Œå®ƒæ˜¯åŸºç¡€ç±»ã€‚å®ƒåŒ…å« <code>train</code> ã€ <code>encode</code> å’Œ <code>decode</code> ã€<code>save/load</code>åŠŸèƒ½ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è§çš„å·¥å…·å‡½æ•°ã€‚è¿™ä¸ªç±»ä¸æ˜¯ç›´æ¥ä½¿ç”¨çš„ï¼Œè€Œæ˜¯ç”¨æ¥ç»§æ‰¿çš„ã€‚</li><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/basic.py>minbpe/basic.py</a>: å®ç°äº† <code>BasicTokenizer</code> ï¼Œè¿™æ˜¯ BPE ç®—æ³•æœ€ç®€å•çš„å®ç°ï¼Œç›´æ¥åœ¨æ–‡æœ¬ä¸Šè¿è¡Œã€‚</li><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/regex.py>minbpe/regex.py</a>: å®ç°äº† <code>RegexTokenizer</code> ï¼Œé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è¿›ä¸€æ­¥åˆ†å‰²è¾“å…¥æ–‡æœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢„å¤„ç†é˜¶æ®µï¼Œåœ¨åˆ†è¯ä¹‹å‰æŒ‰ç±»åˆ«åˆ†å‰²è¾“å…¥æ–‡æœ¬ã€‚è¿™ç¡®ä¿äº†ä¸ä¼šåœ¨ç±»åˆ«è¾¹ç•Œå¤„è¿›è¡Œåˆå¹¶ã€‚è¿™ä¸ªæ–¹æ³•åœ¨ GPT-2 è®ºæ–‡ä¸­è¢«å¼•å…¥ï¼Œå¹¶ä¸”æˆªè‡³ GPT-4 ä»åœ¨ä½¿ç”¨ã€‚è¿™ä¸ªç±»è¿˜å¤„ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚</li><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py>minbpe/gpt4.py</a>: å®ç°äº† <code>GPT4Tokenizer</code> ã€‚è¿™ä¸ªç±»æ˜¯ <code>RegexTokenizer</code> çš„ä¸€ä¸ªè½»é‡çº§åŒ…è£…å™¨ï¼Œç²¾ç¡®åœ°é‡ç°äº† tiktoken åº“ä¸­ GPT-4 çš„åˆ†è¯ã€‚</li></ol><p>æœ€åï¼Œè„šæœ¬ train.py åœ¨è¾“å…¥æ–‡æœ¬ tests/taylorswift.txt ä¸Šè®­ç»ƒäº†ä¸¤ä¸ªä¸»è¦çš„åˆ†è¯å™¨ï¼Œå¹¶å°†è¯æ±‡è¡¨ä¿å­˜åˆ°ç£ç›˜ä»¥ä¾›å¯è§†åŒ–ã€‚</p><br><h2 id=32ä»£ç åˆ†æ>3.2ä»£ç åˆ†æ<a hidden class=anchor aria-hidden=true href=#32ä»£ç åˆ†æ>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>class ä»£ç åˆ†æ(https://zerolovesea.github.io/2024/03/09/MiniBPE%EF%BC%9A%E6%8E%A2%E7%A9%B6Github%E4%B8%8A%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84BPE%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81/#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95):
</span></span><span class=line><span class=cl>	def __init__(self):
</span></span><span class=line><span class=cl>        super().__init__()
</span></span></code></pre></div><br><p><code>BasicTokenizer</code> åœ¨ <code>encode</code> æ—¶ï¼Œå…ˆå°†æ–‡æœ¬è½¬æ¢ä¸ºutf-8ç¼–ç ï¼Œç„¶åè®¡ç®—statsï¼Œå³æ–‡æœ¬ä¸­ç›¸é‚»å­—èŠ‚çš„é¢‘ç‡ã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pair</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>stats</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>p</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>merges</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;inf&#34;</span><span class=p>)))</span>
</span></span></code></pre></div><p>åœ¨ merges ä¸­ å¯»æ‰¾ stats ï¼Œæ‰¾ä¸åˆ°åˆ™è®¾ä¸ºinfã€‚ç„¶åå–æœ€å°å€¼ï¼Œå› ä¸ºå€¼è¶Šå°ï¼Œè¯´æ˜è¯¥tokenè¶Šå¤„äº merge çš„æ—©æœŸï¼Œè¿™æ ·èƒ½å®ç°é€æ¸èšåˆã€‚</p><br><p><code>RegexTokenizer</code> åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œæ”¹è¿›ï¼š</p><ol><li>ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†è¾“å…¥æ–‡æœ¬æ‹†åˆ†ä¸º <code>text_chunks</code>ï¼ˆä¾‹å¦‚ï¼Œå°† <code>"Hello, world!"</code> æ‹†åˆ†ä¸º <code>["Hello", ",", " world", "!"]</code>ï¼‰ã€‚</li><li>å°†æ¯ä¸ªæ–‡æœ¬å—ç¼–ç ä¸º UTF-8 å­—èŠ‚åºåˆ—ã€‚ä¾‹å¦‚ï¼Œ<code>"Hello"</code> ç¼–ç ä¸ºå­—èŠ‚ <code>b'Hello'</code>ï¼Œå†è½¬ä¸º <code>[72, 101, 108, 108, 111]</code>ã€‚</li><li>åœ¨æ¯ä¸ªæ–‡æœ¬å—å†…éƒ¨è®¡ç®—statsï¼Œè¿™ä¸€ç‚¹å’Œbasicä¸åŒã€‚è¿™æ ·è®¡ç®—ç›¸é‚»å­—ç¬¦çš„é¢‘ç‡æ—¶ä¸ä¼šè·¨å•è¯ã€‚</li></ol><br><p>special token çš„å¤„ç†ï¼š</p><ol><li><code>register_special_tokens</code> å°†æŒ‡å®šçš„ special token åŠ å…¥ vocab ä¸­ã€‚</li><li>åœ¨ <code>encode</code> æ—¶ï¼Œå¦‚æœ <code>allowed_special="all"</code>ï¼Œå…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬ä¸­çš„ special token åˆ†ç¦»å‡ºæ¥ï¼Œå¾—åˆ° <code>special_chunks</code>ã€‚<ol><li>å¯¹äºä¸å« special token çš„ partï¼Œä½¿ç”¨ <code>encode_ordinary</code> åˆ†å‰²ã€è½¬æ¢ä¸ºç¼–ç ã€‚</li><li>special token ç›´æ¥è½¬æ¢ä¸ºç¼–ç ï¼Œå®ç°ä¼˜å…ˆå¤„ç†ã€‚</li></ol></li><li>å¦‚æœ <code>allowed_special="none"</code>ï¼Œåˆ™æ•´ä¸ªæ–‡æœ¬ä½¿ç”¨ <code>encode_ordinary</code> å¤„ç†ã€‚</li></ol><br></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>é™ˆ</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>