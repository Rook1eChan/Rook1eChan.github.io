<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>minbpe：BPE算法的极简实现 | 陈</title><meta name=keywords content><meta name=description content='minbpe：BPE算法的极简实现
github 地址：karpathy/minbpe

1.BPE 算法
BPE(Byte Pair Encoding) 是大模型的 tokenizer 常用的算法。它对输入文本的字节进行编码。
该算法因 GPT-2 的论文和代码而被广泛使用于 LLM。Sennrich et al. 2015被认为是 BPE 在 NLP 应用中的原始参考。
简单来说，bpe 把文本看作 utf-8 编码的字节，然后将出现次数最多的相邻字节合并，生成一个新的编码。如此反复操作。

2.minbpe 简介
2.1quickstart
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
text = "aaabdaaabac"

# 训练
tokenizer.train(text, 256 + 3)  # 256tokens, 3merges

# 编码
print(tokenizer.encode(text))

# 解码
print(tokenizer.decode([258, 100, 258, 97, 99]))

# 保存
tokenizer.save("toy")
# writes two files: toy.model (for loading) and toy.vocab (for viewing)
英语字母一个字母对应一个字节。对于"aaabdaaabac"，先计算相邻两字节的出现次数，然后选择次数最多的进行合并（“aa”，4次）。
“a”“a”合并为“aa”，编码为256。
然后再计算相邻两字节的出现次数，再合并。
在 toy.vocab 中可以看到所有字符及对应的编码。
toy.vocab

......
[a][a] -> [aa] 256
[aa][a] -> [aaa] 257
[aaa][b] -> [aaab] 258

2.2minbpe和GPT-4分词器功能相同
# 1.证明RegexTokenizer与GPT-4的分词器性能一致
text = "hello123!!!? (안녕하세요!) 😉"

# pip install tiktoken
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

# 2.标记特殊token
text = "<|endoftext|>hello world"

print(enc.encode(text, allowed_special="all"))
# [100257, 15339, 1917]

# ours
print(tokenizer.encode(text, allowed_special="all"))
# [100257, 15339, 1917]
调用 encode 时必须显示声明处理特殊标记。allowed_special 参数可以设置为"all"、&ldquo;none"或一个特殊token列表。'><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/"><meta property="og:site_name" content="陈"><meta property="og:title" content="minbpe：BPE算法的极简实现"><meta property="og:description" content='minbpe：BPE算法的极简实现 github 地址：karpathy/minbpe
1.BPE 算法 BPE(Byte Pair Encoding) 是大模型的 tokenizer 常用的算法。它对输入文本的字节进行编码。
该算法因 GPT-2 的论文和代码而被广泛使用于 LLM。Sennrich et al. 2015被认为是 BPE 在 NLP 应用中的原始参考。
简单来说，bpe 把文本看作 utf-8 编码的字节，然后将出现次数最多的相邻字节合并，生成一个新的编码。如此反复操作。
2.minbpe 简介 2.1quickstart from minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = "aaabdaaabac" # 训练 tokenizer.train(text, 256 + 3) # 256tokens, 3merges # 编码 print(tokenizer.encode(text)) # 解码 print(tokenizer.decode([258, 100, 258, 97, 99])) # 保存 tokenizer.save("toy") # writes two files: toy.model (for loading) and toy.vocab (for viewing) 英语字母一个字母对应一个字节。对于"aaabdaaabac"，先计算相邻两字节的出现次数，然后选择次数最多的进行合并（“aa”，4次）。
“a”“a”合并为“aa”，编码为256。
然后再计算相邻两字节的出现次数，再合并。
在 toy.vocab 中可以看到所有字符及对应的编码。
toy.vocab ...... [a][a] -> [aa] 256 [aa][a] -> [aaa] 257 [aaa][b] -> [aaab] 258 2.2minbpe和GPT-4分词器功能相同 # 1.证明RegexTokenizer与GPT-4的分词器性能一致 text = "hello123!!!? (안녕하세요!) 😉" # pip install tiktoken import tiktoken enc = tiktoken.get_encoding("cl100k_base") print(enc.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] from minbpe import GPT4Tokenizer tokenizer = GPT4Tokenizer() print(tokenizer.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] # 2.标记特殊token text = "<|endoftext|>hello world" print(enc.encode(text, allowed_special="all")) # [100257, 15339, 1917] # ours print(tokenizer.encode(text, allowed_special="all")) # [100257, 15339, 1917] 调用 encode 时必须显示声明处理特殊标记。allowed_special 参数可以设置为"all"、“none"或一个特殊token列表。'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-23T14:46:23+08:00"><meta property="article:modified_time" content="2025-09-23T14:46:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="minbpe：BPE算法的极简实现"><meta name=twitter:description content='minbpe：BPE算法的极简实现
github 地址：karpathy/minbpe

1.BPE 算法
BPE(Byte Pair Encoding) 是大模型的 tokenizer 常用的算法。它对输入文本的字节进行编码。
该算法因 GPT-2 的论文和代码而被广泛使用于 LLM。Sennrich et al. 2015被认为是 BPE 在 NLP 应用中的原始参考。
简单来说，bpe 把文本看作 utf-8 编码的字节，然后将出现次数最多的相邻字节合并，生成一个新的编码。如此反复操作。

2.minbpe 简介
2.1quickstart
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
text = "aaabdaaabac"

# 训练
tokenizer.train(text, 256 + 3)  # 256tokens, 3merges

# 编码
print(tokenizer.encode(text))

# 解码
print(tokenizer.decode([258, 100, 258, 97, 99]))

# 保存
tokenizer.save("toy")
# writes two files: toy.model (for loading) and toy.vocab (for viewing)
英语字母一个字母对应一个字节。对于"aaabdaaabac"，先计算相邻两字节的出现次数，然后选择次数最多的进行合并（“aa”，4次）。
“a”“a”合并为“aa”，编码为256。
然后再计算相邻两字节的出现次数，再合并。
在 toy.vocab 中可以看到所有字符及对应的编码。
toy.vocab

......
[a][a] -> [aa] 256
[aa][a] -> [aaa] 257
[aaa][b] -> [aaab] 258

2.2minbpe和GPT-4分词器功能相同
# 1.证明RegexTokenizer与GPT-4的分词器性能一致
text = "hello123!!!? (안녕하세요!) 😉"

# pip install tiktoken
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

# 2.标记特殊token
text = "<|endoftext|>hello world"

print(enc.encode(text, allowed_special="all"))
# [100257, 15339, 1917]

# ours
print(tokenizer.encode(text, allowed_special="all"))
# [100257, 15339, 1917]
调用 encode 时必须显示声明处理特殊标记。allowed_special 参数可以设置为"all"、&ldquo;none"或一个特殊token列表。'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"minbpe：BPE算法的极简实现","item":"https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"minbpe：BPE算法的极简实现","name":"minbpe：BPE算法的极简实现","description":"minbpe：BPE算法的极简实现 github 地址：karpathy/minbpe\n1.BPE 算法 BPE(Byte Pair Encoding) 是大模型的 tokenizer 常用的算法。它对输入文本的字节进行编码。\n该算法因 GPT-2 的论文和代码而被广泛使用于 LLM。Sennrich et al. 2015被认为是 BPE 在 NLP 应用中的原始参考。\n简单来说，bpe 把文本看作 utf-8 编码的字节，然后将出现次数最多的相邻字节合并，生成一个新的编码。如此反复操作。\n2.minbpe 简介 2.1quickstart from minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = \u0026#34;aaabdaaabac\u0026#34; # 训练 tokenizer.train(text, 256 + 3) # 256tokens, 3merges # 编码 print(tokenizer.encode(text)) # 解码 print(tokenizer.decode([258, 100, 258, 97, 99])) # 保存 tokenizer.save(\u0026#34;toy\u0026#34;) # writes two files: toy.model (for loading) and toy.vocab (for viewing) 英语字母一个字母对应一个字节。对于\u0026quot;aaabdaaabac\u0026quot;，先计算相邻两字节的出现次数，然后选择次数最多的进行合并（“aa”，4次）。\n“a”“a”合并为“aa”，编码为256。\n然后再计算相邻两字节的出现次数，再合并。\n在 toy.vocab 中可以看到所有字符及对应的编码。\ntoy.vocab ...... [a][a] -\u0026gt; [aa] 256 [aa][a] -\u0026gt; [aaa] 257 [aaa][b] -\u0026gt; [aaab] 258 2.2minbpe和GPT-4分词器功能相同 # 1.证明RegexTokenizer与GPT-4的分词器性能一致 text = \u0026#34;hello123!!!? (안녕하세요!) 😉\u0026#34; # pip install tiktoken import tiktoken enc = tiktoken.get_encoding(\u0026#34;cl100k_base\u0026#34;) print(enc.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] from minbpe import GPT4Tokenizer tokenizer = GPT4Tokenizer() print(tokenizer.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] # 2.标记特殊token text = \u0026#34;\u0026lt;|endoftext|\u0026gt;hello world\u0026#34; print(enc.encode(text, allowed_special=\u0026#34;all\u0026#34;)) # [100257, 15339, 1917] # ours print(tokenizer.encode(text, allowed_special=\u0026#34;all\u0026#34;)) # [100257, 15339, 1917] 调用 encode 时必须显示声明处理特殊标记。allowed_special 参数可以设置为\u0026quot;all\u0026quot;、\u0026ldquo;none\u0026quot;或一个特殊token列表。\n","keywords":[],"articleBody":"minbpe：BPE算法的极简实现 github 地址：karpathy/minbpe\n1.BPE 算法 BPE(Byte Pair Encoding) 是大模型的 tokenizer 常用的算法。它对输入文本的字节进行编码。\n该算法因 GPT-2 的论文和代码而被广泛使用于 LLM。Sennrich et al. 2015被认为是 BPE 在 NLP 应用中的原始参考。\n简单来说，bpe 把文本看作 utf-8 编码的字节，然后将出现次数最多的相邻字节合并，生成一个新的编码。如此反复操作。\n2.minbpe 简介 2.1quickstart from minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = \"aaabdaaabac\" # 训练 tokenizer.train(text, 256 + 3) # 256tokens, 3merges # 编码 print(tokenizer.encode(text)) # 解码 print(tokenizer.decode([258, 100, 258, 97, 99])) # 保存 tokenizer.save(\"toy\") # writes two files: toy.model (for loading) and toy.vocab (for viewing) 英语字母一个字母对应一个字节。对于\"aaabdaaabac\"，先计算相邻两字节的出现次数，然后选择次数最多的进行合并（“aa”，4次）。\n“a”“a”合并为“aa”，编码为256。\n然后再计算相邻两字节的出现次数，再合并。\n在 toy.vocab 中可以看到所有字符及对应的编码。\ntoy.vocab ...... [a][a] -\u003e [aa] 256 [aa][a] -\u003e [aaa] 257 [aaa][b] -\u003e [aaab] 258 2.2minbpe和GPT-4分词器功能相同 # 1.证明RegexTokenizer与GPT-4的分词器性能一致 text = \"hello123!!!? (안녕하세요!) 😉\" # pip install tiktoken import tiktoken enc = tiktoken.get_encoding(\"cl100k_base\") print(enc.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] from minbpe import GPT4Tokenizer tokenizer = GPT4Tokenizer() print(tokenizer.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] # 2.标记特殊token text = \"\u003c|endoftext|\u003ehello world\" print(enc.encode(text, allowed_special=\"all\")) # [100257, 15339, 1917] # ours print(tokenizer.encode(text, allowed_special=\"all\")) # [100257, 15339, 1917] 调用 encode 时必须显示声明处理特殊标记。allowed_special 参数可以设置为\"all\"、“none\"或一个特殊token列表。\n防止恶意注入 风险特殊令牌（如\u003c|endoftext|\u003e、\u003c|user|\u003e等）通常在模型中有特殊含义（如终止序列、角色区分）。如果分词器默认解析用户输入中的特殊令牌，攻击者可能会在用户输入中恶意插入这些令牌，诱导模型执行非预期行为（例如提前终止生成、切换角色）。显式声明allowed_special参数（如限制为 \"none\" 或指定安全列表），可以避免将用户可控数据中的特殊字符误解析为功能性令牌，本质上是一种输入验证机制。 避免意外的语义干扰 即使没有恶意攻击，用户输入中也可能包含与特殊令牌相同的字符串（例如自然文本中恰好出现\u003c|end|\u003e）。如果分词器默认解析这些字符串为特殊令牌，会破坏原始文本的语义，导致模型处理出错（如错误截断、角色混淆）。通过显式控制，开发者可以确保只有预期的特殊令牌被解析，其他类似字符串仅作为普通文本处理。 3.训练 minbpe 可以在语料上训练自己的分词器。\n# 仿照openai，使用正则方法来拆分文本 from minbpe import RegexTokenizer tokenizer = RegexTokenizer() tokenizer.train(very_long_training_string, vocab_size=32768) tokenizer.encode(\"hello world\") # string -\u003e tokens tokenizer.decode([1000, 2000, 3000]) # tokens -\u003e string tokenizer.save(\"tok32k\") # writes tok32k.model and tok32k.vocab tokenizer.load(\"tok32k.model\") # loads the model back from disk 如果要添加special tokens：\n# 前256为原始字节token，接下来32768-256为合并的token，后面的32768是特殊token from minbpe import RegexTokenizer tokenizer = RegexTokenizer() tokenizer.train(very_long_training_string, vocab_size=32768) tokenizer.register_special_tokens({\"\u003c|endoftext|\u003e\": 32768}) tokenizer.encode(\"\u003c|endoftext|\u003ehello world\", allowed_special=\"all\") 3.项目解析 3.1项目结构 项目中有两个分词器，它们都可以执行分词器的三个主要功能：1）在给定文本上训练分词器词汇并执行合并，2）将文本编码为tokens，3）将tokens解码为文本。仓库的文件如下：\nminbpe/base.py: 实现了 Tokenizer 类，它是基础类。它包含 train 、 encode 和 decode 、save/load功能，还有一些常见的工具函数。这个类不是直接使用的，而是用来继承的。 minbpe/basic.py: 实现了 BasicTokenizer ，这是 BPE 算法最简单的实现，直接在文本上运行。 minbpe/regex.py: 实现了 RegexTokenizer ，通过正则表达式模式进一步分割输入文本，这是一个预处理阶段，在分词之前按类别分割输入文本。这确保了不会在类别边界处进行合并。这个方法在 GPT-2 论文中被引入，并且截至 GPT-4 仍在使用。这个类还处理特殊标记（如果有）。 minbpe/gpt4.py: 实现了 GPT4Tokenizer 。这个类是 RegexTokenizer 的一个轻量级包装器，精确地重现了 tiktoken 库中 GPT-4 的分词。 最后，脚本 train.py 在输入文本 tests/taylorswift.txt 上训练了两个主要的分词器，并将词汇表保存到磁盘以供可视化。\n3.2代码分析 class 代码分析(https://zerolovesea.github.io/2024/03/09/MiniBPE%EF%BC%9A%E6%8E%A2%E7%A9%B6Github%E4%B8%8A%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84BPE%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81/#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95): def __init__(self): super().__init__() BasicTokenizer 在 encode 时，先将文本转换为utf-8编码，然后计算stats，即文本中相邻字节的频率。\npair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\"))) 在 merges 中 寻找 stats ，找不到则设为inf。然后取最小值，因为值越小，说明该token越处于 merge 的早期，这样能实现逐渐聚合。\nRegexTokenizer 在训练阶段进行改进：\n使用正则表达式将输入文本拆分为 text_chunks（例如，将 \"Hello, world!\" 拆分为 [\"Hello\", \",\", \" world\", \"!\"]）。 将每个文本块编码为 UTF-8 字节序列。例如，\"Hello\" 编码为字节 b'Hello'，再转为 [72, 101, 108, 108, 111]。 在每个文本块内部计算stats，这一点和basic不同。这样计算相邻字符的频率时不会跨单词。 special token 的处理：\nregister_special_tokens 将指定的 special token 加入 vocab 中。 在 encode 时，如果 allowed_special=\"all\"，先使用正则表达式将文本中的 special token 分离出来，得到 special_chunks。 对于不含 special token 的 part，使用 encode_ordinary 分割、转换为编码。 special token 直接转换为编码，实现优先处理。 如果 allowed_special=\"none\"，则整个文本使用 encode_ordinary 处理。 ","wordCount":"355","inLanguage":"en","datePublished":"2025-09-23T14:46:23+08:00","dateModified":"2025-09-23T14:46:23+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/"},"publisher":{"@type":"Organization","name":"陈","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="陈 (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>陈</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">minbpe：BPE算法的极简实现</h1><div class=post-meta><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#minbpebpe%e7%ae%97%e6%b3%95%e7%9a%84%e6%9e%81%e7%ae%80%e5%ae%9e%e7%8e%b0 aria-label=minbpe：BPE算法的极简实现>minbpe：BPE算法的极简实现</a></li><li><a href=#1bpe-%e7%ae%97%e6%b3%95 aria-label="1.BPE 算法">1.BPE 算法</a></li><li><a href=#2minbpe-%e7%ae%80%e4%bb%8b aria-label="2.minbpe 简介">2.minbpe 简介</a><ul><li><a href=#21quickstart aria-label=2.1quickstart>2.1quickstart</a></li><li><a href=#22minbpe%e5%92%8cgpt-4%e5%88%86%e8%af%8d%e5%99%a8%e5%8a%9f%e8%83%bd%e7%9b%b8%e5%90%8c aria-label=2.2minbpe和GPT-4分词器功能相同>2.2minbpe和GPT-4分词器功能相同</a></li><li><a href=#3%e8%ae%ad%e7%bb%83 aria-label=3.训练>3.训练</a></li></ul></li><li><a href=#3%e9%a1%b9%e7%9b%ae%e8%a7%a3%e6%9e%90 aria-label=3.项目解析>3.项目解析</a><ul><li><a href=#31%e9%a1%b9%e7%9b%ae%e7%bb%93%e6%9e%84 aria-label=3.1项目结构>3.1项目结构</a></li><li><a href=#32%e4%bb%a3%e7%a0%81%e5%88%86%e6%9e%90 aria-label=3.2代码分析>3.2代码分析</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=minbpebpe算法的极简实现>minbpe：BPE算法的极简实现<a hidden class=anchor aria-hidden=true href=#minbpebpe算法的极简实现>#</a></h1><p>github 地址：<a href=https://github.com/karpathy/minbpe>karpathy/minbpe</a></p><br><h1 id=1bpe-算法>1.BPE 算法<a hidden class=anchor aria-hidden=true href=#1bpe-算法>#</a></h1><p>BPE(Byte Pair Encoding) 是大模型的 tokenizer 常用的算法。它对输入文本的字节进行编码。</p><p>该算法因 GPT-2 的<a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>论文</a>和<a href=https://github.com/openai/gpt-2>代码</a>而被广泛使用于 LLM。<a href=https://arxiv.org/abs/1508.07909>Sennrich et al. 2015</a>被认为是 BPE 在 NLP 应用中的原始参考。</p><p>简单来说，bpe 把文本看作 utf-8 编码的字节，然后将出现次数最多的相邻字节合并，生成一个新的编码。如此反复操作。</p><br><h1 id=2minbpe-简介>2.minbpe 简介<a hidden class=anchor aria-hidden=true href=#2minbpe-简介>#</a></h1><h2 id=21quickstart>2.1quickstart<a hidden class=anchor aria-hidden=true href=#21quickstart>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>BasicTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BasicTokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;aaabdaaabac&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=mi>256</span> <span class=o>+</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 256tokens, 3merges</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 编码</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 解码</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=mi>258</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>258</span><span class=p>,</span> <span class=mi>97</span><span class=p>,</span> <span class=mi>99</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 保存</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;toy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># writes two files: toy.model (for loading) and toy.vocab (for viewing)</span>
</span></span></code></pre></div><p>英语字母一个字母对应一个字节。对于"aaabdaaabac"，先计算相邻两字节的出现次数，然后选择次数最多的进行合并（“aa”，4次）。</p><p>“a”“a”合并为“aa”，编码为256。</p><p>然后再计算相邻两字节的出现次数，再合并。</p><p>在 toy.vocab 中可以看到所有字符及对应的编码。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>toy.vocab
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>......
</span></span><span class=line><span class=cl>[a][a] -&gt; [aa] 256
</span></span><span class=line><span class=cl>[aa][a] -&gt; [aaa] 257
</span></span><span class=line><span class=cl>[aaa][b] -&gt; [aaab] 258
</span></span></code></pre></div><br><h2 id=22minbpe和gpt-4分词器功能相同>2.2minbpe和GPT-4分词器功能相同<a hidden class=anchor aria-hidden=true href=#22minbpe和gpt-4分词器功能相同>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1.证明RegexTokenizer与GPT-4的分词器性能一致</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;hello123!!!? (안녕하세요!) 😉&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># pip install tiktoken</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl><span class=n>enc</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>GPT4Tokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>GPT4Tokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2.标记特殊token</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;&lt;|endoftext|&gt;hello world&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=s2>&#34;all&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [100257, 15339, 1917]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ours</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=s2>&#34;all&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [100257, 15339, 1917]</span>
</span></span></code></pre></div><p>调用 encode 时必须显示声明处理特殊标记。<code>allowed_special</code> 参数可以设置为"all"、&ldquo;none"或一个特殊token列表。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>防止恶意注入
</span></span><span class=line><span class=cl>风险特殊令牌（如&lt;|endoftext|&gt;、&lt;|user|&gt;等）通常在模型中有特殊含义（如终止序列、角色区分）。如果分词器默认解析用户输入中的特殊令牌，攻击者可能会在用户输入中恶意插入这些令牌，诱导模型执行非预期行为（例如提前终止生成、切换角色）。显式声明allowed_special参数（如限制为 &#34;none&#34; 或指定安全列表），可以避免将用户可控数据中的特殊字符误解析为功能性令牌，本质上是一种输入验证机制。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>避免意外的语义干扰
</span></span><span class=line><span class=cl>即使没有恶意攻击，用户输入中也可能包含与特殊令牌相同的字符串（例如自然文本中恰好出现&lt;|end|&gt;）。如果分词器默认解析这些字符串为特殊令牌，会破坏原始文本的语义，导致模型处理出错（如错误截断、角色混淆）。通过显式控制，开发者可以确保只有预期的特殊令牌被解析，其他类似字符串仅作为普通文本处理。
</span></span></code></pre></div><br><h2 id=3训练>3.训练<a hidden class=anchor aria-hidden=true href=#3训练>#</a></h2><p>minbpe 可以在语料上训练自己的分词器。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 仿照openai，使用正则方法来拆分文本</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>RegexTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>RegexTokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>very_long_training_string</span><span class=p>,</span> <span class=n>vocab_size</span><span class=o>=</span><span class=mi>32768</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;hello world&#34;</span><span class=p>)</span> <span class=c1># string -&gt; tokens</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=mi>1000</span><span class=p>,</span> <span class=mi>2000</span><span class=p>,</span> <span class=mi>3000</span><span class=p>])</span> <span class=c1># tokens -&gt; string</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;tok32k&#34;</span><span class=p>)</span> <span class=c1># writes tok32k.model and tok32k.vocab</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;tok32k.model&#34;</span><span class=p>)</span> <span class=c1># loads the model back from disk</span>
</span></span></code></pre></div><p>如果要添加<code>special tokens</code>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 前256为原始字节token，接下来32768-256为合并的token，后面的32768是特殊token</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>minbpe</span> <span class=kn>import</span> <span class=n>RegexTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>RegexTokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>very_long_training_string</span><span class=p>,</span> <span class=n>vocab_size</span><span class=o>=</span><span class=mi>32768</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>register_special_tokens</span><span class=p>({</span><span class=s2>&#34;&lt;|endoftext|&gt;&#34;</span><span class=p>:</span> <span class=mi>32768</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;&lt;|endoftext|&gt;hello world&#34;</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=s2>&#34;all&#34;</span><span class=p>)</span>
</span></span></code></pre></div><br><h1 id=3项目解析>3.项目解析<a hidden class=anchor aria-hidden=true href=#3项目解析>#</a></h1><h2 id=31项目结构>3.1项目结构<a hidden class=anchor aria-hidden=true href=#31项目结构>#</a></h2><p>项目中有两个分词器，它们都可以执行分词器的三个主要功能：1）在给定文本上训练分词器词汇并执行合并，2）将文本编码为tokens，3）将tokens解码为文本。仓库的文件如下：</p><ol><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/base.py>minbpe/base.py</a>: 实现了 <code>Tokenizer</code> 类，它是基础类。它包含 <code>train</code> 、 <code>encode</code> 和 <code>decode</code> 、<code>save/load</code>功能，还有一些常见的工具函数。这个类不是直接使用的，而是用来继承的。</li><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/basic.py>minbpe/basic.py</a>: 实现了 <code>BasicTokenizer</code> ，这是 BPE 算法最简单的实现，直接在文本上运行。</li><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/regex.py>minbpe/regex.py</a>: 实现了 <code>RegexTokenizer</code> ，通过正则表达式模式进一步分割输入文本，这是一个预处理阶段，在分词之前按类别分割输入文本。这确保了不会在类别边界处进行合并。这个方法在 GPT-2 论文中被引入，并且截至 GPT-4 仍在使用。这个类还处理特殊标记（如果有）。</li><li><a href=https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py>minbpe/gpt4.py</a>: 实现了 <code>GPT4Tokenizer</code> 。这个类是 <code>RegexTokenizer</code> 的一个轻量级包装器，精确地重现了 tiktoken 库中 GPT-4 的分词。</li></ol><p>最后，脚本 train.py 在输入文本 tests/taylorswift.txt 上训练了两个主要的分词器，并将词汇表保存到磁盘以供可视化。</p><br><h2 id=32代码分析>3.2代码分析<a hidden class=anchor aria-hidden=true href=#32代码分析>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>class 代码分析(https://zerolovesea.github.io/2024/03/09/MiniBPE%EF%BC%9A%E6%8E%A2%E7%A9%B6Github%E4%B8%8A%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84BPE%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81/#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95):
</span></span><span class=line><span class=cl>	def __init__(self):
</span></span><span class=line><span class=cl>        super().__init__()
</span></span></code></pre></div><br><p><code>BasicTokenizer</code> 在 <code>encode</code> 时，先将文本转换为utf-8编码，然后计算stats，即文本中相邻字节的频率。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pair</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>stats</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>p</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>merges</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;inf&#34;</span><span class=p>)))</span>
</span></span></code></pre></div><p>在 merges 中 寻找 stats ，找不到则设为inf。然后取最小值，因为值越小，说明该token越处于 merge 的早期，这样能实现逐渐聚合。</p><br><p><code>RegexTokenizer</code> 在训练阶段进行改进：</p><ol><li>使用正则表达式将输入文本拆分为 <code>text_chunks</code>（例如，将 <code>"Hello, world!"</code> 拆分为 <code>["Hello", ",", " world", "!"]</code>）。</li><li>将每个文本块编码为 UTF-8 字节序列。例如，<code>"Hello"</code> 编码为字节 <code>b'Hello'</code>，再转为 <code>[72, 101, 108, 108, 111]</code>。</li><li>在每个文本块内部计算stats，这一点和basic不同。这样计算相邻字符的频率时不会跨单词。</li></ol><br><p>special token 的处理：</p><ol><li><code>register_special_tokens</code> 将指定的 special token 加入 vocab 中。</li><li>在 <code>encode</code> 时，如果 <code>allowed_special="all"</code>，先使用正则表达式将文本中的 special token 分离出来，得到 <code>special_chunks</code>。<ol><li>对于不含 special token 的 part，使用 <code>encode_ordinary</code> 分割、转换为编码。</li><li>special token 直接转换为编码，实现优先处理。</li></ol></li><li>如果 <code>allowed_special="none"</code>，则整个文本使用 <code>encode_ordinary</code> 处理。</li></ol><br></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>陈</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>