<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>Retrieval_Learning | Chan's Blog</title><meta name=keywords content><meta name=description content='简介：检索相关算法的学习

1.TF-IDF
1.1原理
TF：term frequency（词频）
IDF：inverse document frequency（逆文档频率）
字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。

$TF=\frac{某个词在文章中出现的次数}{文章的总词数}$
考虑到文章长短不同，除以总词数进行标准化

$IDF=log(\frac{语料库的文章总数}{包含该词的文章数量+1})$
加1防止不存在包含该词的文档时分母为0

$TF-IDF=TF \times IDF$

优点：

简单快速、易理解

缺点：

没考虑词语的语义
仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。
没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。


1.2自己实现
import math


# 计算每句话的词频
def counter(word_list):
    wordcount = []
    for doc in word_list:
        count = {}
        for word in doc:
            count[word] = count.get(word, 0) + 1
        wordcount.append(count)
    return wordcount


# 计算tf=某个词在文章中出现的总次数/文章的总词数
def tf(word, word_list):
    return word_list.get(word) / sum(word_list.values())


# 统计含有该单词的句子数
def count_sentence(word, wordcount):
    return sum(1 for i in wordcount if i.get(word))


# 计算idf=log(语料库中的文档总数/(包含该词的文档数+1))
def idf(word, wordcount):
    return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1


# tf-idf=tf*idf
def tfidf(word, word_list, wordcount):
    return tf(word, word_list) * idf(word, wordcount)


if __name__ == "__main__":
    docs = [
        "what is the weather like today",
        "what is for dinner tonight",
        "this is a question worth pondering",
        "it is a beautiful day today"
    ]

    word_list = []  # 记录每个文档分词后的结果
    for doc in docs:
        word_list.append(doc.split(" "))

    # 使用停用词
    # stopwords = ["is", "the"]
    # for i in docs:
    #     all_words = i.split()
    #     new_words = []
    #     for j in all_words:
    #         if j not in stopwords:
    #             new_words.append(j)
    #     word_list.append(new_words)

    wordcount = counter(word_list)  # 统计每个文档词的次数

    for cnt, doc in enumerate(wordcount):
        print("doc{}".format(cnt))
        for word, _ in doc.items():
            print("word:{} --- TF-IDF:{}".format(word, tfidf(word, doc, wordcount)))

1.3使用sklearn库
from sklearn.feature_extraction.text import TfidfVectorizer


if __name__ == "__main__":
    docs = [
        "what is the weather like today",
        "what is for dinner tonight",
        "this is a question worth pondering",
        "it is a beautiful day today"
    ]

    tfidf_vec = TfidfVectorizer()

    # 利用fit_transform得到TFIDF矩阵
    tfidf_matrix = tfidf_vec.fit_transform(docs)

    # 利用get_feature_names_out得到不重复的单词
    print(tfidf_vec.get_feature_names_out())

    # 利用vocabulary_得到各单次的编号
    print(tfidf_vec.vocabulary_)

    # 输出TFIDF矩阵，即每个文档中每个词的tfidf值
    print(tfidf_matrix)

2.BM25
2.1原理
BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。'><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/"><meta property="og:site_name" content="Chan's Blog"><meta property="og:title" content="Retrieval_Learning"><meta property="og:description" content='简介：检索相关算法的学习
1.TF-IDF 1.1原理 TF：term frequency（词频）
IDF：inverse document frequency（逆文档频率）
字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。
$TF=\frac{某个词在文章中出现的次数}{文章的总词数}$
考虑到文章长短不同，除以总词数进行标准化
$IDF=log(\frac{语料库的文章总数}{包含该词的文章数量+1})$
加1防止不存在包含该词的文档时分母为0
$TF-IDF=TF \times IDF$
优点：
简单快速、易理解 缺点：
没考虑词语的语义 仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 1.2自己实现 import math # 计算每句话的词频 def counter(word_list): wordcount = [] for doc in word_list: count = {} for word in doc: count[word] = count.get(word, 0) + 1 wordcount.append(count) return wordcount # 计算tf=某个词在文章中出现的总次数/文章的总词数 def tf(word, word_list): return word_list.get(word) / sum(word_list.values()) # 统计含有该单词的句子数 def count_sentence(word, wordcount): return sum(1 for i in wordcount if i.get(word)) # 计算idf=log(语料库中的文档总数/(包含该词的文档数+1)) def idf(word, wordcount): return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1 # tf-idf=tf*idf def tfidf(word, word_list, wordcount): return tf(word, word_list) * idf(word, wordcount) if __name__ == "__main__": docs = [ "what is the weather like today", "what is for dinner tonight", "this is a question worth pondering", "it is a beautiful day today" ] word_list = [] # 记录每个文档分词后的结果 for doc in docs: word_list.append(doc.split(" ")) # 使用停用词 # stopwords = ["is", "the"] # for i in docs: # all_words = i.split() # new_words = [] # for j in all_words: # if j not in stopwords: # new_words.append(j) # word_list.append(new_words) wordcount = counter(word_list) # 统计每个文档词的次数 for cnt, doc in enumerate(wordcount): print("doc{}".format(cnt)) for word, _ in doc.items(): print("word:{} --- TF-IDF:{}".format(word, tfidf(word, doc, wordcount))) 1.3使用sklearn库 from sklearn.feature_extraction.text import TfidfVectorizer if __name__ == "__main__": docs = [ "what is the weather like today", "what is for dinner tonight", "this is a question worth pondering", "it is a beautiful day today" ] tfidf_vec = TfidfVectorizer() # 利用fit_transform得到TFIDF矩阵 tfidf_matrix = tfidf_vec.fit_transform(docs) # 利用get_feature_names_out得到不重复的单词 print(tfidf_vec.get_feature_names_out()) # 利用vocabulary_得到各单次的编号 print(tfidf_vec.vocabulary_) # 输出TFIDF矩阵，即每个文档中每个词的tfidf值 print(tfidf_matrix) 2.BM25 2.1原理 BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-07T10:07:00+08:00"><meta property="article:modified_time" content="2025-07-07T10:07:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Retrieval_Learning"><meta name=twitter:description content='简介：检索相关算法的学习

1.TF-IDF
1.1原理
TF：term frequency（词频）
IDF：inverse document frequency（逆文档频率）
字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。

$TF=\frac{某个词在文章中出现的次数}{文章的总词数}$
考虑到文章长短不同，除以总词数进行标准化

$IDF=log(\frac{语料库的文章总数}{包含该词的文章数量+1})$
加1防止不存在包含该词的文档时分母为0

$TF-IDF=TF \times IDF$

优点：

简单快速、易理解

缺点：

没考虑词语的语义
仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。
没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。


1.2自己实现
import math


# 计算每句话的词频
def counter(word_list):
    wordcount = []
    for doc in word_list:
        count = {}
        for word in doc:
            count[word] = count.get(word, 0) + 1
        wordcount.append(count)
    return wordcount


# 计算tf=某个词在文章中出现的总次数/文章的总词数
def tf(word, word_list):
    return word_list.get(word) / sum(word_list.values())


# 统计含有该单词的句子数
def count_sentence(word, wordcount):
    return sum(1 for i in wordcount if i.get(word))


# 计算idf=log(语料库中的文档总数/(包含该词的文档数+1))
def idf(word, wordcount):
    return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1


# tf-idf=tf*idf
def tfidf(word, word_list, wordcount):
    return tf(word, word_list) * idf(word, wordcount)


if __name__ == "__main__":
    docs = [
        "what is the weather like today",
        "what is for dinner tonight",
        "this is a question worth pondering",
        "it is a beautiful day today"
    ]

    word_list = []  # 记录每个文档分词后的结果
    for doc in docs:
        word_list.append(doc.split(" "))

    # 使用停用词
    # stopwords = ["is", "the"]
    # for i in docs:
    #     all_words = i.split()
    #     new_words = []
    #     for j in all_words:
    #         if j not in stopwords:
    #             new_words.append(j)
    #     word_list.append(new_words)

    wordcount = counter(word_list)  # 统计每个文档词的次数

    for cnt, doc in enumerate(wordcount):
        print("doc{}".format(cnt))
        for word, _ in doc.items():
            print("word:{} --- TF-IDF:{}".format(word, tfidf(word, doc, wordcount)))

1.3使用sklearn库
from sklearn.feature_extraction.text import TfidfVectorizer


if __name__ == "__main__":
    docs = [
        "what is the weather like today",
        "what is for dinner tonight",
        "this is a question worth pondering",
        "it is a beautiful day today"
    ]

    tfidf_vec = TfidfVectorizer()

    # 利用fit_transform得到TFIDF矩阵
    tfidf_matrix = tfidf_vec.fit_transform(docs)

    # 利用get_feature_names_out得到不重复的单词
    print(tfidf_vec.get_feature_names_out())

    # 利用vocabulary_得到各单次的编号
    print(tfidf_vec.vocabulary_)

    # 输出TFIDF矩阵，即每个文档中每个词的tfidf值
    print(tfidf_matrix)

2.BM25
2.1原理
BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Retrieval_Learning","item":"https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Retrieval_Learning","name":"Retrieval_Learning","description":"简介：检索相关算法的学习\n1.TF-IDF 1.1原理 TF：term frequency（词频）\nIDF：inverse document frequency（逆文档频率）\n字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\nTF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n$TF=\\frac{某个词在文章中出现的次数}{文章的总词数}$\n考虑到文章长短不同，除以总词数进行标准化\n$IDF=log(\\frac{语料库的文章总数}{包含该词的文章数量+1})$\n加1防止不存在包含该词的文档时分母为0\n$TF-IDF=TF \\times IDF$\n优点：\n简单快速、易理解 缺点：\n没考虑词语的语义 仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 1.2自己实现 import math # 计算每句话的词频 def counter(word_list): wordcount = [] for doc in word_list: count = {} for word in doc: count[word] = count.get(word, 0) + 1 wordcount.append(count) return wordcount # 计算tf=某个词在文章中出现的总次数/文章的总词数 def tf(word, word_list): return word_list.get(word) / sum(word_list.values()) # 统计含有该单词的句子数 def count_sentence(word, wordcount): return sum(1 for i in wordcount if i.get(word)) # 计算idf=log(语料库中的文档总数/(包含该词的文档数+1)) def idf(word, wordcount): return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1 # tf-idf=tf*idf def tfidf(word, word_list, wordcount): return tf(word, word_list) * idf(word, wordcount) if __name__ == \u0026#34;__main__\u0026#34;: docs = [ \u0026#34;what is the weather like today\u0026#34;, \u0026#34;what is for dinner tonight\u0026#34;, \u0026#34;this is a question worth pondering\u0026#34;, \u0026#34;it is a beautiful day today\u0026#34; ] word_list = [] # 记录每个文档分词后的结果 for doc in docs: word_list.append(doc.split(\u0026#34; \u0026#34;)) # 使用停用词 # stopwords = [\u0026#34;is\u0026#34;, \u0026#34;the\u0026#34;] # for i in docs: # all_words = i.split() # new_words = [] # for j in all_words: # if j not in stopwords: # new_words.append(j) # word_list.append(new_words) wordcount = counter(word_list) # 统计每个文档词的次数 for cnt, doc in enumerate(wordcount): print(\u0026#34;doc{}\u0026#34;.format(cnt)) for word, _ in doc.items(): print(\u0026#34;word:{} --- TF-IDF:{}\u0026#34;.format(word, tfidf(word, doc, wordcount))) 1.3使用sklearn库 from sklearn.feature_extraction.text import TfidfVectorizer if __name__ == \u0026#34;__main__\u0026#34;: docs = [ \u0026#34;what is the weather like today\u0026#34;, \u0026#34;what is for dinner tonight\u0026#34;, \u0026#34;this is a question worth pondering\u0026#34;, \u0026#34;it is a beautiful day today\u0026#34; ] tfidf_vec = TfidfVectorizer() # 利用fit_transform得到TFIDF矩阵 tfidf_matrix = tfidf_vec.fit_transform(docs) # 利用get_feature_names_out得到不重复的单词 print(tfidf_vec.get_feature_names_out()) # 利用vocabulary_得到各单次的编号 print(tfidf_vec.vocabulary_) # 输出TFIDF矩阵，即每个文档中每个词的tfidf值 print(tfidf_matrix) 2.BM25 2.1原理 BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。\n","keywords":[],"articleBody":"简介：检索相关算法的学习\n1.TF-IDF 1.1原理 TF：term frequency（词频）\nIDF：inverse document frequency（逆文档频率）\n字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\nTF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n$TF=\\frac{某个词在文章中出现的次数}{文章的总词数}$\n考虑到文章长短不同，除以总词数进行标准化\n$IDF=log(\\frac{语料库的文章总数}{包含该词的文章数量+1})$\n加1防止不存在包含该词的文档时分母为0\n$TF-IDF=TF \\times IDF$\n优点：\n简单快速、易理解 缺点：\n没考虑词语的语义 仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。 没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。 1.2自己实现 import math # 计算每句话的词频 def counter(word_list): wordcount = [] for doc in word_list: count = {} for word in doc: count[word] = count.get(word, 0) + 1 wordcount.append(count) return wordcount # 计算tf=某个词在文章中出现的总次数/文章的总词数 def tf(word, word_list): return word_list.get(word) / sum(word_list.values()) # 统计含有该单词的句子数 def count_sentence(word, wordcount): return sum(1 for i in wordcount if i.get(word)) # 计算idf=log(语料库中的文档总数/(包含该词的文档数+1)) def idf(word, wordcount): return math.log(len(wordcount) + 1 / count_sentence(word, wordcount) + 1) + 1 # tf-idf=tf*idf def tfidf(word, word_list, wordcount): return tf(word, word_list) * idf(word, wordcount) if __name__ == \"__main__\": docs = [ \"what is the weather like today\", \"what is for dinner tonight\", \"this is a question worth pondering\", \"it is a beautiful day today\" ] word_list = [] # 记录每个文档分词后的结果 for doc in docs: word_list.append(doc.split(\" \")) # 使用停用词 # stopwords = [\"is\", \"the\"] # for i in docs: # all_words = i.split() # new_words = [] # for j in all_words: # if j not in stopwords: # new_words.append(j) # word_list.append(new_words) wordcount = counter(word_list) # 统计每个文档词的次数 for cnt, doc in enumerate(wordcount): print(\"doc{}\".format(cnt)) for word, _ in doc.items(): print(\"word:{} --- TF-IDF:{}\".format(word, tfidf(word, doc, wordcount))) 1.3使用sklearn库 from sklearn.feature_extraction.text import TfidfVectorizer if __name__ == \"__main__\": docs = [ \"what is the weather like today\", \"what is for dinner tonight\", \"this is a question worth pondering\", \"it is a beautiful day today\" ] tfidf_vec = TfidfVectorizer() # 利用fit_transform得到TFIDF矩阵 tfidf_matrix = tfidf_vec.fit_transform(docs) # 利用get_feature_names_out得到不重复的单词 print(tfidf_vec.get_feature_names_out()) # 利用vocabulary_得到各单次的编号 print(tfidf_vec.vocabulary_) # 输出TFIDF矩阵，即每个文档中每个词的tfidf值 print(tfidf_matrix) 2.BM25 2.1原理 BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。\n对于TF-IDF算法，如果文档长，词语多，TF值就会很大。BM25通过b参数对文档长度进行打压，随着TF的增加，BM25score趋于稳定，如下图所示。\n符号说明:\n$ Q $: 查询，由一组词项组成 $ Q = (q_1, q_2, ..., q_n) $ $ D $: 文档 $ f_i $: 词项 $ q_i $ 在文档 $ D $ 中的出现频率（词频，TF） $ dl $: 文档长度（文档中的词项总数） $ avgdl $: 语料库中所有文档的平均长度 $ N $: 语料库中文档总数 $ n_i $: 包含词项 $ q_i $ 的文档数量 $ k $: 词频调节参数（通常取1.2~2.0） $ b $: 文档长度调节参数（通常取0.75） 参数 典型值 作用 $ k $ 1.2~2.0 控制词频饱和度，值越大饱和度变化越慢 $ b $ 0.75 控制文档长度归一化程度，0=不归一化，1=完全归一化 b 参数默认 0.75，主要是对长文档做惩罚。如果设置为 0，分数将与文档的长度无关。从下图可以看到，b=0时，L与分数无关，b=1时，L越大，分数打压越厉害。\n完整公式：\n$$ \\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\frac{{(k+1) \\cdot f_i}}{{f_i + k \\cdot (1 - b + b \\cdot \\frac{{\\lvert D \\rvert}}{{\\text{avgdl}}})}} \\cdot \\log\\left(\\frac{{N - n_i + 0.5}}{{n_i + 0.5}}\\right) $$单个词项得分\n对于查询中的单个词项 $ q_i $，其BM25得分为：\n$\\text{score}(q_i, D) = \\text{IDF}(q_i) \\cdot \\frac{f_i \\cdot (k_1 + 1)}{f_i + k_1 \\cdot (1 - b + b \\cdot \\frac{dl}{avgdl})}$\nIDF计算\n逆文档频率（IDF）的计算公式为：\n$\\text{IDF}(q_i) = \\log \\left( \\frac{N - n_i + 0.5}{n_i + 0.5} + 1 \\right)$\n整体查询得分\n对于完整查询 $ Q $，BM25得分为所有词项得分的总和： $\\text{BM25}(Q, D) = \\sum_{i=1}^{n} \\text{score}(q_i, D)$\n2.2自己实现 import math from collections import Counter class BM25: def __init__(self, docs, k1=1.5, b=0.75): \"\"\" BM25算法的构造器 :param docs: 分词后的文档列表，每个文档是一个包含词汇的列表 :param k1: BM25算法中的调节参数k1 :param b: BM25算法中的调节参数b \"\"\" self.docs = docs self.k1 = k1 self.b = b self.doc_len = [len(doc) for doc in docs] # 计算每个文档的长度 self.avgdl = sum(self.doc_len) / len(docs) # 计算所有文档的平均长度 self.doc_freqs = [] # 存储每个文档的词频 self.idf = {} # 存储每个词的逆文档频率 self.initialize() def initialize(self): \"\"\" 初始化方法，计算所有词的逆文档频率 \"\"\" df = {} # 用于存储每个词在多少不同文档中出现 for doc in self.docs: # 为每个文档创建一个词频统计 self.doc_freqs.append(Counter(doc)) # 更新df值 for word in set(doc): df[word] = df.get(word, 0) + 1 # 计算每个词的IDF值（出现的文档数越少，得分越高） for word, freq in df.items(): self.idf[word] = math.log((len(self.docs) - freq + 0.5) / (freq + 0.5) + 1) def score(self, doc, query): \"\"\" 计算文档与查询的BM25得分 :param doc: 文档的索引 :param query: 查询词列表 :return: 该文档与查询的相关性得分 \"\"\" score = 0.0 for word in query: if word in self.doc_freqs[doc]: freq = self.doc_freqs[doc][word] # 词在文档中的频率 # 应用BM25计算公式 score += (self.idf[word] * freq * (self.k1 + 1)) / ( freq + self.k1 * (1 - self.b + self.b * self.doc_len[doc] / self.avgdl)) return score if __name__ == \"__main__\": # 示例文档集和查询 docs = [[\"the\", \"quick\", \"brown\", \"fox\"], [\"the\", \"lazy\", \"dog\"], [\"the\", \"quick\", \"dog\"], [\"the\", \"quick\", \"brown\", \"brown\", \"fox\"]] query = [\"quick\", \"brown\"] # 初始化BM25模型并计算得分 bm25 = BM25(docs) scores = [bm25.score(i, query) for i in range(len(docs))] print(scores) 2.3rank_bm25库 from rank_bm25 import BM25Okapi import jieba corpus = [ \"BM25是一种常用的信息检索算法\", \"这个Python库实现了BM25算法\", \"信息检索是搜索引擎的核心技术\", \"BM25比传统的TF-IDF效果更好\", \"中文信息检索需要先进行分词处理\", \"自然语言处理是人工智能的重要领域\", \"Python是最受欢迎的编程语言之一\", ] tokenized_corpus = [list(jieba.cut(doc)) for doc in corpus] bm25 = BM25Okapi(tokenized_corpus) def search(query, top_n=3): tokenized_query = list(jieba.cut(query)) doc_scores = bm25.get_scores(tokenized_query) top_docs = bm25.get_top_n(tokenized_query, corpus, n=top_n) return doc_scores, top_docs query = \"Python信息检索\" print(f\"查询: '{query}'\") scores, results = search(query) print(\"\\n相关文档:\") for i, (score, doc) in enumerate(zip(scores, corpus)): print(f\"{i}. [Score: {score:.2f}] {doc}\") print(\"\\n最相关的3个文档:\") for i, doc in enumerate(results): print(f\"{i+1}. {doc}\") 2.4bm25s 一个快速的bm25实现方法，在大数据上比其它实现要快，很好用。\n项目地址：xhluca/bm25s: Fast lexical search implementing BM25 in Python using Numpy, Numba and Scipy\n","wordCount":"714","inLanguage":"en","datePublished":"2025-07-07T10:07:00+08:00","dateModified":"2025-07-07T10:07:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/%E6%A3%80%E7%B4%A2/"},"publisher":{"@type":"Organization","name":"Chan's Blog","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/apple-touch-icon.png"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="Chan's Blog (Alt + H)"><img src=https://Rook1eChan.github.io/apple-touch-icon.png alt aria-label=logo height=35>Chan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Retrieval_Learning</h1><div class=post-meta><span title='2025-07-07 10:07:00 +0800 +0800'>July 7, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1tf-idf aria-label=1.TF-IDF>1.TF-IDF</a><ul><li><a href=#11%e5%8e%9f%e7%90%86 aria-label=1.1原理>1.1原理</a></li><li><a href=#12%e8%87%aa%e5%b7%b1%e5%ae%9e%e7%8e%b0 aria-label=1.2自己实现>1.2自己实现</a></li><li><a href=#13%e4%bd%bf%e7%94%a8sklearn%e5%ba%93 aria-label=1.3使用sklearn库>1.3使用sklearn库</a></li></ul></li><li><a href=#2bm25 aria-label=2.BM25>2.BM25</a><ul><li><a href=#21%e5%8e%9f%e7%90%86 aria-label=2.1原理>2.1原理</a></li><li><a href=#22%e8%87%aa%e5%b7%b1%e5%ae%9e%e7%8e%b0 aria-label=2.2自己实现>2.2自己实现</a></li><li><a href=#23rank_bm25%e5%ba%93 aria-label=2.3rank_bm25库>2.3rank_bm25库</a></li><li><a href=#24bm25s aria-label=2.4bm25s>2.4bm25s</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p><em>简介：检索相关算法的学习</em></p><br><h1 id=1tf-idf>1.TF-IDF<a hidden class=anchor aria-hidden=true href=#1tf-idf>#</a></h1><h2 id=11原理>1.1原理<a hidden class=anchor aria-hidden=true href=#11原理>#</a></h2><p>TF：term frequency（词频）</p><p>IDF：inverse document frequency（逆文档频率）</p><p><strong>字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</strong></p><p><strong>TF-IDF的主要思想是</strong>：如果某个单词在一篇文章中出现的频率高（TF高），并且在其他文章中很少出现（IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p><br><p>$TF=\frac{某个词在文章中出现的次数}{文章的总词数}$</p><p>考虑到文章长短不同，除以总词数进行标准化</p><br><p>$IDF=log(\frac{语料库的文章总数}{包含该词的文章数量+1})$</p><p>加1防止不存在包含该词的文档时分母为0</p><br><p>$TF-IDF=TF \times IDF$</p><br><p><strong>优点：</strong></p><ol><li>简单快速、易理解</li></ol><p><strong>缺点：</strong></p><ol><li>没考虑词语的语义</li><li>仅用词频考虑词语的重要性不够全面。按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。</li><li>没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。</li></ol><br><h2 id=12自己实现>1.2自己实现<a hidden class=anchor aria-hidden=true href=#12自己实现>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 计算每句话的词频</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>counter</span><span class=p>(</span><span class=n>word_list</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>wordcount</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>word_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>count</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>doc</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>count</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>=</span> <span class=n>count</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=n>wordcount</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>count</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>wordcount</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 计算tf=某个词在文章中出现的总次数/文章的总词数</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>tf</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>word_list</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>word_list</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>word</span><span class=p>)</span> <span class=o>/</span> <span class=nb>sum</span><span class=p>(</span><span class=n>word_list</span><span class=o>.</span><span class=n>values</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 统计含有该单词的句子数</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>count_sentence</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>wordcount</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=mi>1</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>wordcount</span> <span class=k>if</span> <span class=n>i</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>word</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 计算idf=log(语料库中的文档总数/(包含该词的文档数+1))</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>idf</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>wordcount</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>wordcount</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span> <span class=o>/</span> <span class=n>count_sentence</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>wordcount</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># tf-idf=tf*idf</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>tfidf</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>word_list</span><span class=p>,</span> <span class=n>wordcount</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tf</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>word_list</span><span class=p>)</span> <span class=o>*</span> <span class=n>idf</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>wordcount</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>docs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;what is the weather like today&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;what is for dinner tonight&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;this is a question worth pondering&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;it is a beautiful day today&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>word_list</span> <span class=o>=</span> <span class=p>[]</span>  <span class=c1># 记录每个文档分词后的结果</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>word_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>doc</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34; &#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 使用停用词</span>
</span></span><span class=line><span class=cl>    <span class=c1># stopwords = [&#34;is&#34;, &#34;the&#34;]</span>
</span></span><span class=line><span class=cl>    <span class=c1># for i in docs:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#     all_words = i.split()</span>
</span></span><span class=line><span class=cl>    <span class=c1>#     new_words = []</span>
</span></span><span class=line><span class=cl>    <span class=c1>#     for j in all_words:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#         if j not in stopwords:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#             new_words.append(j)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#     word_list.append(new_words)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>wordcount</span> <span class=o>=</span> <span class=n>counter</span><span class=p>(</span><span class=n>word_list</span><span class=p>)</span>  <span class=c1># 统计每个文档词的次数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>cnt</span><span class=p>,</span> <span class=n>doc</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>wordcount</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;doc</span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>cnt</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>doc</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;word:</span><span class=si>{}</span><span class=s2> --- TF-IDF:</span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>tfidf</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>doc</span><span class=p>,</span> <span class=n>wordcount</span><span class=p>)))</span>
</span></span></code></pre></div><br><h2 id=13使用sklearn库>1.3使用sklearn库<a hidden class=anchor aria-hidden=true href=#13使用sklearn库>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>TfidfVectorizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>docs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;what is the weather like today&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;what is for dinner tonight&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;this is a question worth pondering&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;it is a beautiful day today&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>tfidf_vec</span> <span class=o>=</span> <span class=n>TfidfVectorizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 利用fit_transform得到TFIDF矩阵</span>
</span></span><span class=line><span class=cl>    <span class=n>tfidf_matrix</span> <span class=o>=</span> <span class=n>tfidf_vec</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 利用get_feature_names_out得到不重复的单词</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>tfidf_vec</span><span class=o>.</span><span class=n>get_feature_names_out</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 利用vocabulary_得到各单次的编号</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>tfidf_vec</span><span class=o>.</span><span class=n>vocabulary_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 输出TFIDF矩阵，即每个文档中每个词的tfidf值</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>tfidf_matrix</span><span class=p>)</span>
</span></span></code></pre></div><br><h1 id=2bm25>2.BM25<a hidden class=anchor aria-hidden=true href=#2bm25>#</a></h1><h2 id=21原理>2.1原理<a hidden class=anchor aria-hidden=true href=#21原理>#</a></h2><p>BM25是一种基于概率检索框架的排序函数，用于计算查询（Query）与文档（Document）的相关性得分。</p><p>对于TF-IDF算法，如果文档长，词语多，TF值就会很大。BM25通过b参数对文档长度进行打压，随着TF的增加，BM25score趋于稳定，如下图所示。</p><p><img alt=P1 loading=lazy src=/retrieval_learning/p1.png></p><br><p>符号说明:</p><ul><li>$ Q $: 查询，由一组词项组成 $ Q = (q_1, q_2, ..., q_n) $</li><li>$ D $: 文档</li><li>$ f_i $: 词项 $ q_i $ 在文档 $ D $ 中的出现频率（词频，TF）</li><li>$ dl $: 文档长度（文档中的词项总数）</li><li>$ avgdl $: 语料库中所有文档的平均长度</li><li>$ N $: 语料库中文档总数</li><li>$ n_i $: 包含词项 $ q_i $ 的文档数量</li><li>$ k $: 词频调节参数（通常取1.2~2.0）</li><li>$ b $: 文档长度调节参数（通常取0.75）<table><thead><tr><th>参数</th><th>典型值</th><th>作用</th></tr></thead><tbody><tr><td>$ k $</td><td>1.2~2.0</td><td>控制词频饱和度，值越大饱和度变化越慢</td></tr><tr><td>$ b $</td><td>0.75</td><td>控制文档长度归一化程度，0=不归一化，1=完全归一化</td></tr></tbody></table></li></ul><p>b 参数默认 0.75，主要是对长文档做惩罚。如果设置为 0，分数将与文档的长度无关。从下图可以看到，b=0时，L与分数无关，b=1时，L越大，分数打压越厉害。</p><p><img alt=P2 loading=lazy src=/retrieval_learning/p2.png></p><br><p><strong>完整公式：</strong></p>$$ \text{BM25}(D, Q) = \sum_{i=1}^{n} \frac{{(k+1) \cdot f_i}}{{f_i + k \cdot (1 - b + b \cdot \frac{{\lvert D \rvert}}{{\text{avgdl}}})}} \cdot \log\left(\frac{{N - n_i + 0.5}}{{n_i + 0.5}}\right) $$<p><strong>单个词项得分</strong></p><p>对于查询中的单个词项 $ q_i $，其BM25得分为：</p><p>$\text{score}(q_i, D) = \text{IDF}(q_i) \cdot \frac{f_i \cdot (k_1 + 1)}{f_i + k_1 \cdot (1 - b + b \cdot \frac{dl}{avgdl})}$</p><p><strong>IDF计算</strong></p><p>逆文档频率（IDF）的计算公式为：</p><p>$\text{IDF}(q_i) = \log \left( \frac{N - n_i + 0.5}{n_i + 0.5} + 1 \right)$</p><p><strong>整体查询得分</strong></p><p>对于完整查询 $ Q $，BM25得分为所有词项得分的总和：
$\text{BM25}(Q, D) = \sum_{i=1}^{n} \text{score}(q_i, D)$</p><br><h2 id=22自己实现>2.2自己实现<a hidden class=anchor aria-hidden=true href=#22自己实现>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>Counter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BM25</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>docs</span><span class=p>,</span> <span class=n>k1</span><span class=o>=</span><span class=mf>1.5</span><span class=p>,</span> <span class=n>b</span><span class=o>=</span><span class=mf>0.75</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        BM25算法的构造器
</span></span></span><span class=line><span class=cl><span class=s2>        :param docs: 分词后的文档列表，每个文档是一个包含词汇的列表
</span></span></span><span class=line><span class=cl><span class=s2>        :param k1: BM25算法中的调节参数k1
</span></span></span><span class=line><span class=cl><span class=s2>        :param b: BM25算法中的调节参数b
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>docs</span> <span class=o>=</span> <span class=n>docs</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k1</span> <span class=o>=</span> <span class=n>k1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>b</span> <span class=o>=</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>doc_len</span> <span class=o>=</span> <span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>doc</span><span class=p>)</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>]</span>  <span class=c1># 计算每个文档的长度</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>avgdl</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>doc_len</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>  <span class=c1># 计算所有文档的平均长度</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>doc_freqs</span> <span class=o>=</span> <span class=p>[]</span>  <span class=c1># 存储每个文档的词频</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>idf</span> <span class=o>=</span> <span class=p>{}</span>  <span class=c1># 存储每个词的逆文档频率</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>initialize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>initialize</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        初始化方法，计算所有词的逆文档频率
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>df</span> <span class=o>=</span> <span class=p>{}</span>  <span class=c1># 用于存储每个词在多少不同文档中出现</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>docs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 为每个文档创建一个词频统计</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>doc_freqs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>Counter</span><span class=p>(</span><span class=n>doc</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=c1># 更新df值</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=nb>set</span><span class=p>(</span><span class=n>doc</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>df</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算每个词的IDF值（出现的文档数越少，得分越高）</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>df</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>idf</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>docs</span><span class=p>)</span> <span class=o>-</span> <span class=n>freq</span> <span class=o>+</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>freq</span> <span class=o>+</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>score</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>doc</span><span class=p>,</span> <span class=n>query</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        计算文档与查询的BM25得分
</span></span></span><span class=line><span class=cl><span class=s2>        :param doc: 文档的索引
</span></span></span><span class=line><span class=cl><span class=s2>        :param query: 查询词列表
</span></span></span><span class=line><span class=cl><span class=s2>        :return: 该文档与查询的相关性得分
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>query</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>word</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>doc_freqs</span><span class=p>[</span><span class=n>doc</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=n>freq</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>doc_freqs</span><span class=p>[</span><span class=n>doc</span><span class=p>][</span><span class=n>word</span><span class=p>]</span>  <span class=c1># 词在文档中的频率</span>
</span></span><span class=line><span class=cl>                <span class=c1># 应用BM25计算公式</span>
</span></span><span class=line><span class=cl>                <span class=n>score</span> <span class=o>+=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>idf</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>*</span> <span class=n>freq</span> <span class=o>*</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k1</span> <span class=o>+</span> <span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>                            <span class=n>freq</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>k1</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>b</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>b</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>doc_len</span><span class=p>[</span><span class=n>doc</span><span class=p>]</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>avgdl</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>score</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 示例文档集和查询</span>
</span></span><span class=line><span class=cl>    <span class=n>docs</span> <span class=o>=</span> <span class=p>[[</span><span class=s2>&#34;the&#34;</span><span class=p>,</span> <span class=s2>&#34;quick&#34;</span><span class=p>,</span> <span class=s2>&#34;brown&#34;</span><span class=p>,</span> <span class=s2>&#34;fox&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=s2>&#34;the&#34;</span><span class=p>,</span> <span class=s2>&#34;lazy&#34;</span><span class=p>,</span> <span class=s2>&#34;dog&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=s2>&#34;the&#34;</span><span class=p>,</span> <span class=s2>&#34;quick&#34;</span><span class=p>,</span> <span class=s2>&#34;dog&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=s2>&#34;the&#34;</span><span class=p>,</span> <span class=s2>&#34;quick&#34;</span><span class=p>,</span> <span class=s2>&#34;brown&#34;</span><span class=p>,</span> <span class=s2>&#34;brown&#34;</span><span class=p>,</span> <span class=s2>&#34;fox&#34;</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;quick&#34;</span><span class=p>,</span> <span class=s2>&#34;brown&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化BM25模型并计算得分</span>
</span></span><span class=line><span class=cl>    <span class=n>bm25</span> <span class=o>=</span> <span class=n>BM25</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=p>[</span><span class=n>bm25</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>query</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>docs</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span>
</span></span></code></pre></div><br><h2 id=23rank_bm25库>2.3rank_bm25库<a hidden class=anchor aria-hidden=true href=#23rank_bm25库>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>rank_bm25</span> <span class=kn>import</span> <span class=n>BM25Okapi</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jieba</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>corpus</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;BM25是一种常用的信息检索算法&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;这个Python库实现了BM25算法&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;信息检索是搜索引擎的核心技术&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;BM25比传统的TF-IDF效果更好&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;中文信息检索需要先进行分词处理&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;自然语言处理是人工智能的重要领域&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Python是最受欢迎的编程语言之一&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenized_corpus</span> <span class=o>=</span> <span class=p>[</span><span class=nb>list</span><span class=p>(</span><span class=n>jieba</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=n>doc</span><span class=p>))</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>corpus</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>bm25</span> <span class=o>=</span> <span class=n>BM25Okapi</span><span class=p>(</span><span class=n>tokenized_corpus</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>search</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>top_n</span><span class=o>=</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenized_query</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>jieba</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=n>query</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>doc_scores</span> <span class=o>=</span> <span class=n>bm25</span><span class=o>.</span><span class=n>get_scores</span><span class=p>(</span><span class=n>tokenized_query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>top_docs</span> <span class=o>=</span> <span class=n>bm25</span><span class=o>.</span><span class=n>get_top_n</span><span class=p>(</span><span class=n>tokenized_query</span><span class=p>,</span> <span class=n>corpus</span><span class=p>,</span> <span class=n>n</span><span class=o>=</span><span class=n>top_n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>doc_scores</span><span class=p>,</span> <span class=n>top_docs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;Python信息检索&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;查询: &#39;</span><span class=si>{</span><span class=n>query</span><span class=si>}</span><span class=s2>&#39;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scores</span><span class=p>,</span> <span class=n>results</span> <span class=o>=</span> <span class=n>search</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>相关文档:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>doc</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>corpus</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>. [Score: </span><span class=si>{</span><span class=n>score</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>] </span><span class=si>{</span><span class=n>doc</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>最相关的3个文档:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>doc</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>results</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>. </span><span class=si>{</span><span class=n>doc</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span></code></pre></div><h2 id=24bm25s>2.4bm25s<a hidden class=anchor aria-hidden=true href=#24bm25s>#</a></h2><p>一个快速的bm25实现方法，在大数据上比其它实现要快，很好用。</p><p>项目地址：<a href=https://github.com/xhluca/bm25s>xhluca/bm25s: Fast lexical search implementing BM25 in Python using Numpy, Numba and Scipy</a></p><br></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>Chan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>