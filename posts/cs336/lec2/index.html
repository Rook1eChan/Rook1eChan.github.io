<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>CS336 Lec2 | 陈</title><meta name=keywords content><meta name=description content='
CS336: Language Models From Scratch (Spring 2025)
1. Memory accounting
float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。
内存使用的估算：
x = torch.zeros(4, 8)  # 建立矩阵
assert x.dtype == torch.float32  # Default type
assert x.numel() == 4 * 8
assert x.element_size() == 4  # Float is 4 bytes
assert get_memory_usage(x) == 4 * 8 * 4  # 128 bytes

text("One matrix in the feedforward layer of GPT-3:")
assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024  # 2.3 GB
...which is a lot!
float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。
bfloat16，bf16，符号1位，指数8位，尾数7位，和fp16使用内存相同，但是指数范围和fp32相同。
fp8，8位，有E4M3、E5M2两种形式。
训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。

2. Compute accounting
为了利用GPU的并行计算能力，需要将数据迁移到GPU上。
text("By default, tensors are stored in CPU memory.")
x = torch.zeros(32, 32)
assert x.device == torch.device("cpu")

text("However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.")

text("Move the tensor to GPU memory (device 0).")
y = x.to("cuda:0")
assert y.device == torch.device("cuda", 0)

text("Or create a tensor directly on the GPU:")
z = torch.zeros(32, 32, device="cuda:0")

new_memory_allocated = torch.cuda.memory_allocated()  # @inspect new_memory_allocated
memory_used = new_memory_allocated - memory_allocated  # @inspect memory_used
assert memory_used == 2 * (32 * 32 * 4)  # 2 32x32 matrices of 4-byte floats
 
2.1 tensor
Pytorch中的tensor是一个指向具体内存的指针，并且带有步长，辅助寻找元素。'><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/cs336/lec2/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/cs336/lec2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/cs336/lec2/"><meta property="og:site_name" content="陈"><meta property="og:title" content="CS336 Lec2"><meta property="og:description" content=' CS336: Language Models From Scratch (Spring 2025)
1. Memory accounting float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。
内存使用的估算：
x = torch.zeros(4, 8) # 建立矩阵 assert x.dtype == torch.float32 # Default type assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes text("One matrix in the feedforward layer of GPT-3:") assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB ...which is a lot! float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。
bfloat16，bf16，符号1位，指数8位，尾数7位，和fp16使用内存相同，但是指数范围和fp32相同。
fp8，8位，有E4M3、E5M2两种形式。
训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。
2. Compute accounting 为了利用GPU的并行计算能力，需要将数据迁移到GPU上。
text("By default, tensors are stored in CPU memory.") x = torch.zeros(32, 32) assert x.device == torch.device("cpu") text("However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.") text("Move the tensor to GPU memory (device 0).") y = x.to("cuda:0") assert y.device == torch.device("cuda", 0) text("Or create a tensor directly on the GPU:") z = torch.zeros(32, 32, device="cuda:0") new_memory_allocated = torch.cuda.memory_allocated() # @inspect new_memory_allocated memory_used = new_memory_allocated - memory_allocated # @inspect memory_used assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats 2.1 tensor Pytorch中的tensor是一个指向具体内存的指针，并且带有步长，辅助寻找元素。'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-23T14:46:23+08:00"><meta property="article:modified_time" content="2025-09-23T14:46:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CS336 Lec2"><meta name=twitter:description content='
CS336: Language Models From Scratch (Spring 2025)
1. Memory accounting
float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。
内存使用的估算：
x = torch.zeros(4, 8)  # 建立矩阵
assert x.dtype == torch.float32  # Default type
assert x.numel() == 4 * 8
assert x.element_size() == 4  # Float is 4 bytes
assert get_memory_usage(x) == 4 * 8 * 4  # 128 bytes

text("One matrix in the feedforward layer of GPT-3:")
assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024  # 2.3 GB
...which is a lot!
float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。
bfloat16，bf16，符号1位，指数8位，尾数7位，和fp16使用内存相同，但是指数范围和fp32相同。
fp8，8位，有E4M3、E5M2两种形式。
训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。

2. Compute accounting
为了利用GPU的并行计算能力，需要将数据迁移到GPU上。
text("By default, tensors are stored in CPU memory.")
x = torch.zeros(32, 32)
assert x.device == torch.device("cpu")

text("However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.")

text("Move the tensor to GPU memory (device 0).")
y = x.to("cuda:0")
assert y.device == torch.device("cuda", 0)

text("Or create a tensor directly on the GPU:")
z = torch.zeros(32, 32, device="cuda:0")

new_memory_allocated = torch.cuda.memory_allocated()  # @inspect new_memory_allocated
memory_used = new_memory_allocated - memory_allocated  # @inspect memory_used
assert memory_used == 2 * (32 * 32 * 4)  # 2 32x32 matrices of 4-byte floats
 
2.1 tensor
Pytorch中的tensor是一个指向具体内存的指针，并且带有步长，辅助寻找元素。'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"CS336 Lec2","item":"https://Rook1eChan.github.io/posts/cs336/lec2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CS336 Lec2","name":"CS336 Lec2","description":" CS336: Language Models From Scratch (Spring 2025)\n1. Memory accounting float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。\n内存使用的估算：\nx = torch.zeros(4, 8) # 建立矩阵 assert x.dtype == torch.float32 # Default type assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes text(\u0026#34;One matrix in the feedforward layer of GPT-3:\u0026#34;) assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB ...which is a lot! float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。\nbfloat16，bf16，符号1位，指数8位，尾数7位，和fp16使用内存相同，但是指数范围和fp32相同。\nfp8，8位，有E4M3、E5M2两种形式。\n训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。\n2. Compute accounting 为了利用GPU的并行计算能力，需要将数据迁移到GPU上。\ntext(\u0026#34;By default, tensors are stored in CPU memory.\u0026#34;) x = torch.zeros(32, 32) assert x.device == torch.device(\u0026#34;cpu\u0026#34;) text(\u0026#34;However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.\u0026#34;) text(\u0026#34;Move the tensor to GPU memory (device 0).\u0026#34;) y = x.to(\u0026#34;cuda:0\u0026#34;) assert y.device == torch.device(\u0026#34;cuda\u0026#34;, 0) text(\u0026#34;Or create a tensor directly on the GPU:\u0026#34;) z = torch.zeros(32, 32, device=\u0026#34;cuda:0\u0026#34;) new_memory_allocated = torch.cuda.memory_allocated() # @inspect new_memory_allocated memory_used = new_memory_allocated - memory_allocated # @inspect memory_used assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats 2.1 tensor Pytorch中的tensor是一个指向具体内存的指针，并且带有步长，辅助寻找元素。\n","keywords":[],"articleBody":" CS336: Language Models From Scratch (Spring 2025)\n1. Memory accounting float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。\n内存使用的估算：\nx = torch.zeros(4, 8) # 建立矩阵 assert x.dtype == torch.float32 # Default type assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes text(\"One matrix in the feedforward layer of GPT-3:\") assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB ...which is a lot! float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。\nbfloat16，bf16，符号1位，指数8位，尾数7位，和fp16使用内存相同，但是指数范围和fp32相同。\nfp8，8位，有E4M3、E5M2两种形式。\n训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。\n2. Compute accounting 为了利用GPU的并行计算能力，需要将数据迁移到GPU上。\ntext(\"By default, tensors are stored in CPU memory.\") x = torch.zeros(32, 32) assert x.device == torch.device(\"cpu\") text(\"However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.\") text(\"Move the tensor to GPU memory (device 0).\") y = x.to(\"cuda:0\") assert y.device == torch.device(\"cuda\", 0) text(\"Or create a tensor directly on the GPU:\") z = torch.zeros(32, 32, device=\"cuda:0\") new_memory_allocated = torch.cuda.memory_allocated() # @inspect new_memory_allocated memory_used = new_memory_allocated - memory_allocated # @inspect memory_used assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats 2.1 tensor Pytorch中的tensor是一个指向具体内存的指针，并且带有步长，辅助寻找元素。\n某些操作，如切片、转置、改变形状，并不会产生一个新的数组，只是改变了数组的视图，例如\nx = torch.tensor([[1., 2, 3], [4, 5, 6]]) y = x.view(3, 2) x[0][0] = 100 assert y[0][0] == 100 x y 实际上是指向同一片内存的指针。\n但是，如果y等于x的转置，y就变成了一个不连续的tensor，连续访问行的时候，在内存上是跳跃访问的。对于不连续的tensor不能改变其视图，必须先变为连续的。\ny = x.transpose(1, 0).contiguous().view(2, 3) 对于改变tensor值的操作，会产生一个新的tensor。\n有时候我们想对成batch的数据进行处理，Pytorch支持这种方法。\nx = torch.ones(4, 8, 16, 32) w = torch.ones(32, 2) y = x @ w assert y.size() == torch.Size([4, 8, 16, 2]) 3. Einops Einops是一个用于操作张量的库，其中的维度都有明确的名称。它的设计灵感来自爱因斯坦求和符号。\nEinops教程\n3.1 Einops的设计动机 传统的PyTorch代码：\nx = torch.ones(2, 2, 3) # 分别表示batch, sequence, hidden维度 y = torch.ones(2, 2, 3) # 分别表示batch, sequence, hidden维度 z = x @ y.transpose(-2, -1) # 结果维度为batch, sequence, sequence 这种写法很容易搞混维度（比如-2和-1到底指的是什么维度？）\n3.2 Jaxtyping写法 如何跟踪张量的维度？\n旧方法：\nx = torch.ones(2, 2, 1, 3) # 代表batch seq heads hidden 新方法（使用jaxtyping）：\nx: Float[torch.Tensor, \"batch seq heads hidden\"] = torch.ones(2, 2, 1, 3) 注意：这只是一种文档说明方式（没有实际的强制检查）\n# 如果你想尝试einops，别忘了导入以下的库： import torch from jaxtyping import Float from einops import einsum, reduce, rearrange 3.3 Einops的einsum函数 einsum是一种广义的矩阵乘法，带有良好的维度记录功能。\n定义两个张量：\nx: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.ones(2, 3, 4) y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.ones(2, 3, 4) 旧方法：\nz = x @ y.transpose(-2, -1) # 结果维度为batch, sequence, sequence 新方法（使用einops）：\nz = einsum(x, y, \"batch seq1 hidden, batch seq2 hidden -\u003e batch seq1 seq2\") 在输出中没有命名的维度会被求和。\n也可以使用...来表示对任意数量的维度进行广播：\nz = einsum(x, y, \"... seq1 hidden, ... seq2 hidden -\u003e ... seq1 seq2\") 3.4 Einops的reduce函数 你可以通过某些操作（如sum、mean、max、min）对单个张量进行降维。\nx: Float[torch.Tensor, \"batch seq hidden\"] = torch.ones(2, 3, 4) 旧方法：\ny = x.mean(dim=-1) # 对最后一个维度求平均值 新方法（使用einops）：\ny = reduce(x, \"... hidden -\u003e ...\", \"sum\") # 对hidden维度求和 3.5 Einops的rearrange函数 有时，一个维度实际上代表了两个维度，而你想要对其中一个进行操作。\nx: Float[torch.Tensor, \"batch seq total_hidden\"] = torch.ones(2, 3, 8) 这里的total_hidden是heads * hidden1的扁平化表示。\n将total_hidden拆分为两个维度（heads和hidden1）：\nx = rearrange(x, \"... (heads hidden1) -\u003e ... heads hidden1\", heads=2) 通过w执行转换：\nw: Float[torch.Tensor, \"hidden1 hidden2\"] = torch.ones(4, 4) x = einsum(x, w, \"... hidden1, hidden1 hidden2 -\u003e ... hidden2\") 将heads和hidden2重新组合在一起：\nx = rearrange(x, \"... heads hidden2 -\u003e ... (heads hidden2)\") 4.FLOPs\u0026FLOP/s 4.1 FLOPs是表示浮点运算次数的单位。\n比如，训练GPT-3 (2020) 需要 3.14e23 FLOPs。\nFLOP/s或者FLOPS表示每秒浮点运算次数。\n比如 A100 峰值运算 312 teraFLOP/s。\nFLOP/s 取决于使用的硬件和数据类型\n假设有B个token，每个token为D维，要变换为K维，那么相当于[B, D] [D, K] 矩阵相乘。\n在进行矩阵乘法时，需要的flops为2*B*D*K三个维度的乘积。把DK看作parameter，相当于2*token*parameter。\n4.2 Model FLOPs utilization (MFU)\nmfu = 实际的 FLOP/s 除以理论上的 FLOP/s\n","wordCount":"498","inLanguage":"en","datePublished":"2025-09-23T14:46:23+08:00","dateModified":"2025-09-23T14:46:23+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/cs336/lec2/"},"publisher":{"@type":"Organization","name":"陈","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/icon.png"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="陈 (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>陈</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CS336 Lec2</h1><div class=post-meta><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-memory-accounting aria-label="1. Memory accounting">1. Memory accounting</a></li><li><a href=#2-compute-accounting aria-label="2. Compute accounting">2. Compute accounting</a><ul><li><a href=#21-tensor aria-label="2.1 tensor">2.1 tensor</a></li></ul></li><li><a href=#3-einops aria-label="3. Einops">3. Einops</a><ul><li><a href=#31-einops%e7%9a%84%e8%ae%be%e8%ae%a1%e5%8a%a8%e6%9c%ba aria-label="3.1 Einops的设计动机">3.1 Einops的设计动机</a></li><li><a href=#32-jaxtyping%e5%86%99%e6%b3%95 aria-label="3.2 Jaxtyping写法">3.2 Jaxtyping写法</a></li><li><a href=#33-einops%e7%9a%84einsum%e5%87%bd%e6%95%b0 aria-label="3.3 Einops的einsum函数">3.3 Einops的einsum函数</a></li><li><a href=#34-einops%e7%9a%84reduce%e5%87%bd%e6%95%b0 aria-label="3.4 Einops的reduce函数">3.4 Einops的reduce函数</a></li><li><a href=#35-einops%e7%9a%84rearrange%e5%87%bd%e6%95%b0 aria-label="3.5 Einops的rearrange函数">3.5 Einops的rearrange函数</a></li></ul></li><li><a href=#4flopsflops aria-label=4.FLOPs&amp;FLOP/s>4.FLOPs&amp;FLOP/s</a><ul><li><a href=#41 aria-label=4.1>4.1</a></li><li><a href=#42 aria-label=4.2>4.2</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><hr><p>CS336: Language Models From Scratch (Spring 2025)</p><h1 id=1-memory-accounting>1. Memory accounting<a hidden class=anchor aria-hidden=true href=#1-memory-accounting>#</a></h1><p><strong>float32</strong>，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。</p><p>内存使用的估算：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># 建立矩阵</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>  <span class=c1># Default type</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>element_size</span><span class=p>()</span> <span class=o>==</span> <span class=mi>4</span>  <span class=c1># Float is 4 bytes</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>get_memory_usage</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>4</span>  <span class=c1># 128 bytes</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;One matrix in the feedforward layer of GPT-3:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>get_memory_usage</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=mi>12288</span> <span class=o>*</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>12288</span><span class=p>))</span> <span class=o>==</span> <span class=mi>2304</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span>  <span class=c1># 2.3 GB</span>
</span></span><span class=line><span class=cl><span class=o>...</span><span class=n>which</span> <span class=ow>is</span> <span class=n>a</span> <span class=n>lot</span><span class=err>!</span>
</span></span></code></pre></div><p><strong>float16</strong>，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。</p><p><strong>bfloat16</strong>，bf16，符号1位，<strong>指数8位</strong>，尾数7位，和fp16使用内存相同，但是指数范围和fp32相同。</p><p><strong>fp8</strong>，8位，有E4M3、E5M2两种形式。</p><p>训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。</p><br><h1 id=2-compute-accounting>2. Compute accounting<a hidden class=anchor aria-hidden=true href=#2-compute-accounting>#</a></h1><p>为了利用GPU的并行计算能力，需要将数据迁移到GPU上。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;By default, tensors are stored in CPU memory.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>device</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;Move the tensor to GPU memory (device 0).&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda:0&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=o>.</span><span class=n>device</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;Or create a tensor directly on the GPU:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda:0&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>new_memory_allocated</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>memory_allocated</span><span class=p>()</span>  <span class=c1># @inspect new_memory_allocated</span>
</span></span><span class=line><span class=cl><span class=n>memory_used</span> <span class=o>=</span> <span class=n>new_memory_allocated</span> <span class=o>-</span> <span class=n>memory_allocated</span>  <span class=c1># @inspect memory_used</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>memory_used</span> <span class=o>==</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=mi>32</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>*</span> <span class=mi>4</span><span class=p>)</span>  <span class=c1># 2 32x32 matrices of 4-byte floats</span>
</span></span></code></pre></div><br><h2 id=21-tensor>2.1 tensor<a hidden class=anchor aria-hidden=true href=#21-tensor>#</a></h2><p>Pytorch中的tensor是一个指向具体内存的指针，并且带有步长，辅助寻找元素。</p><p>某些操作，如切片、转置、改变形状，并不会产生一个新的数组，只是改变了数组的视图，例如</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=mi>100</span>
</span></span></code></pre></div><p>x y 实际上是指向同一片内存的指针。</p><br><p>但是，如果y等于x的转置，y就变成了一个不连续的tensor，连续访问行的时候，在内存上是跳跃访问的。对于不连续的tensor不能改变其视图，必须先变为连续的。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span></code></pre></div><br><p>对于改变tensor值的操作，会产生一个新的tensor。</p><br><p>有时候我们想对成batch的数据进行处理，Pytorch支持这种方法。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>  
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span></code></pre></div><br><h1 id=3-einops>3. Einops<a hidden class=anchor aria-hidden=true href=#3-einops>#</a></h1><p>Einops是一个用于操作张量的库，其中的维度都有明确的名称。它的设计灵感来自爱因斯坦求和符号。</p><p><a href=https://einops.rocks/1-einops-basics/>Einops教程</a></p><h2 id=31-einops的设计动机>3.1 Einops的设计动机<a hidden class=anchor aria-hidden=true href=#31-einops的设计动机>#</a></h2><p>传统的PyTorch代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 分别表示batch, sequence, hidden维度</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 分别表示batch, sequence, hidden维度</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>y</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 结果维度为batch, sequence, sequence</span>
</span></span></code></pre></div><p>这种写法很容易搞混维度（比如-2和-1到底指的是什么维度？）</p><h2 id=32-jaxtyping写法>3.2 Jaxtyping写法<a hidden class=anchor aria-hidden=true href=#32-jaxtyping写法>#</a></h2><p>如何跟踪张量的维度？</p><p>旧方法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 代表batch seq heads hidden</span>
</span></span></code></pre></div><p>新方法（使用jaxtyping）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq heads hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span></code></pre></div><p>注意：这只是一种文档说明方式（没有实际的强制检查）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 如果你想尝试einops，别忘了导入以下的库：</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>jaxtyping</span> <span class=kn>import</span> <span class=n>Float</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>einops</span> <span class=kn>import</span> <span class=n>einsum</span><span class=p>,</span> <span class=n>reduce</span><span class=p>,</span> <span class=n>rearrange</span>
</span></span></code></pre></div><br><h2 id=33-einops的einsum函数>3.3 Einops的einsum函数<a hidden class=anchor aria-hidden=true href=#33-einops的einsum函数>#</a></h2><p>einsum是一种广义的矩阵乘法，带有良好的维度记录功能。</p><p>定义两个张量：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq1 hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq2 hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span></code></pre></div><p>旧方法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>y</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 结果维度为batch, sequence, sequence</span>
</span></span></code></pre></div><p>新方法（使用einops）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=s2>&#34;batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>在输出中没有命名的维度会被求和。</p><p>也可以使用<code>...</code>来表示对任意数量的维度进行广播：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=s2>&#34;... seq1 hidden, ... seq2 hidden -&gt; ... seq1 seq2&#34;</span><span class=p>)</span>
</span></span></code></pre></div><br><h2 id=34-einops的reduce函数>3.4 Einops的reduce函数<a hidden class=anchor aria-hidden=true href=#34-einops的reduce函数>#</a></h2><p>你可以通过某些操作（如sum、mean、max、min）对单个张量进行降维。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span></code></pre></div><p>旧方法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 对最后一个维度求平均值</span>
</span></span></code></pre></div><p>新方法（使用einops）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>reduce</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... hidden -&gt; ...&#34;</span><span class=p>,</span> <span class=s2>&#34;sum&#34;</span><span class=p>)</span>  <span class=c1># 对hidden维度求和</span>
</span></span></code></pre></div><br><h2 id=35-einops的rearrange函数>3.5 Einops的rearrange函数<a hidden class=anchor aria-hidden=true href=#35-einops的rearrange函数>#</a></h2><p>有时，一个维度实际上代表了两个维度，而你想要对其中一个进行操作。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq total_hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>
</span></span></code></pre></div><p>这里的<code>total_hidden</code>是<code>heads * hidden1</code>的扁平化表示。</p><p>将<code>total_hidden</code>拆分为两个维度（<code>heads</code>和<code>hidden1</code>）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>rearrange</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... (heads hidden1) -&gt; ... heads hidden1&#34;</span><span class=p>,</span> <span class=n>heads</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></div><p>通过<code>w</code>执行转换：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>w</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;hidden1 hidden2&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>w</span><span class=p>,</span> <span class=s2>&#34;... hidden1, hidden1 hidden2 -&gt; ... hidden2&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>将<code>heads</code>和<code>hidden2</code>重新组合在一起：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>rearrange</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... heads hidden2 -&gt; ... (heads hidden2)&#34;</span><span class=p>)</span>
</span></span></code></pre></div><br><h1 id=4flopsflops>4.FLOPs&amp;FLOP/s<a hidden class=anchor aria-hidden=true href=#4flopsflops>#</a></h1><h2 id=41>4.1<a hidden class=anchor aria-hidden=true href=#41>#</a></h2><p><strong>FLOPs</strong>是表示浮点运算次数的单位。</p><p>比如，训练GPT-3 (2020) 需要 3.14e23 FLOPs。</p><p><strong>FLOP/s</strong>或者<strong>FLOPS</strong>表示每秒浮点运算次数。</p><p>比如 A100 峰值运算 312 teraFLOP/s。</p><p>FLOP/s 取决于使用的硬件和数据类型</p><br><p>假设有B个token，每个token为D维，要变换为K维，那么相当于[B, D] [D, K] 矩阵相乘。</p><p>在进行矩阵乘法时，需要的flops为<code>2*B*D*K</code>三个维度的乘积。把DK看作parameter，相当于<code>2*token*parameter</code>。</p><br><h2 id=42>4.2<a hidden class=anchor aria-hidden=true href=#42>#</a></h2><p>Model FLOPs utilization (MFU)</p><p>mfu = 实际的 FLOP/s 除以理论上的 FLOP/s</p><br><br><br><br><p><br><br><br></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>陈</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>