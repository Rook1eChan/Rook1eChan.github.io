<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>CS336 Lec2 | 陈</title><meta name=keywords content><meta name=description content='
CS336: Language Models From Scratch (Spring 2025)

1. Memory
float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。是tensor的默认存储精度。
内存使用的估算：
x = torch.zeros(4, 8)  # 建立矩阵
assert x.dtype == torch.float32  # tensor默认精度为fp32
assert x.numel() == 4 * 8
assert x.element_size() == 4  # Float is 4 bytes
assert get_memory_usage(x) == 4 * 8 * 4  # 128 bytes

text("One matrix in the feedforward layer of GPT-3:")
assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024  # 2.3 GB
float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。
bfloat16，bf16，符号1位，指数8位，尾数7位，在和fp16保持相同存储的同时和fp32有相同的动态范围，牺牲了部分精度但可以接受。
fp8，8位，有E4M3、E5M2两种形式。
训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。

2. Compute
2.1 tensor
Pytorch中的tensor（张量）是一个多维数组，可以是1D的向量，2D的矩阵，3D的cube等。tensor是一个指向具体内存的指针+各种元数据。元数据包括shape和stride，shape告诉我们有几个维度，每个维度有多少个元素，stride告诉我们在内存中跳多少步才能访问下一个维度的元素。

tensor默认存储在cpu上，需要显式将其移动到gpu。
memory_allocated = torch.cuda.memory_allocated()

x = torch.zeros(32, 32)
assert x.device == torch.device("cpu")

text("为了利用GPU的并行计算能力，将tensor迁移到GPU")

text("Move the tensor to GPU memory (device 0).")
y = x.to("cuda:0")
assert y.device == torch.device("cuda", 0)

text("Or create a tensor directly on the GPU:")
z = torch.zeros(32, 32, device="cuda:0")

new_memory_allocated = torch.cuda.memory_allocated()

memory_used = new_memory_allocated - memory_allocated

assert memory_used == 2 * (32 * 32 * 4)  # 2 32x32 matrices of 4-byte floats
 
某些操作，如切片、转置、改变形状，并不会产生一个新的tensor，只是改变了tensor的元数据的值，例如：'><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/cs336/lec2/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/cs336/lec2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/cs336/lec2/"><meta property="og:site_name" content="陈"><meta property="og:title" content="CS336 Lec2"><meta property="og:description" content=' CS336: Language Models From Scratch (Spring 2025)
1. Memory float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。是tensor的默认存储精度。
内存使用的估算：
x = torch.zeros(4, 8) # 建立矩阵 assert x.dtype == torch.float32 # tensor默认精度为fp32 assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes text("One matrix in the feedforward layer of GPT-3:") assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。
bfloat16，bf16，符号1位，指数8位，尾数7位，在和fp16保持相同存储的同时和fp32有相同的动态范围，牺牲了部分精度但可以接受。
fp8，8位，有E4M3、E5M2两种形式。
训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。
2. Compute 2.1 tensor Pytorch中的tensor（张量）是一个多维数组，可以是1D的向量，2D的矩阵，3D的cube等。tensor是一个指向具体内存的指针+各种元数据。元数据包括shape和stride，shape告诉我们有几个维度，每个维度有多少个元素，stride告诉我们在内存中跳多少步才能访问下一个维度的元素。
tensor默认存储在cpu上，需要显式将其移动到gpu。
memory_allocated = torch.cuda.memory_allocated() x = torch.zeros(32, 32) assert x.device == torch.device("cpu") text("为了利用GPU的并行计算能力，将tensor迁移到GPU") text("Move the tensor to GPU memory (device 0).") y = x.to("cuda:0") assert y.device == torch.device("cuda", 0) text("Or create a tensor directly on the GPU:") z = torch.zeros(32, 32, device="cuda:0") new_memory_allocated = torch.cuda.memory_allocated() memory_used = new_memory_allocated - memory_allocated assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats 某些操作，如切片、转置、改变形状，并不会产生一个新的tensor，只是改变了tensor的元数据的值，例如：'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-23T14:46:23+08:00"><meta property="article:modified_time" content="2025-09-23T14:46:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CS336 Lec2"><meta name=twitter:description content='
CS336: Language Models From Scratch (Spring 2025)

1. Memory
float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。是tensor的默认存储精度。
内存使用的估算：
x = torch.zeros(4, 8)  # 建立矩阵
assert x.dtype == torch.float32  # tensor默认精度为fp32
assert x.numel() == 4 * 8
assert x.element_size() == 4  # Float is 4 bytes
assert get_memory_usage(x) == 4 * 8 * 4  # 128 bytes

text("One matrix in the feedforward layer of GPT-3:")
assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024  # 2.3 GB
float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。
bfloat16，bf16，符号1位，指数8位，尾数7位，在和fp16保持相同存储的同时和fp32有相同的动态范围，牺牲了部分精度但可以接受。
fp8，8位，有E4M3、E5M2两种形式。
训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。

2. Compute
2.1 tensor
Pytorch中的tensor（张量）是一个多维数组，可以是1D的向量，2D的矩阵，3D的cube等。tensor是一个指向具体内存的指针+各种元数据。元数据包括shape和stride，shape告诉我们有几个维度，每个维度有多少个元素，stride告诉我们在内存中跳多少步才能访问下一个维度的元素。

tensor默认存储在cpu上，需要显式将其移动到gpu。
memory_allocated = torch.cuda.memory_allocated()

x = torch.zeros(32, 32)
assert x.device == torch.device("cpu")

text("为了利用GPU的并行计算能力，将tensor迁移到GPU")

text("Move the tensor to GPU memory (device 0).")
y = x.to("cuda:0")
assert y.device == torch.device("cuda", 0)

text("Or create a tensor directly on the GPU:")
z = torch.zeros(32, 32, device="cuda:0")

new_memory_allocated = torch.cuda.memory_allocated()

memory_used = new_memory_allocated - memory_allocated

assert memory_used == 2 * (32 * 32 * 4)  # 2 32x32 matrices of 4-byte floats
 
某些操作，如切片、转置、改变形状，并不会产生一个新的tensor，只是改变了tensor的元数据的值，例如：'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"CS336 Lec2","item":"https://Rook1eChan.github.io/posts/cs336/lec2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CS336 Lec2","name":"CS336 Lec2","description":" CS336: Language Models From Scratch (Spring 2025)\n1. Memory float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。是tensor的默认存储精度。\n内存使用的估算：\nx = torch.zeros(4, 8) # 建立矩阵 assert x.dtype == torch.float32 # tensor默认精度为fp32 assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes text(\u0026#34;One matrix in the feedforward layer of GPT-3:\u0026#34;) assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。\nbfloat16，bf16，符号1位，指数8位，尾数7位，在和fp16保持相同存储的同时和fp32有相同的动态范围，牺牲了部分精度但可以接受。\nfp8，8位，有E4M3、E5M2两种形式。\n训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。\n2. Compute 2.1 tensor Pytorch中的tensor（张量）是一个多维数组，可以是1D的向量，2D的矩阵，3D的cube等。tensor是一个指向具体内存的指针+各种元数据。元数据包括shape和stride，shape告诉我们有几个维度，每个维度有多少个元素，stride告诉我们在内存中跳多少步才能访问下一个维度的元素。\ntensor默认存储在cpu上，需要显式将其移动到gpu。\nmemory_allocated = torch.cuda.memory_allocated() x = torch.zeros(32, 32) assert x.device == torch.device(\u0026#34;cpu\u0026#34;) text(\u0026#34;为了利用GPU的并行计算能力，将tensor迁移到GPU\u0026#34;) text(\u0026#34;Move the tensor to GPU memory (device 0).\u0026#34;) y = x.to(\u0026#34;cuda:0\u0026#34;) assert y.device == torch.device(\u0026#34;cuda\u0026#34;, 0) text(\u0026#34;Or create a tensor directly on the GPU:\u0026#34;) z = torch.zeros(32, 32, device=\u0026#34;cuda:0\u0026#34;) new_memory_allocated = torch.cuda.memory_allocated() memory_used = new_memory_allocated - memory_allocated assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats 某些操作，如切片、转置、改变形状，并不会产生一个新的tensor，只是改变了tensor的元数据的值，例如：\n","keywords":[],"articleBody":" CS336: Language Models From Scratch (Spring 2025)\n1. Memory float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。是tensor的默认存储精度。\n内存使用的估算：\nx = torch.zeros(4, 8) # 建立矩阵 assert x.dtype == torch.float32 # tensor默认精度为fp32 assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes text(\"One matrix in the feedforward layer of GPT-3:\") assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。\nbfloat16，bf16，符号1位，指数8位，尾数7位，在和fp16保持相同存储的同时和fp32有相同的动态范围，牺牲了部分精度但可以接受。\nfp8，8位，有E4M3、E5M2两种形式。\n训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。\n2. Compute 2.1 tensor Pytorch中的tensor（张量）是一个多维数组，可以是1D的向量，2D的矩阵，3D的cube等。tensor是一个指向具体内存的指针+各种元数据。元数据包括shape和stride，shape告诉我们有几个维度，每个维度有多少个元素，stride告诉我们在内存中跳多少步才能访问下一个维度的元素。\ntensor默认存储在cpu上，需要显式将其移动到gpu。\nmemory_allocated = torch.cuda.memory_allocated() x = torch.zeros(32, 32) assert x.device == torch.device(\"cpu\") text(\"为了利用GPU的并行计算能力，将tensor迁移到GPU\") text(\"Move the tensor to GPU memory (device 0).\") y = x.to(\"cuda:0\") assert y.device == torch.device(\"cuda\", 0) text(\"Or create a tensor directly on the GPU:\") z = torch.zeros(32, 32, device=\"cuda:0\") new_memory_allocated = torch.cuda.memory_allocated() memory_used = new_memory_allocated - memory_allocated assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats 某些操作，如切片、转置、改变形状，并不会产生一个新的tensor，只是改变了tensor的元数据的值，例如：\nx = torch.tensor([[1, 2, 3], [4, 5, 6]]) y = x.view(3, 2) x[0][0] = 100 assert y[0][0] == 100 x y 实际上是指向同一片内存的指针。\n对于改变tensor值的操作，会产生一个新的tensor。\n有时候我们想对成batch的数据进行处理，Pytorch支持这种方法。\nx = torch.ones(4, 8, 16, 32) w = torch.ones(32, 2) y = x @ w assert y.size() == torch.Size([4, 8, 16, 2]) # 末尾的两个维度进行相乘 2.2 contiguous 当x转置（transpose）得到y时，x和y实际上共享内存数据。但是此时x是一个contiguous（连续的）的矩阵，y是一个非contiguous的矩阵。\nx=[[1,2,3], [4,5,6]] tensor刚创立的时候是contiguous的。 pytorch中，默认按照行优先存储数据。实际上x在内存中是一维数组[1,2,3,4,5,6]，但是x的stride（步长）元数据为[3,1]，表示第 0 维（行）的步长为 3：从第 0 行到第 1 行（沿行维度移动 1 步），需要跳过 3 个元素（因为每行有 3 个元素）。 第 1 维（列）的步长为 1：从第 0 列到第 1 列（沿列维度移动 1 步），只需跳过 1 个元素（同一行内连续存储）。 然后将x transpose得到y，此时xy指针的值是一样的，仅仅stride的值进行了交换。y的stride为[1,3]，表示按第0维（行）每次跳过一个元素，按第一维（列）每次跳过3个元素。所以y输出以后看起来是3*2的矩阵： y=[[1,4], [2,5], [3,6]] 对于不连续的矩阵，不能再接着使用view、flatten，因为它们要重新解释张量的布局，需要假设数据在内存中是连续存储的。不连续的矩阵需要使用.contiguous()先变为连续的矩阵。\ny = x.transpose(1, 0).contiguous().view(2, 3) pytorch中可使用.is_contiguous()来判断矩阵是否连续。判断逻辑为：\n沿第 i 个维度的步长（stride[i]）等于后一个维度的步长（stride[i+1]）乘以该维度的大小（shape[i+1]），即：\nstride[i] = stride[i+1] * shape[i+1] （对所有 0 ≤ i \u003c N-1 成立） 且最后一个维度的步长必须为 1（stride[-1] = 1）。\n比如：\n转置前：\nx = torch.tensor([[1, 2, 3], [4, 5, 6]]) # shape=(2, 3) print(x.stride()) # (3, 1) print(x.is_contiguous()) # True 检查条件：stride[0] = 3，stride[1] * shape[1] = 1 * 3 = 3，满足 stride[0] = stride[1] * shape[1]；且最后一维步长 stride[1] = 1，故连续。 转置后：\nx_t = x.transpose(0, 1) # shape=(3, 2) print(x_t.stride()) # (1, 3) print(x_t.is_contiguous()) # False 检查条件：stride[0] = 1，stride[1] * shape[1] = 3 * 2 = 6，显然 1 ≠ 6，不满足；故非连续。 3. Einops Einops是一个用于操作张量的库，其中的维度都有明确的名称。\nEinops教程\n3.1 Einops的设计动机 传统的PyTorch代码：\nx = torch.ones(2, 2, 3) # 分别表示batch, sequence, hidden维度 y = torch.ones(2, 2, 3) # 分别表示batch, sequence, hidden维度 z = x @ y.transpose(-2, -1) # 结果维度为batch, sequence, sequence 这种写法很容易搞混维度（比如-2和-1到底指的是什么维度？）\n3.2 Jaxtyping写法 如何记录张量各维度的意义？\n旧方法，通过自己加注释：\nx = torch.ones(2, 2, 1, 3) # 代表batch seq heads hidden 新方法（使用jaxtyping）：\nx: Float[torch.Tensor, \"batch seq heads hidden\"] = torch.ones(2, 2, 1, 3) # 如果你想尝试einops，别忘了导入以下的库： import torch from jaxtyping import Float from einops import einsum, reduce, rearrange 3.3 Einops的einsum函数 einsum是一种广义的矩阵乘法，带有良好的维度记录功能。\n定义两个张量：\nx: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.ones(2, 3, 4) y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.ones(2, 3, 4) 旧方法：\nz = x @ y.transpose(-2, -1) # 结果维度为batch, sequence, sequence 新方法（使用einops）：\nz = einsum(x, y, \"batch seq1 hidden, batch seq2 hidden -\u003e batch seq1 seq2\") 在输出中没有命名的维度会被求和。\n也可以使用...来表示对任意数量的维度进行广播：\nz = einsum(x, y, \"... seq1 hidden, ... seq2 hidden -\u003e ... seq1 seq2\") 3.4 Einops的reduce函数 你可以通过某些操作（如sum、mean、max、min）对单个张量进行降维。\nx: Float[torch.Tensor, \"batch seq hidden\"] = torch.ones(2, 3, 4) 旧方法：\ny = x.mean(dim=-1) # 对最后一个维度求平均值 新方法（使用einops）：\ny = reduce(x, \"... hidden -\u003e ...\", \"sum\") # 对hidden维度求和 3.5 Einops的rearrange函数 有时，一个维度实际上代表了两个维度，而你想要对其中一个进行操作。\nx: Float[torch.Tensor, \"batch seq total_hidden\"] = torch.ones(2, 3, 8) 这里的total_hidden是heads * hidden1的扁平化表示。\n将total_hidden拆分为两个维度（heads和hidden1）：\nx = rearrange(x, \"... (heads hidden1) -\u003e ... heads hidden1\", heads=2) 通过w执行转换：\nw: Float[torch.Tensor, \"hidden1 hidden2\"] = torch.ones(4, 4) x = einsum(x, w, \"... hidden1, hidden1 hidden2 -\u003e ... hidden2\") 将heads和hidden2重新组合在一起：\nx = rearrange(x, \"... heads hidden2 -\u003e ... (heads hidden2)\") 4.FLOPs\u0026FLOP/s FLOPs是表示浮点运算次数的单位。一次基础操作（加、乘）记为一次浮点操作。\n训练GPT-3 (2020) 需要 3.14e23 FLOPs。\nFLOP/s或者FLOPS表示每秒浮点运算次数。\n比如 A100 峰值运算 312 teraFLOP/s。\nFLOP/s 取决于使用的硬件和数据类型。\nModel FLOPs utilization (MFU)\nmfu = 实际的 FLOP/s 除以理论上的 FLOP/s\n通常当MFU ≥ 0.5时被认为较好的利用了硬件。\n估算简单线性模型前向传播的FLOPS\n假设有B个token，每个token为D维，要变换为K维，那么相当于[B, D] [D, K] 矩阵相乘。\nB = 16384 # Number of points(batchsize) D = 32768 # Dimension K = 8192 # Number of outputs x = torch.ones(B, D, device=device) w = torch.randn(D, K, device=device) y = x @ w 必要的操作有 (x[i][j] * w[j][k]) 的乘法运算以及每个 (i, j, k) 对进行一次加法运算，总共需要的flops为2 * B * D * K，即三个维度的乘积。把DK看作parameter，相当于2 * token * parameter。\n估算简单线性模型总的的FLOPS\n包括前向传播，反向传播。推导过程省略。\n前向传播：2*数据点数*参数量\n后向传播：4*数据点数*参数量\n总计：6*数据点数*参数量\n5.Model 5.1参数初始化 参数在pytorch中以nn.Parameter形式存储，是tensor。\n假设 x 是一个形如 (input_dim,) 的向量，w 是一个形如 (input_dim, output_dim) 的矩阵，output = x @ w。如果随机初始化，会发现output大约随着sqrt(input_dim)变化。需要所以对参数乘以 1/sqrt(input_dim)。\n如果 output 随 input_dim 膨胀，会导致两个严重问题：\n梯度消失 / 爆炸\n深层网络中，若每一层的输出都随维度膨胀（或收缩），经过多层传递后，数值会变得极大（梯度爆炸）或极小（梯度消失），导致模型无法训练。\n激活函数失效\n例如 ReLU 激活函数（f (x)=max (0,x)），若 input_dim 很大导致 output 数值过大，大部分神经元会处于 “激活状态”（x\u003e0），丧失非线性表达能力；反之，数值过小则大部分神经元 “死亡”（x\u003c0）。\n可以对权重 w 乘以 1/√(input_dim) 进行缩放（Xavier initialization）\n5.2 Randomness 保证实验的可复现是非常有用且必要的，因此要设置随机种子，一共有三个地方\n# Torch seed = 0 torch.manual_seed(seed) # NumPy import numpy as np np.random.seed(seed) # Python import random random.seed(seed) 5.3 pin_memory 和 non_blocking 默认的 CPU tensor 是 paged memory（分页内存）。分页内存是操作系统常规分配给进程的内存，用于正常运行没问题，但是 GPU 无法直接访问这类内存，从 CPU 拷贝到 GPU 较慢。\nif torch.cuda.is_available(): x = x.pin_memory() pin_memory() 会将张量固定到物理内存中，禁止其换页。这使得 GPU 可以使用 DMA（直接内存访问）异步拷贝数据，速度更快。\nx = x.to(device, non_blocking=True) 加了 non_blocking=True，就表示这个 .to() 操作是 异步的（non-blocking），只要 x 是 pinned memory，就能真正做到异步。\n你可以在同时干两件事：\nCPU 异步准备 + 传输下一批数据 GPU 同时处理上一批数据 最终的效果就是提高 GPU 利用率，减少等待数据加载的空转时间\n参考：详解Pytorch里的pin_memory 和 non_blocking - 知乎\n5.4 Optimizer SGD（随机梯度下降）：\n最基础的优化器。特点为每一步都朝着当前梯度方向走；对所有参数使用同一个固定学习率；收敛慢、容易卡在鞍点或局部最小值。$\\theta_{t + 1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta} J(\\theta)$ 其中，$\\theta$为模型参数，$\\eta$为学习率，$\\nabla_{\\theta} J(\\theta)$为损失函数对参数的梯度。\nMomentum(SGD + exponential averaging of grad)： 加了“惯性”，用指数加权移动平均平滑梯度，减少震荡、加快收敛。直觉上就好像在山谷中下坡，Momentum像是加了惯性的小球，可以跨过小波动，不轻易被困住。 $v_t = \\beta v_{t - 1} + (1 - \\beta)\\nabla_{\\theta} J(\\theta)$ $\\theta_{t + 1} = \\theta_t - \\eta \\cdot v_t$ 其中，$v_t$是速度（梯度的指数平均），$\\beta \\in [0.9, 0.99]$控制历史记忆。\nAdaGrad(SGD + averaging by grad^2)：\n对每个参数都使用不同的学习率，且梯度越大，学习率衰减得越快。优点是参数稀疏时非常有效；缺点是随着训练进行，$G_t$越来越大，学习率会衰减到趋近0，也就是学不动了。 $G_t = G_{t - 1} + \\nabla_{\\theta} J(\\theta)^2$ $\\theta_{t + 1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta)$ 其中，$G_t$是累计的平方梯度（对每个参数单独计算），$\\epsilon$防止除以0。\nRMSProp(AdaGrad + exponentially averaging of grad^2)： 是对AdaGrad的修正，用指数移动平均替代了“累加所有历史梯度平方”。优点是避免了AdaGrad的“学习率消失”问题，在非凸优化问题上表现稳定. $s_t = \\beta s_{t - 1} + (1 - \\beta)\\nabla_{\\theta} J(\\theta)^2$ $\\theta_{t + 1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta) $ 其中，$s_t$是对平方梯度的指数平均。\nAdam(RMSProp + momentum)：\n综合了Momentum（动量）+ RMSProp（自适应学习率）的优点。优点是自动调整每个参数的学习率；在许多任务中无需调参效果也很好；收敛速度快，表现稳。 一阶动量估计：$m_t = \\beta_1 m_{t - 1} + (1 - \\beta_1)\\nabla_{\\theta} J(\\theta)$ 二阶动量估计：$v_t = \\beta_2 v_{t - 1} + (1 - \\beta_2)(\\nabla_{\\theta} J(\\theta))^2$ 偏差修正：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$，$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$ 最终更新规则：$\\theta_{t + 1} = \\theta_t - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$ Optimizer 主要机制 SGD 基础版本，直接减梯度 Momentum 平滑梯度方向，加速下降 AdaGrad 每个参数学习率按梯度平方缩放（历史平均） RMSProp 改进AdaGrad，用指数平均避免学习率变太小 Adam RMSProp + Momentum（双重指数平均） SGD：走一步看一步 Momentum：考虑过去的速度，减少来回震荡 AdaGrad：走路时每次都踩相同的地方会越来越慢 RMSProp：记得最近的坑，别总踩同一个坑 Adam：不仅避坑，还顺着山坡惯性滑下去 5.5 Train 如果要使用混合精度进行训练，一个常规的做法是：\n在前向传播中使用bfloat16/bfloat8（保存/计算激活值） 在其余部分使用float32（反向传播，保存参数、梯度等，保证精度） 5.6 Checkpoint 在保存的时候不光是要保存模型参数，如果需要继续训练，还需要保存优化器的参数\ncheckpoint = { \"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), } torch.save(checkpoint, \"model_checkpoint.pt\") 这一讲主要讲了内存和计算的估算，有助于对模型及其训练有一个宏观的把控。还讲到了tensor的一些操作。最后讲了模型训练的的流程，其中有很多有用的技巧，希望以后的训练中能用上，提高效率。\n","wordCount":"955","inLanguage":"en","datePublished":"2025-09-23T14:46:23+08:00","dateModified":"2025-09-23T14:46:23+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/cs336/lec2/"},"publisher":{"@type":"Organization","name":"陈","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/icon.png"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="陈 (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>陈</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CS336 Lec2</h1><div class=post-meta><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-memory aria-label="1. Memory">1. Memory</a></li><li><a href=#2-compute aria-label="2. Compute">2. Compute</a><ul><li><a href=#21-tensor aria-label="2.1 tensor">2.1 tensor</a></li><li><a href=#22-contiguous aria-label="2.2 contiguous">2.2 contiguous</a></li></ul></li><li><a href=#3-einops aria-label="3. Einops">3. Einops</a><ul><li><a href=#31-einops%e7%9a%84%e8%ae%be%e8%ae%a1%e5%8a%a8%e6%9c%ba aria-label="3.1 Einops的设计动机">3.1 Einops的设计动机</a></li><li><a href=#32-jaxtyping%e5%86%99%e6%b3%95 aria-label="3.2 Jaxtyping写法">3.2 Jaxtyping写法</a></li><li><a href=#33-einops%e7%9a%84einsum%e5%87%bd%e6%95%b0 aria-label="3.3 Einops的einsum函数">3.3 Einops的einsum函数</a></li><li><a href=#34-einops%e7%9a%84reduce%e5%87%bd%e6%95%b0 aria-label="3.4 Einops的reduce函数">3.4 Einops的reduce函数</a></li><li><a href=#35-einops%e7%9a%84rearrange%e5%87%bd%e6%95%b0 aria-label="3.5 Einops的rearrange函数">3.5 Einops的rearrange函数</a></li></ul></li><li><a href=#4flopsflops aria-label=4.FLOPs&amp;FLOP/s>4.FLOPs&amp;FLOP/s</a></li><li><a href=#5model aria-label=5.Model>5.Model</a><ul><li><a href=#51%e5%8f%82%e6%95%b0%e5%88%9d%e5%a7%8b%e5%8c%96 aria-label=5.1参数初始化>5.1参数初始化</a></li><li><a href=#52-randomness aria-label="5.2 Randomness">5.2 Randomness</a></li><li><a href=#53-pin_memory-%e5%92%8c-non_blocking aria-label="5.3 pin_memory 和 non_blocking">5.3 pin_memory 和 non_blocking</a></li><li><a href=#54-optimizer aria-label="5.4 Optimizer">5.4 Optimizer</a></li><li><a href=#55-train aria-label="5.5 Train">5.5 Train</a></li><li><a href=#56-checkpoint aria-label="5.6 Checkpoint">5.6 Checkpoint</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><hr><p>CS336: Language Models From Scratch (Spring 2025)</p><p><img loading=lazy src=../pic/banner.png></p><h1 id=1-memory>1. Memory<a hidden class=anchor aria-hidden=true href=#1-memory>#</a></h1><p><strong>float32</strong>，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。是tensor的默认存储精度。</p><p>内存使用的估算：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># 建立矩阵</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>  <span class=c1># tensor默认精度为fp32</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>element_size</span><span class=p>()</span> <span class=o>==</span> <span class=mi>4</span>  <span class=c1># Float is 4 bytes</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>get_memory_usage</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>4</span>  <span class=c1># 128 bytes</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;One matrix in the feedforward layer of GPT-3:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>get_memory_usage</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=mi>12288</span> <span class=o>*</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>12288</span><span class=p>))</span> <span class=o>==</span> <span class=mi>2304</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span>  <span class=c1># 2.3 GB</span>
</span></span></code></pre></div><p><strong>float16</strong>，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。</p><p><strong>bfloat16</strong>，bf16，符号1位，<strong>指数8位</strong>，尾数7位，在和fp16保持相同存储的同时和fp32有相同的动态范围，牺牲了部分精度但可以接受。</p><p><strong>fp8</strong>，8位，有E4M3、E5M2两种形式。</p><p>训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。</p><br><h1 id=2-compute>2. Compute<a hidden class=anchor aria-hidden=true href=#2-compute>#</a></h1><h2 id=21-tensor>2.1 tensor<a hidden class=anchor aria-hidden=true href=#21-tensor>#</a></h2><p>Pytorch中的tensor（张量）是一个多维数组，可以是1D的向量，2D的矩阵，3D的cube等。tensor是一个<strong>指向具体内存的指针+各种元数据</strong>。元数据包括shape和stride，shape告诉我们有几个维度，每个维度有多少个元素，stride告诉我们在内存中跳多少步才能访问下一个维度的元素。</p><br><p>tensor默认存储在cpu上，需要显式将其移动到gpu。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>memory_allocated</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>memory_allocated</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>device</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;为了利用GPU的并行计算能力，将tensor迁移到GPU&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;Move the tensor to GPU memory (device 0).&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda:0&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=o>.</span><span class=n>device</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span><span class=p>(</span><span class=s2>&#34;Or create a tensor directly on the GPU:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda:0&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>new_memory_allocated</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>memory_allocated</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>memory_used</span> <span class=o>=</span> <span class=n>new_memory_allocated</span> <span class=o>-</span> <span class=n>memory_allocated</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>memory_used</span> <span class=o>==</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=mi>32</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>*</span> <span class=mi>4</span><span class=p>)</span>  <span class=c1># 2 32x32 matrices of 4-byte floats</span>
</span></span></code></pre></div><br><p>某些操作，如切片、转置、改变形状，并不会产生一个新的tensor，只是改变了tensor的元数据的值，例如：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=mi>100</span>
</span></span></code></pre></div><p>x y 实际上是指向同一片内存的指针。</p><br><p>对于改变tensor值的操作，会产生一个新的tensor。</p><br><p>有时候我们想对成batch的数据进行处理，Pytorch支持这种方法。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>  
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>  <span class=c1># 末尾的两个维度进行相乘</span>
</span></span></code></pre></div><br><h2 id=22-contiguous>2.2 contiguous<a hidden class=anchor aria-hidden=true href=#22-contiguous>#</a></h2><p>当x转置（transpose）得到y时，x和y实际上共享内存数据。但是此时x是一个contiguous（连续的）的矩阵，y是一个非contiguous的矩阵。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x=[[1,2,3],
</span></span><span class=line><span class=cl>	[4,5,6]]
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>tensor刚创立的时候是contiguous的。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>pytorch中，默认按照行优先存储数据。实际上x在内存中是一维数组[1,2,3,4,5,6]，但是x的stride（步长）元数据为[3,1]，表示第 0 维（行）的步长为 3：从第 0 行到第 1 行（沿行维度移动 1 步），需要跳过 3 个元素（因为每行有 3 个元素）。
</span></span><span class=line><span class=cl>第 1 维（列）的步长为 1：从第 0 列到第 1 列（沿列维度移动 1 步），只需跳过 1 个元素（同一行内连续存储）。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>然后将x transpose得到y，此时xy指针的值是一样的，仅仅stride的值进行了交换。y的stride为[1,3]，表示按第0维（行）每次跳过一个元素，按第一维（列）每次跳过3个元素。所以y输出以后看起来是3*2的矩阵：
</span></span><span class=line><span class=cl>y=[[1,4],
</span></span><span class=line><span class=cl>	[2,5],
</span></span><span class=line><span class=cl>	[3,6]]
</span></span></code></pre></div><br><p>对于不连续的矩阵，不能再接着使用view、flatten，因为它们要重新解释张量的布局，需要假设数据在内存中是连续存储的。不连续的矩阵需要使用<code>.contiguous()</code>先变为连续的矩阵。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span></code></pre></div><br><p>pytorch中可使用<code>.is_contiguous()</code>来判断矩阵是否连续。判断逻辑为：</p><p><strong>沿第 <code>i</code> 个维度的步长（<code>stride[i]</code>）等于后一个维度的步长（<code>stride[i+1]</code>）乘以该维度的大小（<code>shape[i+1]</code>）</strong>，即：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>stride[i] = stride[i+1] * shape[i+1]  （对所有 0 ≤ i &lt; N-1 成立）
</span></span></code></pre></div><p>且最后一个维度的步长必须为 1（<code>stride[-1] = 1</code>）。</p><p>比如：</p><p><strong>转置前</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]])</span>  <span class=c1># shape=(2, 3)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>stride</span><span class=p>())</span>  <span class=c1># (3, 1)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>is_contiguous</span><span class=p>())</span>  <span class=c1># True</span>
</span></span></code></pre></div><ul><li>检查条件：<code>stride[0] = 3</code>，<code>stride[1] * shape[1] = 1 * 3 = 3</code>，满足 <code>stride[0] = stride[1] * shape[1]</code>；且最后一维步长 <code>stride[1] = 1</code>，故连续。</li></ul><p><strong>转置后</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x_t</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>  <span class=c1># shape=(3, 2)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_t</span><span class=o>.</span><span class=n>stride</span><span class=p>())</span>  <span class=c1># (1, 3)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_t</span><span class=o>.</span><span class=n>is_contiguous</span><span class=p>())</span>  <span class=c1># False</span>
</span></span></code></pre></div><ul><li>检查条件：<code>stride[0] = 1</code>，<code>stride[1] * shape[1] = 3 * 2 = 6</code>，显然 <code>1 ≠ 6</code>，不满足；故非连续。</li></ul><br><h1 id=3-einops>3. Einops<a hidden class=anchor aria-hidden=true href=#3-einops>#</a></h1><p>Einops是一个用于操作张量的库，其中的维度都有明确的名称。</p><p><a href=https://einops.rocks/1-einops-basics/>Einops教程</a></p><h2 id=31-einops的设计动机>3.1 Einops的设计动机<a hidden class=anchor aria-hidden=true href=#31-einops的设计动机>#</a></h2><p>传统的PyTorch代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 分别表示batch, sequence, hidden维度</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 分别表示batch, sequence, hidden维度</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>y</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 结果维度为batch, sequence, sequence</span>
</span></span></code></pre></div><p>这种写法很容易搞混维度（比如-2和-1到底指的是什么维度？）</p><br><h2 id=32-jaxtyping写法>3.2 Jaxtyping写法<a hidden class=anchor aria-hidden=true href=#32-jaxtyping写法>#</a></h2><p>如何记录张量各维度的意义？</p><p>旧方法，通过自己加注释：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 代表batch seq heads hidden</span>
</span></span></code></pre></div><p>新方法（使用jaxtyping）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq heads hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 如果你想尝试einops，别忘了导入以下的库：</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>jaxtyping</span> <span class=kn>import</span> <span class=n>Float</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>einops</span> <span class=kn>import</span> <span class=n>einsum</span><span class=p>,</span> <span class=n>reduce</span><span class=p>,</span> <span class=n>rearrange</span>
</span></span></code></pre></div><br><h2 id=33-einops的einsum函数>3.3 Einops的einsum函数<a hidden class=anchor aria-hidden=true href=#33-einops的einsum函数>#</a></h2><p>einsum是一种广义的矩阵乘法，带有良好的维度记录功能。</p><p>定义两个张量：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq1 hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq2 hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span></code></pre></div><p>旧方法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>y</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 结果维度为batch, sequence, sequence</span>
</span></span></code></pre></div><p>新方法（使用einops）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=s2>&#34;batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>在输出中没有命名的维度会被求和。</p><p>也可以使用<code>...</code>来表示对任意数量的维度进行广播：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=s2>&#34;... seq1 hidden, ... seq2 hidden -&gt; ... seq1 seq2&#34;</span><span class=p>)</span>
</span></span></code></pre></div><br><h2 id=34-einops的reduce函数>3.4 Einops的reduce函数<a hidden class=anchor aria-hidden=true href=#34-einops的reduce函数>#</a></h2><p>你可以通过某些操作（如sum、mean、max、min）对单个张量进行降维。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span></code></pre></div><p>旧方法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 对最后一个维度求平均值</span>
</span></span></code></pre></div><p>新方法（使用einops）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>reduce</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... hidden -&gt; ...&#34;</span><span class=p>,</span> <span class=s2>&#34;sum&#34;</span><span class=p>)</span>  <span class=c1># 对hidden维度求和</span>
</span></span></code></pre></div><br><h2 id=35-einops的rearrange函数>3.5 Einops的rearrange函数<a hidden class=anchor aria-hidden=true href=#35-einops的rearrange函数>#</a></h2><p>有时，一个维度实际上代表了两个维度，而你想要对其中一个进行操作。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq total_hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>
</span></span></code></pre></div><p>这里的<code>total_hidden</code>是<code>heads * hidden1</code>的扁平化表示。</p><p>将<code>total_hidden</code>拆分为两个维度（<code>heads</code>和<code>hidden1</code>）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>rearrange</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... (heads hidden1) -&gt; ... heads hidden1&#34;</span><span class=p>,</span> <span class=n>heads</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></div><p>通过<code>w</code>执行转换：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>w</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;hidden1 hidden2&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>w</span><span class=p>,</span> <span class=s2>&#34;... hidden1, hidden1 hidden2 -&gt; ... hidden2&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>将<code>heads</code>和<code>hidden2</code>重新组合在一起：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>rearrange</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... heads hidden2 -&gt; ... (heads hidden2)&#34;</span><span class=p>)</span>
</span></span></code></pre></div><br><h1 id=4flopsflops>4.FLOPs&amp;FLOP/s<a hidden class=anchor aria-hidden=true href=#4flopsflops>#</a></h1><p><strong>FLOPs</strong>是表示浮点运算次数的单位。一次基础操作（加、乘）记为一次浮点操作。</p><p>训练GPT-3 (2020) 需要 3.14e23 FLOPs。</p><p><strong>FLOP/s</strong>或者<strong>FLOPS</strong>表示每秒浮点运算次数。</p><p>比如 A100 峰值运算 312 teraFLOP/s。</p><p>FLOP/s 取决于使用的硬件和数据类型。</p><br><p>Model FLOPs utilization (MFU)</p><p>mfu = 实际的 FLOP/s 除以理论上的 FLOP/s</p><p>通常当MFU ≥ 0.5时被认为较好的利用了硬件。</p><br><p><strong>估算简单线性模型前向传播的FLOPS</strong></p><p>假设有B个token，每个token为D维，要变换为K维，那么相当于[B, D] [D, K] 矩阵相乘。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>B</span> <span class=o>=</span> <span class=mi>16384</span>  <span class=c1># Number of points(batchsize)</span>
</span></span><span class=line><span class=cl><span class=n>D</span> <span class=o>=</span> <span class=mi>32768</span>  <span class=c1># Dimension</span>
</span></span><span class=line><span class=cl><span class=n>K</span> <span class=o>=</span> <span class=mi>8192</span>   <span class=c1># Number of outputs</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>D</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>D</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>
</span></span></code></pre></div><p>必要的操作有 (x[i][j] * w[j][k]) 的乘法运算以及每个 (i, j, k) 对进行一次加法运算，总共需要的flops为<code>2 * B * D * K</code>，即三个维度的乘积。把DK看作parameter，相当于<code>2 * token * parameter</code>。</p><br><p><strong>估算简单线性模型总的的FLOPS</strong></p><p>包括前向传播，反向传播。推导过程省略。</p><p>前向传播：<code>2*数据点数*参数量</code></p><p>后向传播：<code>4*数据点数*参数量</code></p><p>总计：<code>6*数据点数*参数量</code></p><br><h1 id=5model>5.Model<a hidden class=anchor aria-hidden=true href=#5model>#</a></h1><h2 id=51参数初始化>5.1参数初始化<a hidden class=anchor aria-hidden=true href=#51参数初始化>#</a></h2><p>参数在pytorch中以nn.Parameter形式存储，是tensor。</p><p>假设 x 是一个形如 (input_dim,) 的向量，w 是一个形如 (input_dim, output_dim) 的矩阵，output = x @ w。如果随机初始化，会发现output大约随着sqrt(input_dim)变化。需要所以对参数乘以 1/sqrt(input_dim)。</p><p>如果 output 随 input_dim 膨胀，会导致两个严重问题：</p><ol><li><p>梯度消失 / 爆炸</p><p>深层网络中，若每一层的输出都随维度膨胀（或收缩），经过多层传递后，数值会变得极大（梯度爆炸）或极小（梯度消失），导致模型无法训练。</p></li><li><p>激活函数失效</p><p>例如 ReLU 激活函数（f (x)=max (0,x)），若 input_dim 很大导致 output 数值过大，大部分神经元会处于 “激活状态”（x>0），丧失非线性表达能力；反之，数值过小则大部分神经元 “死亡”（x&lt;0）。</p></li></ol><p>可以对权重 w 乘以 1/√(input_dim) 进行缩放（Xavier initialization）</p><br><h2 id=52-randomness>5.2 Randomness<a hidden class=anchor aria-hidden=true href=#52-randomness>#</a></h2><p>保证实验的可复现是非常有用且必要的，因此要设置随机种子，一共有三个地方</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Torch</span>
</span></span><span class=line><span class=cl><span class=n>seed</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># NumPy</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Python</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span></code></pre></div><br><h2 id=53-pin_memory-和-non_blocking>5.3 pin_memory 和 non_blocking<a hidden class=anchor aria-hidden=true href=#53-pin_memory-和-non_blocking>#</a></h2><p>默认的 CPU tensor 是 paged memory（分页内存）。分页内存是操作系统常规分配给进程的内存，用于正常运行没问题，但是 GPU 无法直接访问这类内存，从 CPU 拷贝到 GPU 较慢。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>pin_memory</span><span class=p>()</span>
</span></span></code></pre></div><p>pin_memory() 会将张量固定到物理内存中，禁止其换页。这使得 GPU 可以使用 DMA（直接内存访问）异步拷贝数据，速度更快。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>,</span> <span class=n>non_blocking</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>加了 non_blocking=True，就表示这个 .to() 操作是 异步的（non-blocking），只要 x 是 pinned memory，就能真正做到异步。</p><p>你可以在同时干两件事：</p><ol><li>CPU 异步准备 + 传输下一批数据</li><li>GPU 同时处理上一批数据</li></ol><p>最终的效果就是<strong>提高 GPU 利用率，减少等待数据加载的空转时间</strong></p><p>参考：<a href=https://zhuanlan.zhihu.com/p/477870660>详解Pytorch里的pin_memory 和 non_blocking - 知乎</a></p><br><h2 id=54-optimizer>5.4 Optimizer<a hidden class=anchor aria-hidden=true href=#54-optimizer>#</a></h2><ul><li><p><strong>SGD（随机梯度下降）</strong>：</p><p>最基础的优化器。特点为每一步都朝着当前梯度方向走；对所有参数使用同一个固定学习率；收敛慢、容易卡在鞍点或局部最小值。$\theta_{t + 1} = \theta_t - \eta \cdot \nabla_{\theta} J(\theta)$
其中，$\theta$为模型参数，$\eta$为学习率，$\nabla_{\theta} J(\theta)$为损失函数对参数的梯度。</p></li><li><p><strong>Momentum(SGD + exponential averaging of grad)</strong>：
加了“惯性”，用指数加权移动平均平滑梯度，减少震荡、加快收敛。直觉上就好像在山谷中下坡，Momentum像是加了惯性的小球，可以跨过小波动，不轻易被困住。
$v_t = \beta v_{t - 1} + (1 - \beta)\nabla_{\theta} J(\theta)$
$\theta_{t + 1} = \theta_t - \eta \cdot v_t$
其中，$v_t$是速度（梯度的指数平均），$\beta \in [0.9, 0.99]$控制历史记忆。</p></li><li><p><strong>AdaGrad(SGD + averaging by grad^2)</strong>：</p><p>对每个参数都使用不同的学习率，且梯度越大，学习率衰减得越快。优点是参数稀疏时非常有效；缺点是随着训练进行，$G_t$越来越大，学习率会衰减到趋近0，也就是学不动了。
$G_t = G_{t - 1} + \nabla_{\theta} J(\theta)^2$
$\theta_{t + 1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot \nabla_{\theta} J(\theta)$
其中，$G_t$是累计的平方梯度（对每个参数单独计算），$\epsilon$防止除以0。</p></li><li><p><strong>RMSProp(AdaGrad + exponentially averaging of grad^2)</strong>：
是对AdaGrad的修正，用指数移动平均替代了“累加所有历史梯度平方”。优点是避免了AdaGrad的“学习率消失”问题，在非凸优化问题上表现稳定.
$s_t = \beta s_{t - 1} + (1 - \beta)\nabla_{\theta} J(\theta)^2$
$\theta_{t + 1} = \theta_t - \frac{\eta}{\sqrt{s_t + \epsilon}} \cdot \nabla_{\theta} J(\theta)
$
其中，$s_t$是对平方梯度的指数平均。</p></li><li><p><strong>Adam(RMSProp + momentum)</strong>：</p><ul><li>综合了Momentum（动量）+ RMSProp（自适应学习率）的优点。优点是自动调整每个参数的学习率；在许多任务中无需调参效果也很好；收敛速度快，表现稳。</li><li>一阶动量估计：$m_t = \beta_1 m_{t - 1} + (1 - \beta_1)\nabla_{\theta} J(\theta)$</li><li>二阶动量估计：$v_t = \beta_2 v_{t - 1} + (1 - \beta_2)(\nabla_{\theta} J(\theta))^2$</li><li>偏差修正：$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$，$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</li><li>最终更新规则：$\theta_{t + 1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$</li></ul></li></ul><br><table><thead><tr><th>Optimizer</th><th>主要机制</th></tr></thead><tbody><tr><td>SGD</td><td>基础版本，直接减梯度</td></tr><tr><td>Momentum</td><td>平滑梯度方向，加速下降</td></tr><tr><td>AdaGrad</td><td>每个参数学习率按梯度平方缩放（历史平均）</td></tr><tr><td>RMSProp</td><td>改进AdaGrad，用指数平均避免学习率变太小</td></tr><tr><td>Adam</td><td>RMSProp + Momentum（双重指数平均）</td></tr></tbody></table><br><ul><li>SGD：走一步看一步</li><li>Momentum：考虑过去的速度，减少来回震荡</li><li>AdaGrad：走路时每次都踩相同的地方会越来越慢</li><li>RMSProp：记得最近的坑，别总踩同一个坑</li><li>Adam：不仅避坑，还顺着山坡惯性滑下去</li></ul><br><h2 id=55-train>5.5 Train<a hidden class=anchor aria-hidden=true href=#55-train>#</a></h2><p>如果要使用混合精度进行训练，一个常规的做法是：</p><ul><li>在前向传播中使用bfloat16/bfloat8（保存/计算激活值）</li><li>在其余部分使用float32（反向传播，保存参数、梯度等，保证精度）</li></ul><h2 id=56-checkpoint>5.6 Checkpoint<a hidden class=anchor aria-hidden=true href=#56-checkpoint>#</a></h2><p>在保存的时候不光是要保存模型参数，如果需要继续训练，还需要保存优化器的参数</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>checkpoint</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;optimizer&#34;</span><span class=p>:</span> <span class=n>optimizer</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>,</span> <span class=s2>&#34;model_checkpoint.pt&#34;</span><span class=p>)</span>
</span></span></code></pre></div><br><p>这一讲主要讲了内存和计算的估算，有助于对模型及其训练有一个宏观的把控。还讲到了tensor的一些操作。最后讲了模型训练的的流程，其中有很多有用的技巧，希望以后的训练中能用上，提高效率。</p><br></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>陈</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>