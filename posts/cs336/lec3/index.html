<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>CS336 Lec3 | 陈</title><meta name=keywords content><meta name=description content="
CS336: Language Models From Scratch (Spring 2025)
本节主要讲了模型的架构设计和超参数选择。
1.Architecture
1.1Norm
pre-norm, post-norm, &lsquo;double&rsquo;-norm
自从GPT之后大都采用pre-norm，把layernorm层放到FFN、MHA层之前。

prenorm和postnorm的效果一样好，而且不需要warm。更好的梯度反向传播，更少的spike。
现在有的模型还使用&rsquo;double&rsquo;-norm，即FFN、MHA层之前之后都有layernorm。

LayerNorm, RMSNorm
原始的transformer和早期模型使用LN，现在都改为使用RMSN。
LN：$y = \frac{x - \text{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} * \gamma + \beta$
其中 $\text{E}[x]$ 是均值，$\text{Var}[x]$ 是方差，$\epsilon$ 是防止分母为 0 的小量，$\gamma$（缩放因子）和 $\beta$​​（偏移因子）是可学习参数。

RMSN：$y = \frac{x}{\sqrt{\|x\|_2^2 + \epsilon}} * \gamma$​
其中 $\|x\|_2^2$​ 是输入 x 的二范数平方，$\epsilon$​ 是防止分母为 0 的小量，$\gamma$​ 是可学习的缩放参数。
不减去均值，也不添加偏置项$\beta$​。

RMSN效果和LN一样好，而且更快。操作更少（无需计算平均值），参数更少（没有偏置项）。

曾有研究表明，在模型运算中，矩阵乘法占用的flops达到99.8%，正则化的运算量只占到0.17%。从计算性能的角度看，norm没必要优化。但是内存开销也是一个重要的考量，该研究指出正则化所占的运行时间达到25.5%，在内存搬运上花了相当一部分时间，因此值得优化。

现有的大部分transformer模型都没有bias项，只进行矩阵乘法。reason：更稳定（原因未知）

1.2 Activations
ReLU、GeLU、SwiGLU、GeGLU
GLU（门控线性单元）现在得到广泛使用

ReLU（Rectified Linear Unit，修正线性单元）是深度学习中最常用的激活函数之一。
ReLU 的函数形式非常简单，数学定义为：$\text{ReLU}(x) = \max(0, x)$即："><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/cs336/lec3/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/cs336/lec3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/cs336/lec3/"><meta property="og:site_name" content="陈"><meta property="og:title" content="CS336 Lec3"><meta property="og:description" content=" CS336: Language Models From Scratch (Spring 2025)
本节主要讲了模型的架构设计和超参数选择。
1.Architecture 1.1Norm pre-norm, post-norm, ‘double’-norm
自从GPT之后大都采用pre-norm，把layernorm层放到FFN、MHA层之前。
prenorm和postnorm的效果一样好，而且不需要warm。更好的梯度反向传播，更少的spike。
现在有的模型还使用’double’-norm，即FFN、MHA层之前之后都有layernorm。
LayerNorm, RMSNorm
原始的transformer和早期模型使用LN，现在都改为使用RMSN。
LN：$y = \frac{x - \text{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} * \gamma + \beta$
其中 $\text{E}[x]$ 是均值，$\text{Var}[x]$ 是方差，$\epsilon$ 是防止分母为 0 的小量，$\gamma$（缩放因子）和 $\beta$​​（偏移因子）是可学习参数。
RMSN：$y = \frac{x}{\sqrt{\|x\|_2^2 + \epsilon}} * \gamma$​ 其中 $\|x\|_2^2$​ 是输入 x 的二范数平方，$\epsilon$​ 是防止分母为 0 的小量，$\gamma$​ 是可学习的缩放参数。
不减去均值，也不添加偏置项$\beta$​。
RMSN效果和LN一样好，而且更快。操作更少（无需计算平均值），参数更少（没有偏置项）。
曾有研究表明，在模型运算中，矩阵乘法占用的flops达到99.8%，正则化的运算量只占到0.17%。从计算性能的角度看，norm没必要优化。但是内存开销也是一个重要的考量，该研究指出正则化所占的运行时间达到25.5%，在内存搬运上花了相当一部分时间，因此值得优化。
现有的大部分transformer模型都没有bias项，只进行矩阵乘法。reason：更稳定（原因未知）
1.2 Activations ReLU、GeLU、SwiGLU、GeGLU GLU（门控线性单元）现在得到广泛使用
ReLU（Rectified Linear Unit，修正线性单元）是深度学习中最常用的激活函数之一。
ReLU 的函数形式非常简单，数学定义为：$\text{ReLU}(x) = \max(0, x)$即："><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-23T14:46:23+08:00"><meta property="article:modified_time" content="2025-09-23T14:46:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CS336 Lec3"><meta name=twitter:description content="
CS336: Language Models From Scratch (Spring 2025)
本节主要讲了模型的架构设计和超参数选择。
1.Architecture
1.1Norm
pre-norm, post-norm, &lsquo;double&rsquo;-norm
自从GPT之后大都采用pre-norm，把layernorm层放到FFN、MHA层之前。

prenorm和postnorm的效果一样好，而且不需要warm。更好的梯度反向传播，更少的spike。
现在有的模型还使用&rsquo;double&rsquo;-norm，即FFN、MHA层之前之后都有layernorm。

LayerNorm, RMSNorm
原始的transformer和早期模型使用LN，现在都改为使用RMSN。
LN：$y = \frac{x - \text{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} * \gamma + \beta$
其中 $\text{E}[x]$ 是均值，$\text{Var}[x]$ 是方差，$\epsilon$ 是防止分母为 0 的小量，$\gamma$（缩放因子）和 $\beta$​​（偏移因子）是可学习参数。

RMSN：$y = \frac{x}{\sqrt{\|x\|_2^2 + \epsilon}} * \gamma$​
其中 $\|x\|_2^2$​ 是输入 x 的二范数平方，$\epsilon$​ 是防止分母为 0 的小量，$\gamma$​ 是可学习的缩放参数。
不减去均值，也不添加偏置项$\beta$​。

RMSN效果和LN一样好，而且更快。操作更少（无需计算平均值），参数更少（没有偏置项）。

曾有研究表明，在模型运算中，矩阵乘法占用的flops达到99.8%，正则化的运算量只占到0.17%。从计算性能的角度看，norm没必要优化。但是内存开销也是一个重要的考量，该研究指出正则化所占的运行时间达到25.5%，在内存搬运上花了相当一部分时间，因此值得优化。

现有的大部分transformer模型都没有bias项，只进行矩阵乘法。reason：更稳定（原因未知）

1.2 Activations
ReLU、GeLU、SwiGLU、GeGLU
GLU（门控线性单元）现在得到广泛使用

ReLU（Rectified Linear Unit，修正线性单元）是深度学习中最常用的激活函数之一。
ReLU 的函数形式非常简单，数学定义为：$\text{ReLU}(x) = \max(0, x)$即："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"CS336 Lec3","item":"https://Rook1eChan.github.io/posts/cs336/lec3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CS336 Lec3","name":"CS336 Lec3","description":" CS336: Language Models From Scratch (Spring 2025)\n本节主要讲了模型的架构设计和超参数选择。\n1.Architecture 1.1Norm pre-norm, post-norm, \u0026lsquo;double\u0026rsquo;-norm\n自从GPT之后大都采用pre-norm，把layernorm层放到FFN、MHA层之前。\nprenorm和postnorm的效果一样好，而且不需要warm。更好的梯度反向传播，更少的spike。\n现在有的模型还使用\u0026rsquo;double\u0026rsquo;-norm，即FFN、MHA层之前之后都有layernorm。\nLayerNorm, RMSNorm\n原始的transformer和早期模型使用LN，现在都改为使用RMSN。\nLN：$y = \\frac{x - \\text{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n其中 $\\text{E}[x]$ 是均值，$\\text{Var}[x]$ 是方差，$\\epsilon$ 是防止分母为 0 的小量，$\\gamma$（缩放因子）和 $\\beta$​​（偏移因子）是可学习参数。\nRMSN：$y = \\frac{x}{\\sqrt{\\|x\\|_2^2 + \\epsilon}} * \\gamma$​ 其中 $\\|x\\|_2^2$​ 是输入 x 的二范数平方，$\\epsilon$​ 是防止分母为 0 的小量，$\\gamma$​ 是可学习的缩放参数。\n不减去均值，也不添加偏置项$\\beta$​。\nRMSN效果和LN一样好，而且更快。操作更少（无需计算平均值），参数更少（没有偏置项）。\n曾有研究表明，在模型运算中，矩阵乘法占用的flops达到99.8%，正则化的运算量只占到0.17%。从计算性能的角度看，norm没必要优化。但是内存开销也是一个重要的考量，该研究指出正则化所占的运行时间达到25.5%，在内存搬运上花了相当一部分时间，因此值得优化。\n现有的大部分transformer模型都没有bias项，只进行矩阵乘法。reason：更稳定（原因未知）\n1.2 Activations ReLU、GeLU、SwiGLU、GeGLU GLU（门控线性单元）现在得到广泛使用\nReLU（Rectified Linear Unit，修正线性单元）是深度学习中最常用的激活函数之一。\nReLU 的函数形式非常简单，数学定义为：$\\text{ReLU}(x) = \\max(0, x)$即：\n","keywords":[],"articleBody":" CS336: Language Models From Scratch (Spring 2025)\n本节主要讲了模型的架构设计和超参数选择。\n1.Architecture 1.1Norm pre-norm, post-norm, ‘double’-norm\n自从GPT之后大都采用pre-norm，把layernorm层放到FFN、MHA层之前。\nprenorm和postnorm的效果一样好，而且不需要warm。更好的梯度反向传播，更少的spike。\n现在有的模型还使用’double’-norm，即FFN、MHA层之前之后都有layernorm。\nLayerNorm, RMSNorm\n原始的transformer和早期模型使用LN，现在都改为使用RMSN。\nLN：$y = \\frac{x - \\text{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n其中 $\\text{E}[x]$ 是均值，$\\text{Var}[x]$ 是方差，$\\epsilon$ 是防止分母为 0 的小量，$\\gamma$（缩放因子）和 $\\beta$​​（偏移因子）是可学习参数。\nRMSN：$y = \\frac{x}{\\sqrt{\\|x\\|_2^2 + \\epsilon}} * \\gamma$​ 其中 $\\|x\\|_2^2$​ 是输入 x 的二范数平方，$\\epsilon$​ 是防止分母为 0 的小量，$\\gamma$​ 是可学习的缩放参数。\n不减去均值，也不添加偏置项$\\beta$​。\nRMSN效果和LN一样好，而且更快。操作更少（无需计算平均值），参数更少（没有偏置项）。\n曾有研究表明，在模型运算中，矩阵乘法占用的flops达到99.8%，正则化的运算量只占到0.17%。从计算性能的角度看，norm没必要优化。但是内存开销也是一个重要的考量，该研究指出正则化所占的运行时间达到25.5%，在内存搬运上花了相当一部分时间，因此值得优化。\n现有的大部分transformer模型都没有bias项，只进行矩阵乘法。reason：更稳定（原因未知）\n1.2 Activations ReLU、GeLU、SwiGLU、GeGLU GLU（门控线性单元）现在得到广泛使用\nReLU（Rectified Linear Unit，修正线性单元）是深度学习中最常用的激活函数之一。\nReLU 的函数形式非常简单，数学定义为：$\\text{ReLU}(x) = \\max(0, x)$即：\n当输入 $x \\geq 0$ 时，输出等于输入本身 $x$； 当输入 $x \u003c 0$ 时，输出为 0。 ReLU的优点\nReLU 之所以能取代 Sigmoid、Tanh 等传统激活函数，成为主流选择，核心原因在于以下几点：\n计算效率极高：传统激活函数（如 Sigmoid 的 $\\sigma(x)=1/(1+e^{-x})$）需要指数运算，计算成本高；而 ReLU 仅需一个简单的阈值判断（x 是否大于 0），几乎没有计算开销，能显著加速模型训练。 缓解梯度消失问题：Sigmoid 的导数在 $|x|$ 较大时趋近于 0，导致梯度难以传递到浅层。ReLU 的导数在 (x\u003e0) 时为 1，梯度可以 “无损” 地反向传播，避免了深层网络中梯度被逐层衰减的问题。 稀疏激活特性：ReLU 会将所有负输入 “抑制” 为 0，只有正输入被保留，这种 “稀疏性” 使得神经网络在处理数 据时，仅激活部分神经元（类似生物大脑的工作模式），减少了冗余计算，同时增强了模型的泛化能力（避免过拟合）。 梯度消失 反向传播时，若梯度经过多层传递后逐渐衰减至接近 0，导致浅层参数几乎无法更新，称为梯度消失。 典型场景：使用 Sigmoid 激活函数时，其导数在输入绝对值较大时接近 0（最大值仅 0.25），多层相乘后梯度快速趋近于 0。 影响：深层网络的浅层参数难以优化，模型无法学习到有效特征。 梯度爆炸 反向传播时，若梯度经过多层传递后急剧增大至非常大的值，导致参数更新幅度过大，模型训练不稳定（如损失值跳变、NaN 等），称为梯度爆炸。 典型场景：循环神经网络（RNN）处理长序列时，梯度可能随时间步累积而指数级增长；或权重初始化过大，导致梯度被放大。 影响：参数更新失控，模型难以收敛。 ReLU 的缺点\n尽管优势显著，ReLU 仍存在一些不容忽视的缺陷：\n死亡 ReLU 问题（Dead ReLU Problem）：当输入 (x \u003c 0) 时，ReLU 的输出为 0，且导数也为 0。若在训练中，某个神经元的输入长期为负（例如，权重更新不当导致输入始终小于 0），该神经元的梯度将永远为 0，无法通过反向传播更新参数，最终 “永久死亡”，失去学习能力。 常见诱因：学习率过大（导致权重更新幅度过大，输入被 “推到” 负区间）、数据预处理不当（输入分布偏移）等。 输出非对称，均值偏移：ReLU 的输出范围是 $[0, +\\infty)$，非零均值（输出均值偏向正方向）。这可能导致后续层的输入分布逐渐偏向正值，影响梯度下降的稳定性（例如，梯度方向可能被 “挤压” 到特定区域）。 swish\nSwish 由 Google 提出，结合了 ReLU 和 Sigmoid 的特性，定义为：$\\text{Swish}(x) = x \\cdot \\sigma(\\beta x)$\n其中 $\\sigma$ 是 Sigmoid 函数，$\\beta$是可学习参数（或固定为 1）。\n优势：处处可导（无 ReLU 的 “折点”），负输入区域输出非零（避免死亡神经元），在深层网络（如 Transformer）中表现优异； 缺陷：计算复杂度略高于 ReLU。 SwigLU\nSwigLU 是一种结合了 Swish 激活函数和 GLU（Gated Linear Unit）门控机制的激活函数。\nSwigLU 数学定义为：$\\text{SwigLU}(x) = \\text{Swish}(x_1) \\otimes x_2$\n其中：\nx 是输入向量，被拆分为两个等长的子向量 $x_1$ 和 $x_2$（通常通过将输入维度切分或用两个线性层映射得到）； $\\text{Swish}(x_1) = x_1 \\cdot \\sigma(\\beta x_1)$ 是激活函数（$\\sigma$ 为 Sigmoid，$\\beta$ 为超参数，通常取 1），用于计算 “门控值”； $\\otimes$ 表示逐元素相乘（Hadamard 乘积），即门控值与 $x_2$ 相乘，实现信息筛选。 1.3 serial、parallel layers 经典的transformer是serial串行结构。\n公式：$y = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x))))$\nparallel并行结构最早在GPT-J中提出.\n公式：$y = x + \\text{MLP}(\\text{LayerNorm}(x)) + \\text{Attention}(\\text{LayerNorm}(x))$\n输入x经过层归一化后，同时送入MLP和注意力模块，实现并行。\n现在大多数采用serial结构。\n1.4 Position Embedding transformer的自注意力机制无法感知序列顺序，不能处理文本序列等需要考虑位置的问题。所以使用位置编码告诉模型输入之间的先后顺序。\ntransformer中位置编码位于embedding层之后。\n位置编码为每个位置生成独特的编码向量，将其与输入的词嵌入向量相加，让模型学习到每个元素在序列中的位置。\nsine embeddings（正弦位置嵌入） sine Embeddings 是一种经典的绝对位置嵌入方法，在论文《Attention Is All You Need》中提出，用于 Transformer 模型中。利用不同频率的正弦和余弦函数为每个位置生成一个独特的向量。\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$$$PE_{(pos, 2i + 1)} = \\cos\\left(\\frac{pos}{10000^{2i+1/d_{model}}}\\right)$$​\npos 是 token 在序列中的位置索引（从0开始） i 是维度索引（不是维度），取值范围：$0 ≤ i ＜ d_{model} / 2 - 1$ $d_{\\text{model}}$ 是模型的嵌入维度 $PE_{(pos, j)}$表示位置 pos 的编码向量中第 j 维的值 正弦余弦交替使用 对于每对相邻维度，偶数维度使用正弦函数，奇数维度使用余弦函数。\n波长的递增序列 指数$10000^{2i/d_{model}}$为不同维度创建了不同的波长：\n当$i = 0$时：$10000^{0}=1$，波长为$2\\pi$，波长短，函数值变化快。有助于捕捉近距离关系，具有高敏感性。 当$i = d_{model}/2 - 1$时：$10000^{(d_{model}-2)/d_{model}}\\approx10000$，波长为$10000\\cdot2\\pi$，在很长的序列范围内，高维度的编码值几乎恒定，有助于学习长距离依赖关系，提供稳定的、粗粒度的 “区域” 或 “上下文” 信号。 为什么选择10000？ 波长依据维度形成从$2\\pi$到$10000\\cdot2\\pi$的几何级数，保证位置编码的频率覆盖范围足够大，同时兼顾长序列与短序列。\nsine embeddings的特性\n唯一性和有界性\n各位置的编码向量互不相同 所有值都在[-1, 1]范围内 显示引入位置信息，不需要学习 无限外推能力\nsin 和 cos 对于任何实数都有定义，可以对任意位置生成编码，不受训练数据长度限制 编码模式沿着序列位置平滑变化，没有突变点 线性表示相对位置\n任意固定位置偏移的相对位置关系可以被表达为线性变换。\n对于任意固定偏移k，$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数。\n对于任意位置$pos$和偏移$k$，考虑特定维度$i$：\n令$\\alpha=\\frac{pos}{10000^{2i/d_{model}}}$和$\\beta=\\frac{k}{10000^{2i/d_{model}}}$，则\n$PE_{(pos,2i)}=\\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)=\\sin\\alpha$，$PE_{(pos,2i + 1)}=\\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)=\\cos\\alpha$​\n通过积化和差公式：\n$PE_{(pos + k,2i)}=\\sin(\\alpha + \\beta)=\\sin(\\alpha)\\cos(\\beta)+\\cos(\\alpha)\\sin(\\beta)$\n$PE_{(pos + k,2i + 1)}=\\cos(\\alpha + \\beta)=\\cos(\\alpha)\\cos(\\beta)-\\sin(\\alpha)\\sin(\\beta)$\n这表明$PE_{pos + k}$可以通过$PE_{pos}$的线性组合得到，线性系数仅与偏移$k$相关，独立于位置$pos$。\n能高效推断相对关系，但编码的仍然是每个 token 的绝对位置。有没有直接编码相对位置关系的方式呢？\n参考：\n【Transformer】9分钟认识位置编码 | 数学解析 | 旋转位置编码_哔哩哔哩_bilibili\nRope embeddings（旋转位置嵌入） 简单理解：我们需要一种表示相对位置的算法。RoPE在计算注意力时，通过将qk都乘以旋转矩阵，来把注意力运算结果中加入位置信息。\n特性：\n相对距离决定注意力：两个 token 之间的注意力分数只依赖于它们的相对位置差，而不是绝对位置 平移不变性：序列整体平移不会改变内部的注意力模式 保留长度外推能力：随着相对距离增加，注意力分数自然衰减。这符合靠的近的token注意力高，离得远的token注意力低的要求 原理解析参考，这两个已经解释的较为清楚了：\n旋转位置编码RoPE的简单理解_哔哩哔哩_bilibili\n十分钟读懂旋转编码（RoPE）\n2.Hyperparameters 经过这些年来大模型的迭代发展，人们在一些超参数的选择上逐渐形成了共识。\n2.1 Feedforward - model dimension ratio 一般模型： $d_{ff}=4d_{model}$\n如果使用GLU及其变体作为激活函数，需要额外引入一个权重矩阵。为了保持与原始 FFN 相同的计算成本和参数规模，GLU 变体普遍将隐藏层维度 $d_{ff}$ 缩小为原来的大约 2/3。所以 $d_{ff}=\\frac{2}{3}\\times(4d_{model})=\\frac{8}{3}d_{model}$\n2.2 Head-dim*num-heads to model-dim ratio Head-dim*num-heads和model-dim的比例基本为1。\n2.3 Aspect ratios 模型应该更宽还是更深？\n一般 $d_{model}/n_{layer}=128$​\n如下图所示，不同参数量的模型基本都在比率100左右表现最好。\n2.4 Vocabulary sizes 早期的单语言的模型的词表大小在30-50k。现在的多语言模型在100-250k左右。\n2.5 Dropout\u0026Regularization 在预训练时，由于数据量非常大，只需要在数据集上训练一遍，不大会出现过拟合。\n早期模型会在预训练时采用dropout。现在大多只使用weight decay。\n一般来说，使用weight decay是用来防止过拟合的。但实验发现weight decay并不会影响 val loss和train loss（下图左）。weight decay会与cosine模式的optimizer相作用，当学习率减小时，train loss会有效下降（下图中）。\n3.stability 我们希望模型在训练时，loss稳定下降，而不要波动过大，出现过多spike。\ntransformer的不稳定成因主要来自softmax：$\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$\n其中的指数运算易出现数值不稳定。其中的除法运算可能会除0错误。\nsoftmax出现在模型的最后和注意力层中，以下分别从这两个位置进行优化。\n3.1 z-loss 增加一个惩罚项，使Z的值稳定在1。\n3.2 QK norm 看不懂，跳过\n3.3 Soft-capping 看不懂，跳过\n4.Attention 4.1 GQA/MQA 当我们生成文字时，需要不断地预测下一个token，这一过程不能并行。\n通过KV cache可以减少开销。\nKV cache\nKV Cache 是大语言模型（LLM）推理阶段的核心优化技术，通过缓存注意力机制中重复计算的键（Key）和值（Value），显著提升生成效率。\n在 Transformer 解码器（如 GPT 系列模型）中，每一步生成新 token 时，都需要对已生成序列和当前输入计算自注意力（Self-Attention）。假设已生成序列长度为 n，新生成 token 为第 (n+1) 个，则自注意力的核心公式为：\n(\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V)\n其中：\nQ（Query）：当前输入的查询向量（维度 (d_k)）； K（Key）、V（Value）：已生成序列中所有 token 的键和值向量（维度均为 (d_k)）。 未优化时的问题：生成第 (n+1) 个 token 时，需要重新计算前 n 个 token 的 K 和 V，导致计算量随序列长度增长呈 (O(n^2)) 增长，推理速度极慢。\n原理：\nKV Cache 的本质是空间换时间：将已生成序列的 K 和 V 向量缓存起来，避免重复计算。\n首次计算（生成第 1 个 token）： 输入：初始序列（如提示词）； 计算：为每个 token 生成对应的 (K_1, V_1)，并缓存到内存中。 后续计算（生成第 t 个 token，(t \u003e 1)）： 输入：前 (t-1) 个 token 生成的序列 + 新输入（第 t 个 token 的 Query）； 计算：仅需为第 t 个 token 生成 (Q_t)，直接复用缓存中的 (K_1, K_2, …, K_{t-1}) 和 (V_1, V_2, …, V_{t-1})； 更新缓存：将新生成的 (K_t, V_t) 追加到缓存中，供下一步使用。 通过这一机制，每次生成新 token 时，仅需计算当前 token 的 Q 以及与历史 (K/V) 的注意力得分，计算量从 (O(n^2)) 降至 (O(n))。\n还讲了GQA、MQA、sparse attn、sliding window attn、interleave’full’ and ‘LR’ attn\n慢慢补吧……\n","wordCount":"561","inLanguage":"en","datePublished":"2025-09-23T14:46:23+08:00","dateModified":"2025-09-23T14:46:23+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://Rook1eChan.github.io/posts/cs336/lec3/"},"publisher":{"@type":"Organization","name":"陈","logo":{"@type":"ImageObject","url":"https://Rook1eChan.github.io/icon.png"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="陈 (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>陈</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Rook1eChan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CS336 Lec3</h1><div class=post-meta><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1architecture aria-label=1.Architecture>1.Architecture</a><ul><li><a href=#11norm aria-label=1.1Norm>1.1Norm</a></li><li><a href=#12-activations aria-label="1.2 Activations">1.2 Activations</a><ul><li><a href=#relugeluswiglugeglu aria-label=ReLU、GeLU、SwiGLU、GeGLU>ReLU、GeLU、SwiGLU、GeGLU</a></li></ul></li><li><a href=#13-serialparallel-layers aria-label="1.3 serial、parallel layers">1.3 serial、parallel layers</a></li><li><a href=#14-position-embedding aria-label="1.4 Position Embedding">1.4 Position Embedding</a><ul><li><a href=#sine-embeddings%e6%ad%a3%e5%bc%a6%e4%bd%8d%e7%bd%ae%e5%b5%8c%e5%85%a5 aria-label="sine embeddings（正弦位置嵌入）"><strong>sine embeddings（正弦位置嵌入）</strong></a></li><li><a href=#rope-embeddings%e6%97%8b%e8%bd%ac%e4%bd%8d%e7%bd%ae%e5%b5%8c%e5%85%a5 aria-label="Rope embeddings（旋转位置嵌入）"><strong>Rope embeddings（旋转位置嵌入）</strong></a></li></ul></li></ul></li><li><a href=#2hyperparameters aria-label=2.Hyperparameters>2.Hyperparameters</a><ul><li><a href=#21-feedforward---model-dimension-ratio aria-label="2.1 Feedforward - model dimension ratio">2.1 Feedforward - model dimension ratio</a></li><li><a href=#22-head-dimnum-heads-to-model-dim-ratio aria-label="2.2 Head-dim*num-heads to model-dim ratio">2.2 Head-dim*num-heads to model-dim ratio</a></li><li><a href=#23-aspect-ratios aria-label="2.3 Aspect ratios">2.3 Aspect ratios</a></li><li><a href=#24-vocabulary-sizes aria-label="2.4 Vocabulary sizes">2.4 Vocabulary sizes</a></li><li><a href=#25-dropoutregularization aria-label="2.5 Dropout&amp;Regularization">2.5 Dropout&amp;Regularization</a></li></ul></li><li><a href=#3stability aria-label=3.stability>3.stability</a><ul><li><a href=#31-z-loss aria-label="3.1 z-loss">3.1 z-loss</a></li><li><a href=#32-qk-norm aria-label="3.2 QK norm">3.2 QK norm</a></li><li><a href=#33-soft-capping aria-label="3.3 Soft-capping">3.3 Soft-capping</a></li></ul></li><li><a href=#4attention aria-label=4.Attention>4.Attention</a><ul><li><a href=#41-gqamqa aria-label="4.1 GQA/MQA">4.1 GQA/MQA</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><hr><p>CS336: Language Models From Scratch (Spring 2025)</p><p>本节主要讲了模型的架构设计和超参数选择。</p><h1 id=1architecture>1.Architecture<a hidden class=anchor aria-hidden=true href=#1architecture>#</a></h1><h2 id=11norm>1.1Norm<a hidden class=anchor aria-hidden=true href=#11norm>#</a></h2><p><strong>pre-norm, post-norm, &lsquo;double&rsquo;-norm</strong></p><p>自从GPT之后大都采用pre-norm，把layernorm层放到FFN、MHA层之前。</p><img src=../pic/3-p1.png alt=3-p1 style=zoom:50%><p>prenorm和postnorm的效果一样好，而且不需要warm。更好的梯度反向传播，更少的spike。</p><p>现在有的模型还使用&rsquo;double&rsquo;-norm，即FFN、MHA层之前之后都有layernorm。</p><br><p><strong>LayerNorm, RMSNorm</strong></p><p>原始的transformer和早期模型使用LN，现在都改为使用RMSN。</p><p><strong>LN</strong>：$y = \frac{x - \text{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} * \gamma + \beta$</p><p>其中 $\text{E}[x]$ 是均值，$\text{Var}[x]$ 是方差，$\epsilon$ 是防止分母为 0 的小量，$\gamma$（缩放因子）和 $\beta$​​（偏移因子）是可学习参数。</p><br><p><strong>RMSN</strong>：$y = \frac{x}{\sqrt{\|x\|_2^2 + \epsilon}} * \gamma$​
其中 $\|x\|_2^2$​ 是输入 x 的二范数平方，$\epsilon$​ 是防止分母为 0 的小量，$\gamma$​ 是可学习的缩放参数。</p><p>不减去均值，也不添加偏置项$\beta$​。</p><br><p>RMSN效果和LN一样好，而且更快。操作更少（无需计算平均值），参数更少（没有偏置项）。</p><img src=../pic/3-p2.png alt=3-p2 style=zoom:50%><p>曾有研究表明，在模型运算中，矩阵乘法占用的flops达到99.8%，正则化的运算量只占到0.17%。从计算性能的角度看，norm没必要优化。但是内存开销也是一个重要的考量，该研究指出正则化所占的运行时间达到25.5%，在内存搬运上花了相当一部分时间，因此值得优化。</p><br><p>现有的大部分transformer模型都没有bias项，只进行矩阵乘法。reason：更稳定（原因未知）</p><br><h2 id=12-activations>1.2 Activations<a hidden class=anchor aria-hidden=true href=#12-activations>#</a></h2><h3 id=relugeluswiglugeglu>ReLU、GeLU、SwiGLU、GeGLU<a hidden class=anchor aria-hidden=true href=#relugeluswiglugeglu>#</a></h3><p>GLU（门控线性单元）现在得到广泛使用</p><br><p><strong>ReLU</strong>（Rectified Linear Unit，修正线性单元）是深度学习中最常用的激活函数之一。</p><p>ReLU 的函数形式非常简单，数学定义为：$\text{ReLU}(x) = \max(0, x)$即：</p><ul><li>当输入 $x \geq 0$ 时，输出等于输入本身 $x$；</li><li>当输入 $x < 0$ 时，输出为 0。</li></ul><img src=../pic/3-p3.png alt=3-p3 style=zoom:50%><p><strong>ReLU的优点</strong></p><p>ReLU 之所以能取代 Sigmoid、Tanh 等传统激活函数，成为主流选择，核心原因在于以下几点：</p><ol><li><strong>计算效率极高</strong>：传统激活函数（如 Sigmoid 的 $\sigma(x)=1/(1+e^{-x})$）需要指数运算，计算成本高；而 ReLU 仅需一个简单的阈值判断（x 是否大于 0），几乎没有计算开销，能显著加速模型训练。</li><li><strong>缓解梯度消失问题</strong>：Sigmoid 的导数在 $|x|$ 较大时趋近于 0，导致梯度难以传递到浅层。ReLU 的导数在 (x>0) 时为 1，梯度可以 “无损” 地反向传播，避免了深层网络中梯度被逐层衰减的问题。</li><li><strong>稀疏激活特性</strong>：ReLU 会将所有负输入 “抑制” 为 0，只有正输入被保留，这种 “稀疏性” 使得神经网络在处理数 据时，仅激活部分神经元（类似生物大脑的工作模式），减少了冗余计算，同时增强了模型的泛化能力（避免过拟合）。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>梯度消失
</span></span><span class=line><span class=cl>反向传播时，若梯度经过多层传递后逐渐衰减至接近 0，导致浅层参数几乎无法更新，称为梯度消失。
</span></span><span class=line><span class=cl>典型场景：使用 Sigmoid 激活函数时，其导数在输入绝对值较大时接近 0（最大值仅 0.25），多层相乘后梯度快速趋近于 0。
</span></span><span class=line><span class=cl>影响：深层网络的浅层参数难以优化，模型无法学习到有效特征。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>梯度爆炸
</span></span><span class=line><span class=cl>反向传播时，若梯度经过多层传递后急剧增大至非常大的值，导致参数更新幅度过大，模型训练不稳定（如损失值跳变、NaN 等），称为梯度爆炸。
</span></span><span class=line><span class=cl>典型场景：循环神经网络（RNN）处理长序列时，梯度可能随时间步累积而指数级增长；或权重初始化过大，导致梯度被放大。
</span></span><span class=line><span class=cl>影响：参数更新失控，模型难以收敛。
</span></span></code></pre></div><br><p><strong>ReLU 的缺点</strong></p><p>尽管优势显著，ReLU 仍存在一些不容忽视的缺陷：</p><ol><li><strong>死亡 ReLU 问题（Dead ReLU Problem）</strong>：当输入 (x &lt; 0) 时，ReLU 的输出为 0，且导数也为 0。若在训练中，某个神经元的输入长期为负（例如，权重更新不当导致输入始终小于 0），该神经元的梯度将永远为 0，无法通过反向传播更新参数，最终 “永久死亡”，失去学习能力。<ul><li>常见诱因：学习率过大（导致权重更新幅度过大，输入被 “推到” 负区间）、数据预处理不当（输入分布偏移）等。</li></ul></li><li><strong>输出非对称，均值偏移</strong>：ReLU 的输出范围是 $[0, +\infty)$，非零均值（输出均值偏向正方向）。这可能导致后续层的输入分布逐渐偏向正值，影响梯度下降的稳定性（例如，梯度方向可能被 “挤压” 到特定区域）。</li></ol><br><p><strong>swish</strong></p><p>Swish 由 Google 提出，结合了 ReLU 和 Sigmoid 的特性，定义为：$\text{Swish}(x) = x \cdot \sigma(\beta x)$</p><p>其中 $\sigma$ 是 Sigmoid 函数，$\beta$是可学习参数（或固定为 1）。</p><ul><li>优势：处处可导（无 ReLU 的 “折点”），负输入区域输出非零（避免死亡神经元），在深层网络（如 Transformer）中表现优异；</li><li>缺陷：计算复杂度略高于 ReLU。</li></ul><br><p><strong>SwigLU</strong></p><p>SwigLU 是一种结合了 Swish 激活函数和 GLU（Gated Linear Unit）门控机制的激活函数。</p><p>SwigLU 数学定义为：$\text{SwigLU}(x) = \text{Swish}(x_1) \otimes x_2$</p><p>其中：</p><ol><li>x 是输入向量，被拆分为两个等长的子向量 $x_1$ 和 $x_2$（通常通过将输入维度切分或用两个线性层映射得到）；</li><li>$\text{Swish}(x_1) = x_1 \cdot \sigma(\beta x_1)$ 是激活函数（$\sigma$ 为 Sigmoid，$\beta$ 为超参数，通常取 1），用于计算 “门控值”；</li><li>$\otimes$ 表示逐元素相乘（Hadamard 乘积），即门控值与 $x_2$ 相乘，实现信息筛选。</li></ol><img src=../pic/3-p4.png alt=3-p4 style=zoom:50%><br><h2 id=13-serialparallel-layers>1.3 serial、parallel layers<a hidden class=anchor aria-hidden=true href=#13-serialparallel-layers>#</a></h2><p>经典的transformer是serial串行结构。</p><p>公式：$y = x + \text{MLP}(\text{LayerNorm}(x + \text{Attention}(\text{LayerNorm}(x))))$</p><br><p>parallel并行结构最早在GPT-J中提出.</p><p>公式：$y = x + \text{MLP}(\text{LayerNorm}(x)) + \text{Attention}(\text{LayerNorm}(x))$</p><p>输入x经过层归一化后，同时送入MLP和注意力模块，实现并行。</p><p>现在大多数采用serial结构。</p><br><h2 id=14-position-embedding>1.4 Position Embedding<a hidden class=anchor aria-hidden=true href=#14-position-embedding>#</a></h2><p>transformer的自注意力机制无法感知序列顺序，不能处理文本序列等需要考虑位置的问题。所以使用位置编码告诉模型输入之间的先后顺序。</p><p>transformer中位置编码位于embedding层之后。</p><img src=../pic/3-p5.png alt=3-p5 style=zoom:50%><p>位置编码为每个位置生成独特的编码向量，将其与输入的词嵌入向量相加，让模型学习到每个元素在序列中的位置。</p><img src=3-p6.png alt=3-p6 style=zoom:50%><br><h3 id=sine-embeddings正弦位置嵌入><strong>sine embeddings（正弦位置嵌入）</strong><a hidden class=anchor aria-hidden=true href=#sine-embeddings正弦位置嵌入>#</a></h3><p>sine Embeddings 是一种经典的绝对位置嵌入方法，在论文《Attention Is All You Need》中提出，用于 Transformer 模型中。利用不同频率的正弦和余弦函数为每个位置生成一个独特的向量。</p><br>$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$$$PE_{(pos, 2i + 1)} = \cos\left(\frac{pos}{10000^{2i+1/d_{model}}}\right)$$<p>​</p><ul><li>pos 是 token 在序列中的位置索引（从0开始）</li><li>i 是维度索引（不是维度），取值范围：$0 ≤ i ＜ d_{model} / 2 - 1$</li><li>$d_{\text{model}}$ 是模型的嵌入维度</li><li>$PE_{(pos, j)}$表示位置 pos 的编码向量中第 j 维的值</li></ul><br><p><strong>正弦余弦交替使用</strong>
对于每对相邻维度，偶数维度使用正弦函数，奇数维度使用余弦函数。</p><br><p><strong>波长的递增序列</strong>
指数$10000^{2i/d_{model}}$为不同维度创建了不同的波长：</p><ul><li>当$i = 0$时：$10000^{0}=1$，波长为$2\pi$，波长短，函数值变化快。有助于捕捉近距离关系，具有高敏感性。</li><li>当$i = d_{model}/2 - 1$时：$10000^{(d_{model}-2)/d_{model}}\approx10000$，波长为$10000\cdot2\pi$，在很长的序列范围内，高维度的编码值几乎恒定，有助于学习长距离依赖关系，提供稳定的、粗粒度的 “区域” 或 “上下文” 信号。</li></ul><br><p><strong>为什么选择10000？</strong>
波长依据维度形成从$2\pi$到$10000\cdot2\pi$的几何级数，保证位置编码的频率覆盖范围足够大，同时兼顾长序列与短序列。</p><br><p><strong>sine embeddings的特性</strong></p><ol><li><p><strong>唯一性和有界性</strong></p><ol><li>各位置的编码向量互不相同</li><li>所有值都在[-1, 1]范围内</li><li>显示引入位置信息，不需要学习</li></ol></li><li><p><strong>无限外推能力</strong></p><ol><li>sin 和 cos 对于任何实数都有定义，可以对任意位置生成编码，不受训练数据长度限制</li><li>编码模式沿着序列位置平滑变化，没有突变点</li></ol></li><li><p><strong>线性表示相对位置</strong></p><p>任意固定位置偏移的相对位置关系可以被表达为线性变换。</p><ul><li><p>对于任意固定偏移k，$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数。</p></li><li><p>对于任意位置$pos$和偏移$k$，考虑特定维度$i$：</p><p>令$\alpha=\frac{pos}{10000^{2i/d_{model}}}$和$\beta=\frac{k}{10000^{2i/d_{model}}}$，则</p><p>$PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)=\sin\alpha$，$PE_{(pos,2i + 1)}=\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)=\cos\alpha$​</p><p>通过积化和差公式：</p><p>$PE_{(pos + k,2i)}=\sin(\alpha + \beta)=\sin(\alpha)\cos(\beta)+\cos(\alpha)\sin(\beta)$</p><p>$PE_{(pos + k,2i + 1)}=\cos(\alpha + \beta)=\cos(\alpha)\cos(\beta)-\sin(\alpha)\sin(\beta)$</p></li></ul><p>这表明$PE_{pos + k}$可以通过$PE_{pos}$的线性组合得到，线性系数仅与偏移$k$相关，独立于位置$pos$。</p><p>能高效推断相对关系，但编码的仍然是每个 token 的绝对位置。有没有直接编码相对位置关系的方式呢？</p></li></ol><br><p>参考：</p><p><a href="https://www.bilibili.com/video/BV1nFarzjE6B/?spm_id_from=333.337.search-card.all.click&amp;vd_source=72711b8f61a3026963de2d6a03a34dea">【Transformer】9分钟认识位置编码 | 数学解析 | 旋转位置编码_哔哩哔哩_bilibili</a></p><br><h3 id=rope-embeddings旋转位置嵌入><strong>Rope embeddings（旋转位置嵌入）</strong><a hidden class=anchor aria-hidden=true href=#rope-embeddings旋转位置嵌入>#</a></h3><p>简单理解：我们需要一种表示相对位置的算法。RoPE在计算注意力时，通过将qk都乘以旋转矩阵，来把注意力运算结果中加入位置信息。</p><p>特性：</p><ol><li><strong>相对距离决定注意力</strong>：两个 token 之间的注意力分数只依赖于它们的相对位置差，而不是绝对位置</li><li>平移不变性：序列整体平移不会改变内部的注意力模式</li><li><strong>保留长度外推能力</strong>：随着相对距离增加，注意力分数自然衰减。这符合靠的近的token注意力高，离得远的token注意力低的要求</li></ol><p>原理解析参考，这两个已经解释的较为清楚了：</p><p><a href="https://www.bilibili.com/video/BV1CQoaY2EU2/?spm_id_from=333.337.search-card.all.click&amp;vd_source=72711b8f61a3026963de2d6a03a34dea">旋转位置编码RoPE的简单理解_哔哩哔哩_bilibili</a></p><p><a href=https://www.zhihu.com/tardis/bd/art/647109286>十分钟读懂旋转编码（RoPE）</a></p><br><h1 id=2hyperparameters>2.Hyperparameters<a hidden class=anchor aria-hidden=true href=#2hyperparameters>#</a></h1><p>经过这些年来大模型的迭代发展，人们在一些超参数的选择上逐渐形成了共识。</p><h2 id=21-feedforward---model-dimension-ratio>2.1 Feedforward - model dimension ratio<a hidden class=anchor aria-hidden=true href=#21-feedforward---model-dimension-ratio>#</a></h2><p>一般模型： $d_{ff}=4d_{model}$</p><p>如果使用GLU及其变体作为激活函数，需要额外引入一个权重矩阵。为了保持与原始 FFN 相同的计算成本和参数规模，GLU 变体普遍将隐藏层维度 $d_{ff}$ 缩小为原来的大约 2/3。所以 $d_{ff}=\frac{2}{3}\times(4d_{model})=\frac{8}{3}d_{model}$</p><img src=../pic/3-p7.png alt=3-p7 style=zoom:50%><br><h2 id=22-head-dimnum-heads-to-model-dim-ratio>2.2 Head-dim*num-heads to model-dim ratio<a hidden class=anchor aria-hidden=true href=#22-head-dimnum-heads-to-model-dim-ratio>#</a></h2><p>Head-dim*num-heads和model-dim的比例基本为1。</p><img src=../pic/3-p8.png alt=3-p8 style=zoom:50%><br><h2 id=23-aspect-ratios>2.3 Aspect ratios<a hidden class=anchor aria-hidden=true href=#23-aspect-ratios>#</a></h2><p>模型应该更宽还是更深？</p><img src=../pic/3-p9.png alt=3-p9 style=zoom:50%><p>一般 $d_{model}/n_{layer}=128$​</p><p>如下图所示，不同参数量的模型基本都在比率100左右表现最好。</p><img src=../pic/3-p10.png alt=3-p10 style=zoom:50%><br><h2 id=24-vocabulary-sizes>2.4 Vocabulary sizes<a hidden class=anchor aria-hidden=true href=#24-vocabulary-sizes>#</a></h2><p>早期的单语言的模型的词表大小在30-50k。现在的多语言模型在100-250k左右。</p><img src=../pic/3-p11.png alt=3-p11 style=zoom:50%><br><h2 id=25-dropoutregularization>2.5 Dropout&amp;Regularization<a hidden class=anchor aria-hidden=true href=#25-dropoutregularization>#</a></h2><p>在预训练时，由于数据量非常大，只需要在数据集上训练一遍，不大会出现过拟合。</p><p>早期模型会在预训练时采用dropout。现在大多只使用weight decay。</p><p>一般来说，使用weight decay是用来防止过拟合的。但实验发现weight decay并不会影响 val loss和train loss（下图左）。weight decay会与cosine模式的optimizer相作用，当学习率减小时，train loss会有效下降（下图中）。</p><img src=../pic/3-p12.png alt=3-p12 style=zoom:50%><br><h1 id=3stability>3.stability<a hidden class=anchor aria-hidden=true href=#3stability>#</a></h1><p>我们希望模型在训练时，loss稳定下降，而不要波动过大，出现过多spike。</p><p>transformer的不稳定成因主要来自softmax：$\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$</p><p>其中的指数运算易出现数值不稳定。其中的除法运算可能会除0错误。</p><p>softmax出现在模型的最后和注意力层中，以下分别从这两个位置进行优化。</p><h2 id=31-z-loss>3.1 z-loss<a hidden class=anchor aria-hidden=true href=#31-z-loss>#</a></h2><img src=../pic/3-p13.png alt=3-p13 style=zoom:50%><p>增加一个惩罚项，使Z的值稳定在1。</p><br><h2 id=32-qk-norm>3.2 QK norm<a hidden class=anchor aria-hidden=true href=#32-qk-norm>#</a></h2><p>看不懂，跳过</p><br><h2 id=33-soft-capping>3.3 Soft-capping<a hidden class=anchor aria-hidden=true href=#33-soft-capping>#</a></h2><p>看不懂，跳过</p><br><h1 id=4attention>4.Attention<a hidden class=anchor aria-hidden=true href=#4attention>#</a></h1><h2 id=41-gqamqa>4.1 GQA/MQA<a hidden class=anchor aria-hidden=true href=#41-gqamqa>#</a></h2><p>当我们生成文字时，需要不断地预测下一个token，这一过程不能并行。</p><p>通过KV cache可以减少开销。</p><p><strong>KV cache</strong></p><p>KV Cache 是大语言模型（LLM）推理阶段的核心优化技术，通过缓存注意力机制中重复计算的键（Key）和值（Value），显著提升生成效率。</p><p>在 Transformer 解码器（如 GPT 系列模型）中，每一步生成新 token 时，都需要对<strong>已生成序列</strong>和<strong>当前输入</strong>计算自注意力（Self-Attention）。假设已生成序列长度为 n，新生成 token 为第 (n+1) 个，则自注意力的核心公式为：</p><p>(\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V)</p><p>其中：</p><ul><li>Q（Query）：当前输入的查询向量（维度 (d_k)）；</li><li>K（Key）、V（Value）：已生成序列中所有 token 的键和值向量（维度均为 (d_k)）。</li></ul><p><strong>未优化时的问题</strong>：生成第 (n+1) 个 token 时，需要重新计算前 n 个 token 的 K 和 V，导致计算量随序列长度增长呈 <strong>(O(n^2))</strong> 增长，推理速度极慢。</p><p><strong>原理：</strong></p><p>KV Cache 的本质是<strong>空间换时间</strong>：将已生成序列的 K 和 V 向量缓存起来，避免重复计算。</p><ol><li><strong>首次计算（生成第 1 个 token）</strong>：<ul><li>输入：初始序列（如提示词）；</li><li>计算：为每个 token 生成对应的 (K_1, V_1)，并缓存到内存中。</li></ul></li><li><strong>后续计算（生成第 t 个 token，(t > 1)）</strong>：<ul><li>输入：前 (t-1) 个 token 生成的序列 + 新输入（第 t 个 token 的 Query）；</li><li>计算：仅需为第 t 个 token 生成 (Q_t)，直接复用缓存中的 (K_1, K_2, &mldr;, K_{t-1}) 和 (V_1, V_2, &mldr;, V_{t-1})；</li><li>更新缓存：将新生成的 (K_t, V_t) 追加到缓存中，供下一步使用。</li></ul></li></ol><p>通过这一机制，每次生成新 token 时，仅需计算当前 token 的 Q 以及与历史 (K/V) 的注意力得分，计算量从 (O(n^2)) 降至 (O(n))。</p><br><p>还讲了GQA、MQA、sparse attn、sliding window attn、interleave&rsquo;full&rsquo; and &lsquo;LR&rsquo; attn</p><p>慢慢补吧……</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>陈</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>