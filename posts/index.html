<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><meta name=robots content="index, follow"><title>Posts | 陈</title><meta name=keywords content><meta name=description content="Posts - 陈"><meta name=author content><link rel=canonical href=https://Rook1eChan.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.fa96dd64da4525ec034a55f3eb441e4de99a65fdd163418122efb8a659de45d8.css integrity="sha256-+pbdZNpFJewDSlXz60QeTemaZf3RY0GBIu+4plneRdg=" rel="preload stylesheet" as=style><link rel=icon href=https://Rook1eChan.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://Rook1eChan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Rook1eChan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Rook1eChan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Rook1eChan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://Rook1eChan.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://Rook1eChan.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://Rook1eChan.github.io/posts/"><meta property="og:site_name" content="陈"><meta property="og:title" content="Posts"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Rook1eChan.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Rook1eChan.github.io/ accesskey=h title="陈 (Alt + H)"><img src=https://Rook1eChan.github.io/icon.png alt aria-label=logo height=35>陈</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Rook1eChan.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://Rook1eChan.github.io/>Home</a></div><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>2025届CS保研经验贴</h2></header><div class=entry-content><p>一路上受到了很多朋友和陌生网友的帮助，无以为报。因此，我也将自己的经验写下来，供后来者参考。（更适合双非四非的同学参考）
个人背景 **学校：**北京普通一本
**GPA（rk）：**排名2
**英语：**CET-4：611；CET-6：583
**所获荣誉：**校三好、优团（国奖用处大，其它荣誉没啥用）
**竞赛（用处不大）：**1.蓝桥杯C++省二；2.北京市校际合作市优秀奖；3.美赛H
**科研：**一篇论文在投，一段科研
**最终去向：**计算所
0.写在前面 **你是否应该选择保研？**除了保研，升学的道路还有考研和留学。我们在努力之前，应该先问清自己的目标是什么。
在时间上，保研意味着三年的持续付出，你需要在课内卷绩点，在课外卷竞赛和论文，比其它同学付出更多。而考研则可以享受前三年的大学生活，只紧张最后一年。当然，保研er可以更早解脱。当考研er还在紧张备考时，你已经上岸了。
在去向上，考研的上限更高，只要你分数够高，就能去到对应的好的院校。保研的下限更高，一般大家都能去比本校高一个档次的院校。但是保研的上限较低，保研不是单一用分数评价人，同届有大量手握竞赛论文的大神和你竞争，而且有的院校根本不收双非四非的学生，所以想保研去tp华五这样的顶级院校还是挺难的。
在风险上，保研的风险较小，三年的时间你有很多机会去提升自己，在夏令营预推免的时候你可以联系大量的老师，机会很多。而考研就是看分数，如果发挥不好也没有别的办法了。这也是我选择保研的原因，经历过高考以后，我再也不想参加这种一局定胜负的考试了。
**今年的保研形势？**今年和往年有很大变化：
大部分夏令营停办或无效力，使得剩下开夏令营的学校人数暴涨，oq现象更严重，仍然是20%的人拿80%的offer。相当于少了一个占坑的机会。而且今年是第一年改革，很多人由于变化带来的担忧选择投递档次更低的院校，导致后面的人没了位置。 由于夏令营大家没有offer，导致预推免的门槛也水涨船高。大家的offer比往年档次要低。 院校保研率增加，但接收名额并未增加，保本校的学生会占掉更多名额。 推免系统开放提前。而且今年是统一发复试、预录取通知，学生统一确定（往年是各院校按自己的时间），之间间隔时间变长为两天。这一点利于鸽子起飞，利于候补的学生拿offer。 个人预测，夏令营没效力、保研名额增加会成为趋势。
1.保研术语 名词 含义 夏令营 保研途径1，5月-7月底，竞争非常激烈 预推免 保研途径2，8月底-9月底，难度比夏令营低一些 九推 保研途径3，925开系统之后参加面试，相当于补录，机会较少 rk/rank 专业排名，一般指学习成绩排名，夏令营指前五学期排名，预推免指前六学期排名 title 学校的title高不高，指学校的名气大不大，牌子响不响 强com 学院的招生办和行政单位在招生中起决定性作用，导师不能决定能否录取你，如北航，人大等； 弱com 与上述概念相反，导师在招生中起决定性作用，导师愿意要你的话，就基本稳了。 oq over qualified，你太强了，超过了投递院校的招生资格，不让你入营。如本科清北佬报名末九，学校会认为该生拿到优营也不会来，索性直接拒绝该生入营。 bar 入营的门槛，入营的难度，预推免的bar一般比夏令营低 优营/offer 优秀营员，夏令营考核通过的人，一般优营就是指的offer，即学校发给你的预录取承诺，928填该校，就会录取你（信誉好的学校一定会录取你，有些学校会鸽人）。 候补/wl waiting list，候补队列，如果前面有人鸽了，那么优营名额就会顺延到你，不是所有学校都有wl bg background，个人背景 2.保研准备 论文=本科背景>项目>六级>竞赛>其它
2.1文书材料 以下是保研填系统时要用到的材料。个人陈述可以用doc，其余都是pdf格式：
材料 内容 证件照 一般系统有不超过1M的要求，还有的系统有150*200像素的要求，用画图工具裁剪 身份证扫描件 学生证扫描件 成绩单 大三下成绩出来之前用前五学期的，出成绩后用前六学期的 排名证明 教务处开证明，大三下成绩出来之前用前五学期的，出成绩后用前六学期的 教育部学籍在线验证报告 去学信网开 个人陈述 1000字、300字、100字 四六级证书 所有的获奖证书扫描件 论文封面 个人简历 可以把这些文件都存在一个文件夹里。在这期间，你可能会得了奖或发表了论文，及时更新。
...</p></div><footer class=entry-footer><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></footer><a class=entry-link aria-label="post link to 2025届CS保研经验贴" href=https://Rook1eChan.github.io/posts/%E4%BF%9D%E7%A0%94%E7%BB%8F%E9%AA%8C%E8%B4%B4/%E4%BF%9D%E7%A0%94%E7%BB%8F%E9%AA%8C%E8%B4%B4/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CS336 Lec1</h2></header><div class=entry-content><p>CS336: Language Models From Scratch (Spring 2025)
0.开场白 为什么要从基础的角度学习大模型？我们可以用prompt操纵训练好的模型，但直接使用毕竟是高度抽象的，我们有必要深入了解其实现细节。想要理解大模型，最好的方法就是重建一个大模型。
我们自制的小模型是否有效果？不一定，因为参数量的不同，不同的层（如MLP，MHA）的计算量大不相同，我们的所做的优化在参数扩大后不一定起作用。另外，模型存在“涌现”现象，在计算量达到一定量后，模型的表现会突然变好，所以模型必须有一定的参数量。
这门课的重点是什么？1.全面从底层了解大模型；2.给定计算资源和时间限制，明白怎么去高效的训练模型。
1.课程主要内容 1.1Basics 得到一个能运行的简易pipeline，包括tokenizer（BPE算法）、model architecture（Transformer架构及其各个组件）、training（优化器、学习率等等）。
1.2Systems 如何进一步优化。kernels（数据在内存和GPU之间的传送）、parallelism（多卡训练）、inference（使用模型）
1.3scaling laws FLOPs和模型参量量，和训练使用token量成线性关系。
1.4data 如何选择并处理数据、如何进行模型评估
1.5alignment 将基础模型进行对齐，让其学会follow instructions、具有一定风格、避免输出有害内容。包括SFT、RLHF。
2.Tokenization 模型只能对数值进行运算，而人类需要自然语言进行输入输出。tokenizer就是负责将自然语言（string）转换为tokens（list(int)），以便传入模型。
一个训练好的tokenizer可以对句子进行编码，可以将数组解码回句子。
compress_ratio：字节数/token数，它表示一个token平均对应几个字节。
以下是tokenizer曾使用过的方法：
chatacter_tokenizer：一个简单的想法是：将每个字符直接转换为对应的Unicode。
Unicode是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。 这样做可行，但是token对应的数值范围会变得非常大；而且有很多字符并不常用，存在稀疏的问题。
byte_tokenizer：那么，也可以将句子全部转换为字节。英文字母对应一个字节，有些emoji（:earth_asia:）则对应四个字节。如果使用UTF-8编码，所有的数值都会限制在0-255之间。数值大、稀疏的问题解决了，但是token序列会很长。
word-based tokenization：将句子分为单词，然后对单词编码。但是，单词是无上限的，而且无法处理写错单词（UNK）的情况。
BPE：现在最常用的方法，最早应用于GPT-2。先使用word-based tokenization将句子进行粗略的拆分，然后在每一个分块上使用BPE算法。BPE算法可以简单理解为：把常用的字符组进行聚合，使用一个token表示；不常用的则用多个token表示。
BPE原理及实现：minbpe：BPE算法的极简实现</p></div><footer class=entry-footer><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></footer><a class=entry-link aria-label="post link to CS336 Lec1" href=https://Rook1eChan.github.io/posts/cs336/lec1/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CS336 Lec2</h2></header><div class=entry-content><p>CS336: Language Models From Scratch (Spring 2025)
1. Memory float32，32位浮点数，包括1位符号位，8位指数位，23位尾数位。也称为fp32，单精度。是tensor的默认存储精度。
内存使用的估算：
x = torch.zeros(4, 8) # 建立矩阵 assert x.dtype == torch.float32 # tensor默认精度为fp32 assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes text("One matrix in the feedforward layer of GPT-3:") assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB float16，符号1位，指数5位，尾数10位，也叫半精度。相较于fp32内存可以减半。但是fp16的范围比较小，会出现上溢下溢的问题，影响模型。
bfloat16，bf16，符号1位，指数8位，尾数7位，在和fp16保持相同存储的同时和fp32有相同的动态范围，牺牲了部分精度但可以接受。
fp8，8位，有E4M3、E5M2两种形式。
训练时用fp32效果最好，内存开销也最高；fp16、bf16、fp8内存开销小，但不稳定；一种折中的办法是使用混合精度，只在关键的层使用高精度。
2. Compute 2.1 tensor Pytorch中的tensor（张量）是一个多维数组，可以是1D的向量，2D的矩阵，3D的cube等。tensor是一个指向具体内存的指针+各种元数据。元数据包括shape和stride，shape告诉我们有几个维度，每个维度有多少个元素，stride告诉我们在内存中跳多少步才能访问下一个维度的元素。
tensor默认存储在cpu上，需要显式将其移动到gpu。
memory_allocated = torch.cuda.memory_allocated() x = torch.zeros(32, 32) assert x.device == torch.device("cpu") text("为了利用GPU的并行计算能力，将tensor迁移到GPU") text("Move the tensor to GPU memory (device 0).") y = x.to("cuda:0") assert y.device == torch.device("cuda", 0) text("Or create a tensor directly on the GPU:") z = torch.zeros(32, 32, device="cuda:0") new_memory_allocated = torch.cuda.memory_allocated() memory_used = new_memory_allocated - memory_allocated assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats 某些操作，如切片、转置、改变形状，并不会产生一个新的tensor，只是改变了tensor的元数据的值，例如：
...</p></div><footer class=entry-footer><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></footer><a class=entry-link aria-label="post link to CS336 Lec2" href=https://Rook1eChan.github.io/posts/cs336/lec2/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CS336 Lec2
<span class=entry-hint title=Draft><svg height="20" viewBox="0 -960 960 960" fill="currentColor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h2></header><div class=entry-content><p>CS336: Language Models From Scratch (Spring 2025)</p></div><footer class=entry-footer><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></footer><a class=entry-link aria-label="post link to CS336 Lec2" href=https://Rook1eChan.github.io/posts/cs336/new/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CS336 Lec3</h2></header><div class=entry-content><p>CS336: Language Models From Scratch (Spring 2025)
本节主要讲了模型的架构设计和超参数选择。
1.Architecture 1.1Norm pre-norm, post-norm, ‘double’-norm
自从GPT之后大都采用pre-norm，把layernorm层放到FFN、MHA层之前。
prenorm和postnorm的效果一样好，而且不需要warm。更好的梯度反向传播，更少的spike。
现在有的模型还使用’double’-norm，即FFN、MHA层之前之后都有layernorm。
LayerNorm, RMSNorm
原始的transformer和早期模型使用LN，现在都改为使用RMSN。
LN：$y = \frac{x - \text{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} * \gamma + \beta$
其中 $\text{E}[x]$ 是均值，$\text{Var}[x]$ 是方差，$\epsilon$ 是防止分母为 0 的小量，$\gamma$（缩放因子）和 $\beta$​​（偏移因子）是可学习参数。
RMSN：$y = \frac{x}{\sqrt{\|x\|_2^2 + \epsilon}} * \gamma$​ 其中 $\|x\|_2^2$​ 是输入 x 的二范数平方，$\epsilon$​ 是防止分母为 0 的小量，$\gamma$​ 是可学习的缩放参数。
不减去均值，也不添加偏置项$\beta$​。
RMSN效果和LN一样好，而且更快。操作更少（无需计算平均值），参数更少（没有偏置项）。
曾有研究表明，在模型运算中，矩阵乘法占用的flops达到99.8%，正则化的运算量只占到0.17%。从计算性能的角度看，norm没必要优化。但是内存开销也是一个重要的考量，该研究指出正则化所占的运行时间达到25.5%，在内存搬运上花了相当一部分时间，因此值得优化。
现有的大部分transformer模型都没有bias项，只进行矩阵乘法。reason：更稳定（原因未知）
1.2 Activations ReLU、GeLU、SwiGLU、GeGLU GLU（门控线性单元）现在得到广泛使用
ReLU（Rectified Linear Unit，修正线性单元）是深度学习中最常用的激活函数之一。
ReLU 的函数形式非常简单，数学定义为：$\text{ReLU}(x) = \max(0, x)$即：
...</p></div><footer class=entry-footer><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></footer><a class=entry-link aria-label="post link to CS336 Lec3" href=https://Rook1eChan.github.io/posts/cs336/lec3/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>minbpe：BPE算法的极简实现</h2></header><div class=entry-content><p>minbpe：BPE算法的极简实现 github 地址：karpathy/minbpe
1.BPE 算法 BPE(Byte Pair Encoding) 是大模型的 tokenizer 常用的算法。它对输入文本的字节进行编码。
该算法因 GPT-2 的论文和代码而被广泛使用于 LLM。Sennrich et al. 2015被认为是 BPE 在 NLP 应用中的原始参考。
简单来说，bpe 把文本看作 utf-8 编码的字节，然后将出现次数最多的相邻字节合并，生成一个新的编码。如此反复操作。
2.minbpe 简介 2.1quickstart from minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = "aaabdaaabac" # 训练 tokenizer.train(text, 256 + 3) # 256tokens, 3merges # 编码 print(tokenizer.encode(text)) # 解码 print(tokenizer.decode([258, 100, 258, 97, 99])) # 保存 tokenizer.save("toy") # writes two files: toy.model (for loading) and toy.vocab (for viewing) 英语字母一个字母对应一个字节。对于"aaabdaaabac"，先计算相邻两字节的出现次数，然后选择次数最多的进行合并（“aa”，4次）。
“a”“a”合并为“aa”，编码为256。
然后再计算相邻两字节的出现次数，再合并。
在 toy.vocab 中可以看到所有字符及对应的编码。
toy.vocab ...... [a][a] -> [aa] 256 [aa][a] -> [aaa] 257 [aaa][b] -> [aaab] 258 2.2minbpe和GPT-4分词器功能相同 # 1.证明RegexTokenizer与GPT-4的分词器性能一致 text = "hello123!!!? (안녕하세요!) 😉" # pip install tiktoken import tiktoken enc = tiktoken.get_encoding("cl100k_base") print(enc.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] from minbpe import GPT4Tokenizer tokenizer = GPT4Tokenizer() print(tokenizer.encode(text)) # [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037] # 2.标记特殊token text = "&lt;|endoftext|>hello world" print(enc.encode(text, allowed_special="all")) # [100257, 15339, 1917] # ours print(tokenizer.encode(text, allowed_special="all")) # [100257, 15339, 1917] 调用 encode 时必须显示声明处理特殊标记。allowed_special 参数可以设置为"all"、“none"或一个特殊token列表。
...</p></div><footer class=entry-footer><span title='2025-09-23 14:46:23 +0800 +0800'>September 23, 2025</span></footer><a class=entry-link aria-label="post link to minbpe：BPE算法的极简实现" href=https://Rook1eChan.github.io/posts/minbpe/minbpebpe%E7%AE%97%E6%B3%95%E7%9A%84%E6%9E%81%E7%AE%80%E5%AE%9E%E7%8E%B0/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>在Autodl中使用LLaMA-Factory进行微调</h2></header><div class=entry-content><p>一、环境准备 1.1创建虚拟环境 conda create -n lf python==3.11 conda init 然后重开cmd
conda activate lf 1.2下载相关的包 conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia 验证GPU版本的Pytorch是否成功
python -c "import torch; print(torch.cuda.is_available())" 1.3下载llama factory sudo apt install git 开科学上网
git clone https://github.com/hiyouga/LLaMA-Factory.git 1.4安装依赖 python -m pip install --upgrade pip pip install -r requirements.txt pip install -e ".[torch,metrics]" 如果下载有问题，可以尝试清华源
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e ".[torch,metrics]" 1.5清理pip pip cache purge 二、下载模型 2.1从modelscope下载模型权重文件 pip install modelscope 可以下载到默认的内存，一般在/root/.cache/modelscope/hub/model/里面
...</p></div><footer class=entry-footer><span title='2025-09-22 21:15:56 +0800 +0800'>September 22, 2025</span></footer><a class=entry-link aria-label="post link to 在Autodl中使用LLaMA-Factory进行微调" href=https://Rook1eChan.github.io/posts/llama-factory/llama-factory/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>LumberChunker</h2></header><div class=entry-content><p>EMNLP2024
0.主要贡献 提出了Lumberchunker文本分割方法 提出了GuntenQA数据集 验证了Lumberchunker在下游RAG任务上的效果 1.LumberChunker 使用LLM动态的将文档分割为语义独立的片段。每个片段的长短是不固定的，确保每个片段的语义完整性、独立性。也就是说分割后，每一段包含的语义是完整的，同时与其它段有区别。由LLM来确定合适的分割点，这一决策过程考虑到文本的结构和语义，从而能够创建出大小最优且上下文连贯的片段。
1.先按照paragraph分割目标文档，然后把paragraph顺序连接，直到累计的token数超过一个阈值 $\theta$，形成 $ G_i$。该阈值如何设置后文会说。$\theta$ 应该足够大，防止把具有相关性的段落分开；同时 $\theta$​ 也要足够小，防止过多内容影响LLM进行推理。
2.让LLM寻找 $G_i$ 中“语义断层”的地方，作为分割点。分割点之前即形成一个chunk。剩下的内容继续与paragraph顺序拼接、超过阈值停止、LLM分割……分割整体是串行进行的。
2.GutenQA 数据来源于Project Gutenberg电子图书馆。
1.100本英文书籍，手动提取HTML内容（附录里和NarrativeQA进行了对比，手动提取没有编码错误等问题）
2.使用ChatGPT3.5为每本书生成问题、答案和包含答案的原文片段，人工为每本书筛选30个高质量问题。
问题需要基于给定片段中的具体信息，且不能用书中的其它地方的信息来回答。问题大多以‘what,’ ‘when,’ ‘where’ 开头， ‘why’ and ‘how’较少。
3.原文片段需要简短，以确保任何分块方法都不会把它切开。评估方法是在检索到的文本中精确匹配字符串。
3.Experiments 3.1 propmt的阈值怎么选择 这个阈值就是paragraph顺序连接的阈值 $\theta$​ 。由于是LLM寻找分割点，token过长会影响模型的推理能力。
在不同阈值下使用DCG评估效果。DCG表明了是否检索到，检索结果是否靠前。
3.2 Lumberchunk是否增强了检索效果？ 与其它分块基准进行对比。评估指标为DCG@K、RECALL@K。
此外，注意到semantic chunk和paragraph level的指标并没有随K有效增加，表明其在大规模文档检索方面的局限性。
proposition level的引用在哪？？？
附录F展示了各分割方法的统计结果：
Lumberchunk切分后的块平均长度为334，比预设的550阈值低了40%，这说明LLM有效的对文本进行了切分，而不是持续选择靠近末尾的ID。说明未出现Lost in the Middle现象。
在论文《Lost in the Middle: How Language Models Use Long Contexts》中，作者发现，当针对长文本的不同位置信息设计专门问题，测试大语言模型对不同位置信息的记忆能力时，模型的性能呈现一种 “U 型” 表现，即对于前段与后段的信息有着较强的关注与记忆能力，能较好地解决问题，而对于中段信息的利用则有所逊色。
这种现象的产生可能是由于训练数据中的无意偏差。LLM 的预训练侧重于根据最近的一些 token 预测下一个 token，而在微调过程中，真正的指令又往往位于上下文开始的位置，这在不知不觉中引入了一种立场偏见，让 LLM 认为重要信息总是位于上下文的开头和结尾。
...</p></div><footer class=entry-footer><span title='2025-08-16 21:23:00 +0800 +0800'>August 16, 2025</span></footer><a class=entry-link aria-label="post link to LumberChunker" href=https://Rook1eChan.github.io/posts/lumberchunker/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>论文阅读 | DeepSeek-VL: Towards Real-World Vision-Language Understanding</h2></header><div class=entry-content><p>DeepSeek-VL: Towards Real-World Vision-Language Understanding 0. Abstract 本文的主要贡献：
数据集构建：构建了具有多样性和可扩展性，广泛覆盖真实世界场景的数据集。包括网页截图、PDF文档、OCR文本、图表以及知识型内容（如专家知识、教科书）等。此外，根据真实用户场景将数据进行分类，并据此创建了指令微调数据集。通过该数据集的微调，显著提升了模型在实际应用中的用户体验。
创新的模型架构：采用了混合视觉编码器（hybrid vision encoder），能在固定的token预算下高效处理高分辨率图像（1024*1024），同时保持较低的计算开销。该架构保证模型多种视觉任务中能捕捉到关键的语义和细节信息。
创新的训练策略：既使LLM学会新模态，也保证原有的的语言能力不退化。调控语言和视觉的竞争关系，实现两种模态的均衡融合。
1. Introduction 大语言模型的巨大成功引发了人们对多模态模型的追求。这些模型能同时理解语言和图像，在执行现实世界任务时展现出巨大的潜力。
目前出现了很多开源的VLM方案，在benchmark上表现优秀，但在现实世界中表现不佳。大都存在以下问题（本文的改进方案）：
许多方案将重心放在指令微调阶段。作者认为应当使用大量的视觉-语言数据进行充分预训练。（深度预训练）
现有方案多使用学术上的数据集进行微调，缺乏现实世界经验。（精心构建数据集）
现有方案多采用vision transformer与预训练语言模型结合的方式，这类模型分辨率低，不能胜任OCR或微小物体识别任务。（高分辨率处理架构）
有些模型在长期的多模态训练中会出现语言能力的退化。应采用一种既保留语言能力，又掌握新模态能力的训练方式。（平衡多模态特征的训练策略）
DeepSeek-VL具有通用的多模态理解能力，能够处理逻辑图、网页、公式识别、科学文献、自然图像等。
DeepSeek-VL的优势：
Deepseek-VL的预训练数据涵盖了广泛的世界知识，包括网络爬虫、网页代码、电子书、教育资料、arxiv文章等等，全面覆盖现实世界中的场景，数据质量高，具有广泛性和实用性。同时作者团队还精心设计了指令调优数据集，具体来说，作者从网上收集了GPT-4V和Gemini的真实案例，并进行分类，为每个测试图像选择合适的prompt。该分类体系还用于构建评估数据集。
视觉模块采用混合视觉编码器架构，384$\times$384的文本对齐编码器用于粗粒度语义提取，1024$\times$1024的高分辨率编码器用于捕捉细节视觉信息。两者结合，可以将1024$\times$1024的图像压缩为576个token，在视觉表征和token开销间取得平衡，使视觉模块支持文-图交织处理和多轮推理场景。
为了使多模态模型不出现语言能力的退化：1.保持至少70%的语言数据，这对维护模型内部的语言知识完整性至关重要。2.作者提出了模态预热(modality warm-up)策略。该方法通过在训练过程中动态调整模态比例，逐步引入更多视觉-语言数据。
在迭代模型时，首先在小模型上进行实验。然而，形如1B的小模型在benchmark上难以展现理想性能，无法真实的反映模型的实际表现。因此，作者把评估措施从多选改为了各选项的困惑度（PPL）对比；此外，为避免指令跟随能力成为瓶颈，在预训练阶段我们混合了少量指令调优数据。通过这种方式，我们既能利用1B模型获得合理性能表现，又能更精准地量化实验中每次迭代的影响效果。
2. Data Construction 数据集包括两大模块：VL-Pretrain数据、VL-SFT数据
VL-Pretrain整合了多源视觉文本数据，旨在强化模型的基础跨模态理解能力。
VL-SFT相对较小，主要用于训练模型完成特定下游任务。
在stage1，VL-Pretrain用于预热VL adapter
stage2，VL-Pretrain用于联合预训练VL adaptor和VL model
stage3，使用VL-SFT微调整个模型
2.1 VL-Pretraining Data 分为以下7个类别：
Interleaved image-text data（交错式图文数据，使模型对多模态输入具有更好的上下文学习能力），MMC4、Wiki等
Image caption data（图像描述，包含高质量图-文对），Capsfusion、TaiSu等
Table and chart data（图表数据），Chart2text、Unichart
Web Code data（网页代码，使模型具有从图形界面或图表重建代码的能力。从Stack数据集中的jupyter notebook清洗出2million图像-代码对。最终选择1.1million作为是主要训练集，包括一张图像-至少5行代码）
OCR data（文档光学字符识别数据，作者构建了一个中英混合的OCR数据集，包括两部分：1.arxiv文章 2.电子书和教育材料，来自Anna’s Archive）
Scene text OCR（增强模型识别场景中文本的能力）ArT、MLT-17等。
Text-only corpus（纯文本，和DeepSeek LLM的一致）
2.2 VL-SFT Data 包括多个知名开源数据集ShareGPT4V、LAION-GPTV等。
...</p></div><footer class=entry-footer><span title='2025-07-24 16:00:00 +0800 +0800'>July 24, 2025</span></footer><a class=entry-link aria-label="post link to 论文阅读 | DeepSeek-VL: Towards Real-World Vision-Language Understanding" href=https://Rook1eChan.github.io/posts/deepseekvl/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>博客 | Stop Saying RAG Is Dead</h2></header><div class=entry-content><p>这是一个系列博客，包括五篇文章。博客地址：Stop Saying RAG Is Dead – Hamel’s Blog
作者批驳了“RAG已死”的说法，认为真正被淘汰的是“Chuck documents into a vector database, do cosine similarity, call it a day. ”的过时的RAG。RAG技术仍在进化，在后面的的文章里可以看到在检索、评估等方面上的创新。很高兴看到有人对RAG持积极态度，毕竟在一个有希望的领域进行研究学习更有动力。
五篇文章的简介如下：
标题 内容简介 Part 1: I don’t use RAG, I just retrieve documents Ben Clavié 介绍了RAG的现状 Part 2: Modern IR Evals For RAG 评估是必不可少的步骤，高质量的benchmark有助于我们选择更好的方法。Nandan Thakur （BIER作者）认为传统的IR指标不适合评估RAG的表现，应该采用新的指标 Part 3: Optimizing Retrieval with Reasoning Models Orion Weller 提出了一种能遵循instruct的检索系统，在检索时就进行推理，优于传统的语义检索 Part 4: Late Interaction Models For RAG Antoine Chaffin 介绍了ColBERT这类迟交互、多向量模型 Part 5: RAG with Multiple Representations Bryan Bischof and Ayush Chaurasia 提出，我们需要对不同模态的问题智能化的选用不用的指标 P1: I don’t use RAG, I just retrieve documents 现在有一些说法，认为长上下文窗口的出现使得我们不再需要RAG了。
...</p></div><footer class=entry-footer><span title='2025-07-17 10:07:00 +0800 +0800'>July 17, 2025</span></footer><a class=entry-link aria-label="post link to 博客 | Stop Saying RAG Is Dead" href=https://Rook1eChan.github.io/posts/%E5%8D%9A%E5%AE%A2-stop-saying-rag-is-dead/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://Rook1eChan.github.io/posts/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=JetBrains+Mono&display=swap" rel=stylesheet><footer class=footer><span>&copy; 2025 <a href=https://Rook1eChan.github.io/>陈</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>